{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import pickle\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images first\n",
    "numImages = len(glob.glob('./images/*jpg'))\n",
    "images = [None for i in xrange(numImages)]\n",
    "for fileName in glob.glob('./images/*jpg'):\n",
    "    fileNum = int(fileName[9:][:-4])\n",
    "    images[fileNum-1] = np.array(cv2.imread(fileName))\n",
    "images = np.array(images)\n",
    "\n",
    "# Load csv data next\n",
    "train_data = pd.read_csv('data/train.csv').drop(['species'], axis=1).values\n",
    "train_labels = pd.read_csv('data/train.csv')['species'].values\n",
    "labels=train_labels.tolist()\n",
    "train_images = [images[int(data[0]-1)] for data in train_data]\n",
    "train_ids = [data[0] for data in train_data]\n",
    "train_data = np.delete(train_data, 0, 1)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv').values\n",
    "test_images = [images[int(data[0]-1)] for data in test_data]\n",
    "test_ids = [data[0] for data in test_data]\n",
    "test_data = np.delete(test_data, 0, 1)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN train data\n",
    "def img_norm(img):\n",
    "    t= 2 * (np.float32(img) / 255 - 0.5) # normalize img pixels to [-1, 1]\n",
    "    return t\n",
    "def minibatchData(data,labels_encoded,img_size,channel_num=3,batch_num=30):\n",
    "    images=[]\n",
    "    for img in data:\n",
    "        if channel_num is 1:\n",
    "            images.append(img_norm(cv2.resize(cv2.cvtColor(img,cv2.COLOR_BGR2GRAY),img_size)))\n",
    "        else:\n",
    "            images.append(np.transpose(img_norm(cv2.resize(img,img_size)),[2,0,1]))\n",
    "        \n",
    "        \n",
    "    if batch_num > 1:\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        print(len(images))\n",
    "        print(batch_num)\n",
    "        \n",
    "        for i in range(int(len(images) / batch_num)):\n",
    "            minibatch_d = images[i*batch_num: (i+1)*batch_num]\n",
    "            minibatch_d = np.reshape(minibatch_d, (batch_num, channel_num,img_size[0],img_size[1]))\n",
    "            batch_data.append(torch.from_numpy(minibatch_d))\n",
    "            if labels_encoded is not None:\n",
    "                minibatch_l = labels_encoded[i*batch_num: (i+1)*batch_num]\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "            else:\n",
    "                minibatch_l = np.zeros(batch_num)\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "        #data, labels = batch_data, batch_labels \n",
    "        \n",
    "    return zip(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le= preprocessing.LabelEncoder()\n",
    "#encode train labels\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded=le.transform(train_labels)\n",
    "Y_labels = np_utils.to_categorical(train_labels_encoded,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "img_size=(224,224)\n",
    "cnn_train_data = list(minibatchData(train_images,train_labels_encoded,img_size))\n",
    "#plt.imshow(cnn_train_data[0][0][3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "2\n",
      "990\n",
      "30\n",
      "594\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10c02df50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFzdJREFUeJzt3X20FXW9x/H3l8OToCaYD3AgAQEfbwqhAnm7FtVRIrF8WJrXyEjEh6xWtwK95bLr6uay0lwqhqJR10QueZUMRSW8aQgCooYigqBwgAS9aiqJwPneP2Y27Dnswzlnz8ye/fB5rcU6M7PnzHyZs/d3f3+/efiZuyMiktMh6wBEpLwoKYhIhJKCiEQoKYhIhJKCiEQoKYhIhJKCiESklhTM7FQzW2lmq81sUlr7EZFkWRoXL5lZHfAy8DmgEVgMnOfuLya+MxFJVMeUtnsisNrd1wCY2QxgLFAwKXS2Lt6V7imFIiIA7/LWG+5+UGvrpZUU6oH1efONwEn5K5jZBGACQFe6cZKNSikUEQF4zGe91pb10upTsALLIu0Ud5/q7sPcfVgnuqQUhoi0V1pJoRHomzffB9iY0r5EJEFpJYXFwCAz629mnYFzgdkp7UtEEpRKn4K77zCzy4G5QB1wp7u/kMa+RCRZaXU04u5zgDlpbV9E0qErGkUkQklBRCKUFEQkQklBRCKUFEQkQklBRCKUFEQkQklBRCKUFEQkQklBRCKUFEQkQklBRCKUFGQPczc+y9yNz2YdhmQktbskpTKVNBlY3gO6NPp52VClICIRqhQEgDkbngmnSvc9MadxKQCj64eWbJ/SOlUKIhKhSqGMnbliMwAndl3LVZ86E4Ad6xsT3cfMxqcAqLN9Et1uW9SZvpPKkf4qIhKhSqFMNM3ry6NH/aGFV7skWiE0zQuevh/sb88KYfj3JgLwERYmts98nR7vFU7ptGc5UlLI2O5TgC1/QAb+biKHx/yAvnz7Caz9wu2t7qsUHhz8EADbfWemcUhhaj6ISIQqhRLaMGkkAM998+Z2dbKt/sptNPzb8UXt89VrRwCw9gtTivr9pOVfHNXJ6jKMRFqiSkFEIlQplMCtrz0JwOGdct+S6eXirV8OBvd+4uZfhUva33+wzxs7Eowo8PoVI8MpdS6Wu6KTgpn1BX4DHAo0AVPd/Zdm1hO4F+gHvAqc4+5vxQ+1stQNGsCc/70vnNs39vZyZffof/nyrmU7V60psE78D13nuUtibyOnLR2pUl7ifGXtAL7r7kcBw4HLzOxoYBIwz90HAfPCeRGpEEVXCu6+CdgUTr9rZiuAemAscEq42nTgceAHsaKsQLurhMrYbhp0+3VlSqRxa2b9gCHAIuCQMGHkEsfBLfzOBDNbYmZLtrMtiTBEJAGxOxrNbF/g98C33f3vln+P/F64+1RgKsD+1rOibqYf+dyHkfkFx3Xetezqg17MIqREfXXlegB+c0Tfdv1eNR2DWhYrKZhZJ4KEcLe75+ra182sl7tvMrNewOa4QZaDo5YGh+rGXgU64TaWOJiUnb/fmwA8tXggAM/9JLhG4oPxe/YXLx46s3SBtdGa3wXxDviKmi/FKLr5YEFJMA1Y4e6/yHtpNjAunB4HPFB8eCJSanEqhU8CFwB/NbNcSr4S+Ckw08zGA+uAs+OFmK1a7iy7uX5RMHHLomwDaYOtXzqJJ24Jrs1o6J1xMBUuztmHJ4GWOhBGFbtdEcmWrmhsga7AK2+5v8+zk24Nl+jvlBTd+yAiEaoUCqgbNCDvG0jSVly/jSqDtCgpFFBJVw3Kbju9KesQqoKaDyISoUohzys/Hx5OqTStRAPnXAzAYBZnHEllU6UgIhGqFPKsPu+2rEMQyZySArV91WI1GXyRmg1JUPNBRCJqPim8fcGIrEMQKSs1nxREJKrmk8Ki68pjPASJr2le311D4knxarajscdfemYdgiQsNxZnA8UNnJNvzAvBA2UePKZH7G1VmpqvFEQkquYqBevSBYAZ/f+UcSSSluanmE8bOHLXdNPWrXus36FrVwCsezfm/DV4XzT0jl9tVCpVCiISUXOVwsNry//RYpKsh1Yv2DV9//vBaF1TBg3c/fqahSWPqZypUhCRiJqrFKS2ndH9veCnLm1vUU1VCrrHQfZGD2kJ1FRSEJHW1VRS0DeB7E2d1dTHoUU6CiISkcQAs3XAEmCDu48xs/7ADKAn8Axwgbt/uLdtpG13X4JyoOzdaQNyj+T7ILFtrv7tEAAGXrAssW2mKYlPybeAFXnz1wE3uPsg4C1gfAL7EEld0s3LukMOpu6Qgxl4wbKKSQgQMymYWR/gC8Ad4bwBnwFmhatMB86Isw8RKa24zYcbge8D+4XzBwJvu/uOcL4RqI+5j6LpFKS0R5114P5XngDg9PoTWl7RwiFU3VtcNnvDYrpYJ6Dy7qOIMxT9GGCzuy/NX1xgVS+wDDObYGZLzGzJdrYVG4aIJCzuUPSnm9looCuwP0HlcICZdQyrhT7AxkK/7O5TgakA+1vPgolDpNRy3+6nv/hmZPnsow/cPRNWA7e99iT9OwX3UuT6I3af1uyUbqApKrpScPfJ7t7H3fsB5wJ/cvfzgfnAWeFq44AHYkcpIiWTxr0PPwBmmNm1wDJgWgr72Cv1JUhclx2wPjq/cX2BtfbdNVXowqf+sycAMJinE40tbYkkBXd/HHg8nF4DnJjEdkUq2eCJlZUMcnQ1j4hEVF1SmLPhmaxDEKloVZcURCSeqksKutNNJJ7qefJSh7qsIxCpCvpaFZGIqqkU5jYubX0lEWmVKgURiVBSEJEIJQURiVBSEJGIiu9o1M1PUq5y782aeciKiFQnJQWRlG2+fCSbLx+ZdRhtpqQgIhEV36cgUu6WXXkrAKPPHM3OTxd8OmFZUVIQKZE5R8zZ44mlo4/5NE3vvgtAhx499vidnVu2lCK0CDUfRCRClYJIhua8ML/F13Z6E6Prh5YwmoAqBRGJqOik0LFPZoNPiaQuiyoBKrT50LHfxwD444LZGUciUn0qulIQkeRVVKWw9j9HAPDyuCkZRyJSvVQpiEhErErBzA4A7gCOJRhd+uvASuBeoB/wKnCOu78VK8qQKgSR9MWtFH4JPOzuRwLHASuAScA8dx8EzAvnY/vHWI1EJ1IKRScFM9sf+BThALLu/qG7vw2MBaaHq00HzogbpIiUTpxKYQCwBbjLzJaZ2R1m1h04xN03AYQ/D04gTvZ54Gkaeh9PQ+/jea/pA95r+iCJzYpIM3GSQkdgKDDF3YcA79OOpoKZTTCzJWa2ZDvbYoQhIkmK09HYCDS6+6JwfhZBUnjdzHq5+yYz6wVsLvTL7j4VmAqwv/X09uz4zD7DAehw7JG7lj30yIz2xi8iBRRdKbj734D1ZnZEuGgU8CIwGxgXLhsHPBArQhEpqbgXL30TuNvMOgNrgAsJEs1MMxsPrAPOjrmPFjUtfymtTYtkLv+hxKV8+Ku5t6tyT8X+1tNPslHxNmIGwJzGpRp5WqpasQniMZ+11N2HtbaePj0iElFR9z7sVVjxqEqQapdrVvSfPQGAwROfTnT7+gSJSET1VAqhbb6dLtYp6zBEUrf6i7cBUHd6h0Q7IquuUji9/oSsQxApiTrrkEpzueqSgojEo6QgUuEa6ockuj0lBRGJqLqORoCFH+wEYHjXuowjEUne8O9NBOAjdy8MlyR7AaIqBRGJqMpKQaSa7a4Q0lGVlcLVAz7B1QM+kXUYIrHt9CZ2etOu+cHTL0l9n1WZFESkeFXZfLhmzdJwSh2NUtlyFyflrljsz1Op71OVgohEVGWlMPnS4JTN/DtvzzgSkXhK+XCVnKpMCp0fXpx1CCKxZZEQQM0HEWmmKisFkUo2+uO5RxO+mcn+VSmISERVVwq5Nln+U3FFytUjW4OHA+18I5sKIaeqk4JIJfn5wGOyDgFQ80FEmqmJpJAbmFaknK370UjW/Whk1mHURlIQkbaLlRTM7Dtm9oKZLTeze8ysq5n1N7NFZrbKzO4Nh5QrC0u3fZh1CCItWn7xzSy/+Oaswyg+KZhZPXAFMMzdjyW4++hc4DrgBncfBLwFjE8iUBEpjbjNh47APmbWEegGbAI+QzAsPcB04IyY+0jMlf1PzDoEkRaNrh/K6PqhWYdR/ClJd99gZj8jGFn6H8AjwFLgbXffEa7WCNTHjjJBW5uCJkS3DmXTqpEatnb7e0w87OTownCwZDIa/DlO86EHMBboD/QGugOnFVi14P/MzCaY2RIzW7KdbcWGISIJi3Px0meBte6+BcDM7gNGAgeYWcewWugDbCz0y+4+FZgKwVD0MeJoly/1CZoQuspRysEeVQJkViHkxOlTWAcMN7NuZmbAKOBFYD5wVrjOOOCBeCGKSCnF6VNYZGazgGeAHcAygm/+PwIzzOzacNm0JAIVqQafujQYPn6f+5MdPj5J5hmXKhA0H06yUa2vmKD3Hx4AwJMfv6+k+5XalfVVtY/5rKXuPqy19XRFo4hE1Oxdkt1PXRNMFOwGFUlO1hVCe6lSEJGImq0URNJWaRVCjioFkRRUakIAJQURaabmmw8NvY/X1Y2SmCNvvxSAw1iQcSTFU6UgIhE1XymIJOmwqyu3QshRpSAiEUoKQMOKMTSsGJN1GCJlQc0HgFGNwU9d3Sgx5TqtdUpSRKqGkkKea7YczTVbjs46DKkCO0Z9IusQiqakICIR6lPIs+C48GGu6luQNjrrlc9G5t/95zcA6MjSLMJJhJJCAUfddikrJt6adRhSRu5/f1+mDBpY4JU3Sh5L2tR8EJEIVQoFfOzHC2Bi1lFIFk4bGAzw2rR1a8aRZEeVgohEKCm0QMPX157Pn/01mrZurekqAZQUWjXsR5cw7EeXZB2GlID9RbfQg5KCiDSjjsZWHHjHUwCccMY5ACweOjPLcERSp0pBRCJaTQpmdqeZbTaz5XnLeprZo2a2KvzZI1xuZnaTma02s+fNbGiawZdSzzEv03PMy1mHIZK6tlQKvwZObbZsEjDP3QcB88J5CIaiHxT+mwBMSSZMESmVVpOCu/8Z+L9mi8cC08Pp6cAZect/44GFBMPS90oq2HKg05RS7YrtaDzE3TcBuPsmMzs4XF4PrM9brzFctqn4EMtPLjHoKdDZO/rW4OnJfa/d89mIb44fAcCt/34TJ3bpBMALH/4DgGM677PH+jPWL+DcviPTCrViJH32wQosKzistZlNIGhi0JVuCYchIsUqNim8bma9wiqhF7A5XN4I9M1brw8t3Ijs7lOBqRAMRV9kHJk69bATAXj4taczjqT2HDEtuKCsX4EKIefAacHp5B9OO6Hg69YxePvbkcHdjzMevivJECtWsackZwPjwulxwAN5y78anoUYDryTa2aISGVotVIws3uAU4CPmlkjcDXwU2CmmY0H1gFnh6vPAUYDq4GtwIUpxFw2fPuHAAz9j0t45oc60dJWn77wG5H5+Xfd0e5tdNxaqKXaPr5jR/Bz+UsAnNNnROxtVoNWk4K7n9fCS6MKrOvAZXGDqjQHTXkKfhhMr9vxHgBfvOH7PPc9PailuYbex9OZJXssa+71K4IOv2cn6RiWmq5oFJEI3fuQkNy33fWvLgTg0BsWMKBf8KSWNWfflllc5WLaO4e2a/1Dbgo6EBtuamEA4PitB2mBKgURibCgGyBb+1tPP8n26KKoGq/N/CcAXjr5txlHkp0krgR97cdBR+BL39jdqasrTNvuMZ+11N2HtbaeKgURiVClkDQLG7sFjmunx4PbQB4c/FApIyoLSX+jV8OYjaWmSiEr7gUTAsD2Uzax/ZRNNflG7rDffnTYb7+sw5A2UFIQkQidksxIQ/0QAOZuWJZxJKVx1tOrAJh5VPtOTbZk9JDPh1Ob97qetJ8qBRGJUKWQlbDfIde/MGhxF26uXxRrkzu9iTpLLs//8+UXA/DEzb+Kva3/+vYYgD0ucS7WztdVIaRFSaFMrDphW9GjXe/0JoBEEkL/P1zE4IsXA9DNglvCG+6L3zE6f2Nw01MtdrJWGjUfRCRClUIZGfKT4NFiy65s+c7A/G9a/2QwvWpc8KixtWNujx1DrkoIdpDcNSz9/3hRsH0Wt7KmZE2VgohE6IrGMtQ0L3ii3aNH/WHXsuOuD6qIQ29o+fFj+Yp9qKza/NWrrVc0qvlQhjqMCh6I3cDup0a3NRnk6InTUiw1H0QkQkmhAsQp6Rt6H68mgbSLkoKIRCgp1IjcvRYirVFSEJEIJYVa4b6rf2HTjvfYFD6KXqQ5nZKsQV/72Mm7pl+7Jhhf4aWLNL6CBFQpiEhEW4aNuxMYA2x292PDZdcDXwQ+BF4BLnT3t8PXJgPjgZ3AFe4+N6XYJQGHXR1cFHX44cEIfwOpjYe+SMvaUin8Gji12bJHgWPd/ePAy8BkADM7GjgXOCb8nVvNrC6xaEUkdW0ZS/LPZtav2bJH8mYXAmeF02OBGe6+DVhrZquBE4GnEolWUjPwX8MKwSzRuyOl8iTRp/B1IPfM8npgfd5rjeEyqRRKCDUv1tkHM7sK2AHcnVtUYLWC7zIzmwBMAOhKtzhhiEiCik4KZjaOoANylO++/7oR6Ju3Wh9aeMiYu08FpkJw63SxcYhIsopqPpjZqcAPgNPdfWveS7OBc82si5n1BwYBT8cPU0RKpS2nJO8BTgE+amaNwNUEZxu6AI9aMEzaQnef6O4vmNlM4EWCZsVl7r4zreBFJHl68pJIjdBYkiJSFCUFEYlQUhCRCCUFEYlQUhCRCCUFEYlQUhCRCCUFEYkoi4uXzGwL8D7wRtaxAB9FceRTHFGVHMdh7n5QayuVRVIAMLMlbbnaSnEoDsWRbhxqPohIhJKCiESUU1KYmnUAIcURpTiiqj6OsulTEJHyUE6VgoiUgbJICmZ2qpmtNLPVZjapRPvsa2bzzWyFmb1gZt8Kl/c0s0fNbFX4s0eJ4qkzs2Vm9mA439/MFoVx3GtmnUsQwwFmNsvMXgqPy4gsjoeZfSf8myw3s3vMrGupjoeZ3Wlmm81sed6ygsfAAjeF79vnzWxoynFcH/5tnjez/zGzA/JemxzGsdLMGuLsO/OkEI4LcQtwGnA0cF44fkTadgDfdfejgOHAZeF+JwHz3H0QMC+cL4VvASvy5q8DbgjjeItggJ20/RJ42N2PBI4L4ynp8TCzeuAKYFg4+FAdwVgipToev2bPcU5aOganETxycBDBQ4inpBxHacZbcfdM/wEjgLl585OByRnE8QDwOWAl0Ctc1gtYWYJ99yF4s30GeJDgqdhvAB0LHaOUYtgfWEvYz5S3vKTHg93DBPQkeFzgg0BDKY8H0A9Y3toxAH4FnFdovTTiaPbal4C7w+nIZwaYC4wodr+ZVwqUwVgR4WA3Q4BFwCHuvgkg/HlwCUK4Efg+0BTOHwi87e47wvlSHJMBwBbgrrAZc4eZdafEx8PdNwA/A9YBm4B3gKWU/njka+kYZPneTW28lXJICm0eKyKVnZvtC/we+La7/71U+83bf26czqX5iwusmvYx6QgMBaa4+xCCy85L1XTaJWyvjwX6A72B7gRlenPlcNosk/dunPFW2qIckkKbx4pImpl1IkgId7v7feHi182sV/h6L2BzymF8EjjdzF4FZhA0IW4EDjCz3NO2S3FMGoFGd18Uzs8iSBKlPh6fBda6+xZ33w7cB4yk9McjX0vHoOTv3bzxVs73sK2QdBzlkBQWA4PC3uXOBB0ms9PeqQXPpp8GrHD3X+S9NBsYF06PI+hrSI27T3b3Pu7ej+D//id3Px+Yz+4xOksRx9+A9WZ2RLhoFMGj+kt6PAiaDcPNrFv4N8rFUdLj0UxLx2A28NXwLMRw4J1cMyMNJRtvJc1Oo3Z0qIwm6E19BbiqRPs8maDEeh54Nvw3mqA9Pw9YFf7sWcLjcArwYDg9IPzDrgb+G+hSgv0fDywJj8n9QI8sjgdwDfASsBz4LcEYIyU5HsA9BH0Z2wm+gce3dAwIyvZbwvftXwnOmKQZx2qCvoPc+/W2vPWvCuNYCZwWZ9+6olFEIsqh+SAiZURJQUQilBREJEJJQUQilBREJEJJQUQilBREJEJJQUQi/h/ljRsnyKclHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_test_data = list(minibatchData(test_images,None,img_size,batch_num=2))\n",
    "#print cnn_train_data.size\n",
    "plt.imshow(cnn_test_data[0][0][1][0])\n",
    "\n",
    "img_size_small=(128,128)\n",
    "small_cnn_train_data = list(minibatchData(train_images,train_labels_encoded,img_size_small,channel_num=1))\n",
    "small_cnn_test_data = list(minibatchData(test_images,None,img_size_small,channel_num=1,batch_num=2))\n",
    "\n",
    "plt.imshow(small_cnn_test_data[0][0][1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(4, 4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        #self.conv3 = nn.Conv2d(32, 64, 4)\n",
    "        self.fc1 = nn.Linear(32*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 99)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print x.shape\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print x.shape\n",
    "        x=self.dropout(x)\n",
    "        x = x.view(-1, 32*6*6)\n",
    "        #print x.shape\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #print x.shape\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 0.139\n",
      "[2,    30] loss: 0.139\n",
      "[3,    30] loss: 0.139\n",
      "[4,    30] loss: 0.139\n",
      "[5,    30] loss: 0.139\n",
      "[6,    30] loss: 0.139\n",
      "[7,    30] loss: 0.139\n",
      "[8,    30] loss: 0.139\n",
      "[9,    30] loss: 0.139\n",
      "[10,    30] loss: 0.139\n",
      "[11,    30] loss: 0.139\n",
      "[12,    30] loss: 0.139\n",
      "[13,    30] loss: 0.139\n",
      "[14,    30] loss: 0.139\n",
      "[15,    30] loss: 0.139\n",
      "[16,    30] loss: 0.139\n",
      "[17,    30] loss: 0.139\n",
      "[18,    30] loss: 0.139\n",
      "[19,    30] loss: 0.138\n",
      "[20,    30] loss: 0.138\n",
      "[21,    30] loss: 0.138\n",
      "[22,    30] loss: 0.138\n",
      "[23,    30] loss: 0.138\n",
      "[24,    30] loss: 0.137\n",
      "[25,    30] loss: 0.137\n",
      "[26,    30] loss: 0.136\n",
      "[27,    30] loss: 0.135\n",
      "[28,    30] loss: 0.133\n",
      "[29,    30] loss: 0.131\n",
      "[30,    30] loss: 0.128\n",
      "[31,    30] loss: 0.126\n",
      "[32,    30] loss: 0.122\n",
      "[33,    30] loss: 0.116\n",
      "[34,    30] loss: 0.112\n",
      "[35,    30] loss: 0.105\n",
      "[36,    30] loss: 0.101\n",
      "[37,    30] loss: 0.096\n",
      "[38,    30] loss: 0.092\n",
      "[39,    30] loss: 0.088\n",
      "[40,    30] loss: 0.087\n",
      "[41,    30] loss: 0.083\n",
      "[42,    30] loss: 0.079\n",
      "[43,    30] loss: 0.077\n",
      "[44,    30] loss: 0.075\n",
      "[45,    30] loss: 0.073\n",
      "[46,    30] loss: 0.070\n",
      "[47,    30] loss: 0.069\n",
      "[48,    30] loss: 0.067\n",
      "[49,    30] loss: 0.065\n",
      "[50,    30] loss: 0.063\n",
      "[51,    30] loss: 0.061\n",
      "[52,    30] loss: 0.061\n",
      "[53,    30] loss: 0.060\n",
      "[54,    30] loss: 0.058\n",
      "[55,    30] loss: 0.056\n",
      "[56,    30] loss: 0.054\n",
      "[57,    30] loss: 0.054\n",
      "[58,    30] loss: 0.053\n",
      "[59,    30] loss: 0.052\n",
      "[60,    30] loss: 0.051\n",
      "[61,    30] loss: 0.050\n",
      "[62,    30] loss: 0.050\n",
      "[63,    30] loss: 0.049\n",
      "[64,    30] loss: 0.049\n",
      "[65,    30] loss: 0.049\n",
      "[66,    30] loss: 0.046\n",
      "[67,    30] loss: 0.046\n",
      "[68,    30] loss: 0.045\n",
      "[69,    30] loss: 0.044\n",
      "[70,    30] loss: 0.043\n",
      "[71,    30] loss: 0.044\n",
      "[72,    30] loss: 0.042\n",
      "[73,    30] loss: 0.042\n",
      "[74,    30] loss: 0.041\n",
      "[75,    30] loss: 0.041\n",
      "[76,    30] loss: 0.040\n",
      "[77,    30] loss: 0.040\n",
      "[78,    30] loss: 0.038\n",
      "[79,    30] loss: 0.040\n",
      "[80,    30] loss: 0.037\n",
      "[81,    30] loss: 0.039\n",
      "[82,    30] loss: 0.037\n",
      "[83,    30] loss: 0.037\n",
      "[84,    30] loss: 0.037\n",
      "[85,    30] loss: 0.035\n",
      "[86,    30] loss: 0.035\n",
      "[87,    30] loss: 0.035\n",
      "[88,    30] loss: 0.034\n",
      "[89,    30] loss: 0.032\n",
      "[90,    30] loss: 0.034\n",
      "[91,    30] loss: 0.032\n",
      "[92,    30] loss: 0.032\n",
      "[93,    30] loss: 0.032\n",
      "[94,    30] loss: 0.033\n",
      "[95,    30] loss: 0.031\n",
      "[96,    30] loss: 0.031\n",
      "[97,    30] loss: 0.030\n",
      "[98,    30] loss: 0.028\n",
      "[99,    30] loss: 0.030\n",
      "[100,    30] loss: 0.030\n",
      "('Finished Training in:', 402.57535886764526)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(small_cnn_train_data,0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        #print inputs\n",
    "        #print inputs.shape\n",
    "        optimizer_ft.zero_grad()\n",
    "       # print inputs.type('torch.DoubleTensor')\n",
    "        #inputs=inputs.type('torch.DoubleTensor')\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print outputs.shape\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 30 == 29:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 990))   \n",
    "            running_loss = 0.0\n",
    "end_time = time.time()\n",
    "\n",
    "print('Finished Training in:',end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 99)\n"
     ]
    }
   ],
   "source": [
    "small_probs_cnn_train=np.empty([0,99])\n",
    "small_probs_cnn_test=np.empty([0,99])\n",
    "sm = torch.nn.Softmax()\n",
    "for data in small_cnn_train_data:\n",
    "    images,labels=data\n",
    "    small_probs_cnn_train=np.append(small_probs_cnn_train,sm(net(images)).data.numpy(),axis=0)\n",
    "print small_probs_cnn_train.shape\n",
    "    \n",
    "for data in small_cnn_test_data:\n",
    "    images, labels = data\n",
    "    small_probs_cnn_test=np.append(small_probs_cnn_test,sm(net(images)).data.numpy(),axis=0)\n",
    "    #outputs=np.append(outputs,net(images).data.numpy(),axis=0)\n",
    "    #print probs\n",
    "    \n",
    "\n",
    "print small_probs_cnn_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved pretrained AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='tunedAlex.sav'\n",
    "model_ft = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 99)\n"
     ]
    }
   ],
   "source": [
    "probs_cnn_train=np.empty([0,99])\n",
    "#sm = torch.nn.Softmax()\n",
    "for data in cnn_train_data:\n",
    "    images,labels=data\n",
    "    probs_cnn_train=np.append(probs_cnn_train,(model_ft(images)).data.numpy(),axis=0)\n",
    "print probs_cnn_train.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 99)\n"
     ]
    }
   ],
   "source": [
    "probs_cnn_test=np.empty([0,99])\n",
    "for data in cnn_test_data:\n",
    "    images, labels = data\n",
    "    probs_cnn_test=np.append(probs_cnn_test,(model_ft(images)).data.numpy(),axis=0)\n",
    "    #outputs=np.append(outputs,net(images).data.numpy(),axis=0)\n",
    "    #print probs\n",
    "    \n",
    "\n",
    "print probs_cnn_test.shape\n",
    "\n",
    "#sm = torch.nn.Softmax()\n",
    "#probabilities = sm(output) \n",
    "#print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the 3 histograms\n",
    "train_margin_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'margin1':'margin64']).values\n",
    "train_shape_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'shape1':'shape64']).values\n",
    "train_texture_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'texture1':'texture64']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_margin_data=((pd.read_csv('data/test.csv')).loc[:,'margin1':'margin64']).values\n",
    "test_shape_data=((pd.read_csv('data/test.csv')).loc[:,'shape1':'shape64']).values\n",
    "test_texture_data=((pd.read_csv('data/test.csv')).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "#print train_margin_data.head()\n",
    "#print train_shape_data.head()\n",
    "#print train_texture_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute descriptors, clusters and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(images, dense=False):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    des_per_Img = np.array([sift.detectAndCompute(img,None)[1] for img in images])\n",
    "    return des_per_Img\n",
    "        \n",
    "def get_clusters(descriptors, vocabSize):\n",
    "    des_list = np.concatenate(descriptors)\n",
    "\n",
    "    kmeans = MiniBatchKMeans(vocabSize, batch_size=100)\n",
    "    kmeans.fit(np.array(des_list))\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def get_vocabulary(descriptors, clusters, vocabSize):\n",
    "    return np.array([normalize(np.histogram(clusters.predict(dscrs), bins=range(vocabSize))[0].reshape(1,-1)).ravel() for dscrs in descriptors])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors computed in 135.876172 seconds\n"
     ]
    }
   ],
   "source": [
    "des_start_time =time.time()\n",
    "des_list_train = get_descriptor(train_images)\n",
    "\n",
    "des_list_test = get_descriptor(test_images)\n",
    "des_end_time =time.time()\n",
    "print \"Descriptors computed in {:2f} seconds\".format(des_end_time-des_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed in 0.655427 seconds\n"
     ]
    }
   ],
   "source": [
    "clustering_start_time=time.time()\n",
    "clusters = get_clusters(des_list_train,150)\n",
    "clustering_end_time=time.time()\n",
    "print \"Clustering completed in {:2f} seconds\".format(clustering_end_time-clustering_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 149)\n"
     ]
    }
   ],
   "source": [
    "vocab_train = get_vocabulary(des_list_train,clusters,150)\n",
    "vocab_test = get_vocabulary(des_list_test,clusters,150)\n",
    "\n",
    "print vocab_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "from keras.utils import np_utils\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_shape = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dense(99, activation='softmax')])\n",
    "deep_texture = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dense(99, activation='softmax')])\n",
    "deep_margin = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dense(99, activation='softmax')])\n",
    "deep_sift = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dense(99, activation='softmax')])\n",
    "deep_alex = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dense(99, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_shape.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_texture.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_margin.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_sift.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_alex.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 99)\n",
      "(990, 99)\n",
      "Epoch 1/150\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 5.0683\n",
      "Epoch 2/150\n",
      "990/990 [==============================] - 0s 294us/step - loss: 2.6370\n",
      "Epoch 3/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 1.5924\n",
      "Epoch 4/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 1.0434\n",
      "Epoch 5/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.7516\n",
      "Epoch 6/150\n",
      "990/990 [==============================] - 0s 296us/step - loss: 0.6434\n",
      "Epoch 7/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.5290\n",
      "Epoch 8/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.4260\n",
      "Epoch 9/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.3686\n",
      "Epoch 10/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 0.3618\n",
      "Epoch 11/150\n",
      "990/990 [==============================] - 0s 294us/step - loss: 0.3145\n",
      "Epoch 12/150\n",
      "990/990 [==============================] - 0s 369us/step - loss: 0.3128\n",
      "Epoch 13/150\n",
      "990/990 [==============================] - 0s 385us/step - loss: 0.2530\n",
      "Epoch 14/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.2412\n",
      "Epoch 15/150\n",
      "990/990 [==============================] - 0s 368us/step - loss: 0.2163\n",
      "Epoch 16/150\n",
      "990/990 [==============================] - 0s 349us/step - loss: 0.2195\n",
      "Epoch 17/150\n",
      "990/990 [==============================] - 0s 431us/step - loss: 0.1741\n",
      "Epoch 18/150\n",
      "990/990 [==============================] - 0s 378us/step - loss: 0.2199\n",
      "Epoch 19/150\n",
      "990/990 [==============================] - 0s 322us/step - loss: 0.1655\n",
      "Epoch 20/150\n",
      "990/990 [==============================] - 0s 302us/step - loss: 0.1739\n",
      "Epoch 21/150\n",
      "990/990 [==============================] - 0s 366us/step - loss: 0.1716\n",
      "Epoch 22/150\n",
      "990/990 [==============================] - 0s 300us/step - loss: 0.1828\n",
      "Epoch 23/150\n",
      "990/990 [==============================] - 0s 351us/step - loss: 0.1668\n",
      "Epoch 24/150\n",
      "990/990 [==============================] - 0s 331us/step - loss: 0.1466\n",
      "Epoch 25/150\n",
      "990/990 [==============================] - 0s 339us/step - loss: 0.1585\n",
      "Epoch 26/150\n",
      "990/990 [==============================] - 0s 422us/step - loss: 0.1219\n",
      "Epoch 27/150\n",
      "990/990 [==============================] - 0s 348us/step - loss: 0.1408\n",
      "Epoch 28/150\n",
      "990/990 [==============================] - 0s 371us/step - loss: 0.1332\n",
      "Epoch 29/150\n",
      "990/990 [==============================] - 0s 395us/step - loss: 0.1115\n",
      "Epoch 30/150\n",
      "990/990 [==============================] - 0s 410us/step - loss: 0.1291\n",
      "Epoch 31/150\n",
      "990/990 [==============================] - 0s 382us/step - loss: 0.1150\n",
      "Epoch 32/150\n",
      "990/990 [==============================] - 0s 382us/step - loss: 0.1467\n",
      "Epoch 33/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.1199\n",
      "Epoch 34/150\n",
      "990/990 [==============================] - 0s 369us/step - loss: 0.1316\n",
      "Epoch 35/150\n",
      "990/990 [==============================] - 0s 401us/step - loss: 0.1717\n",
      "Epoch 36/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.1597\n",
      "Epoch 37/150\n",
      "990/990 [==============================] - 0s 379us/step - loss: 0.1403\n",
      "Epoch 38/150\n",
      "990/990 [==============================] - 0s 312us/step - loss: 0.1406\n",
      "Epoch 39/150\n",
      "990/990 [==============================] - 0s 399us/step - loss: 0.0914\n",
      "Epoch 40/150\n",
      "990/990 [==============================] - 0s 357us/step - loss: 0.1341\n",
      "Epoch 41/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.1142\n",
      "Epoch 42/150\n",
      "990/990 [==============================] - 0s 339us/step - loss: 0.1269\n",
      "Epoch 43/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.1120\n",
      "Epoch 44/150\n",
      "990/990 [==============================] - 0s 371us/step - loss: 0.1039\n",
      "Epoch 45/150\n",
      "990/990 [==============================] - 0s 453us/step - loss: 0.0984\n",
      "Epoch 46/150\n",
      "990/990 [==============================] - 0s 442us/step - loss: 0.1139\n",
      "Epoch 47/150\n",
      "990/990 [==============================] - 0s 359us/step - loss: 0.1482\n",
      "Epoch 48/150\n",
      "990/990 [==============================] - 0s 439us/step - loss: 0.1240\n",
      "Epoch 49/150\n",
      "990/990 [==============================] - 0s 358us/step - loss: 0.1474\n",
      "Epoch 50/150\n",
      "990/990 [==============================] - 0s 431us/step - loss: 0.1072\n",
      "Epoch 51/150\n",
      "990/990 [==============================] - 0s 462us/step - loss: 0.1069 0s - loss:\n",
      "Epoch 52/150\n",
      "990/990 [==============================] - 0s 461us/step - loss: 0.0951\n",
      "Epoch 53/150\n",
      "990/990 [==============================] - 0s 413us/step - loss: 0.0876\n",
      "Epoch 54/150\n",
      "990/990 [==============================] - 0s 475us/step - loss: 0.1065\n",
      "Epoch 55/150\n",
      "990/990 [==============================] - 0s 411us/step - loss: 0.1434\n",
      "Epoch 56/150\n",
      "990/990 [==============================] - 0s 380us/step - loss: 0.1008\n",
      "Epoch 57/150\n",
      "990/990 [==============================] - 0s 365us/step - loss: 0.1098\n",
      "Epoch 58/150\n",
      "990/990 [==============================] - 0s 388us/step - loss: 0.1083\n",
      "Epoch 59/150\n",
      "990/990 [==============================] - 0s 355us/step - loss: 0.0731\n",
      "Epoch 60/150\n",
      "990/990 [==============================] - 0s 377us/step - loss: 0.0857\n",
      "Epoch 61/150\n",
      "990/990 [==============================] - 0s 418us/step - loss: 0.0880\n",
      "Epoch 62/150\n",
      "990/990 [==============================] - 0s 415us/step - loss: 0.0980\n",
      "Epoch 63/150\n",
      "990/990 [==============================] - 0s 365us/step - loss: 0.1053\n",
      "Epoch 64/150\n",
      "990/990 [==============================] - 0s 339us/step - loss: 0.1342\n",
      "Epoch 65/150\n",
      "990/990 [==============================] - 0s 297us/step - loss: 0.0732\n",
      "Epoch 66/150\n",
      "990/990 [==============================] - 0s 312us/step - loss: 0.0952\n",
      "Epoch 67/150\n",
      "990/990 [==============================] - 0s 298us/step - loss: 0.1081\n",
      "Epoch 68/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.1264\n",
      "Epoch 69/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.1054\n",
      "Epoch 70/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.1025\n",
      "Epoch 71/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.1299\n",
      "Epoch 72/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.1027\n",
      "Epoch 73/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.1092\n",
      "Epoch 74/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 0.0447\n",
      "Epoch 75/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0956\n",
      "Epoch 76/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.1065\n",
      "Epoch 77/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.0917\n",
      "Epoch 78/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.1225\n",
      "Epoch 79/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.1054\n",
      "Epoch 80/150\n",
      "990/990 [==============================] - 0s 298us/step - loss: 0.0872\n",
      "Epoch 81/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.0861\n",
      "Epoch 82/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.0704\n",
      "Epoch 83/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 0.1163\n",
      "Epoch 84/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0908\n",
      "Epoch 85/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.0957\n",
      "Epoch 86/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 0.0737\n",
      "Epoch 87/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.1176\n",
      "Epoch 88/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 0.0982\n",
      "Epoch 89/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.0936\n",
      "Epoch 90/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.1172\n",
      "Epoch 91/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0950\n",
      "Epoch 92/150\n",
      "990/990 [==============================] - 0s 298us/step - loss: 0.0978\n",
      "Epoch 93/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.1160\n",
      "Epoch 94/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.1169\n",
      "Epoch 95/150\n",
      "990/990 [==============================] - 0s 294us/step - loss: 0.0675\n",
      "Epoch 96/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.1206\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 0s 287us/step - loss: 0.0780\n",
      "Epoch 98/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.1099\n",
      "Epoch 99/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.0974\n",
      "Epoch 100/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.0929\n",
      "Epoch 101/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.1066\n",
      "Epoch 102/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.0652\n",
      "Epoch 103/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.1114\n",
      "Epoch 104/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.1135\n",
      "Epoch 105/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.0705\n",
      "Epoch 106/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.0347\n",
      "Epoch 107/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.0600\n",
      "Epoch 108/150\n",
      "990/990 [==============================] - 0s 293us/step - loss: 0.0371\n",
      "Epoch 109/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.0594\n",
      "Epoch 110/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.0662\n",
      "Epoch 111/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.0669\n",
      "Epoch 112/150\n",
      "990/990 [==============================] - 0s 371us/step - loss: 0.0746\n",
      "Epoch 113/150\n",
      "990/990 [==============================] - 0s 341us/step - loss: 0.0738\n",
      "Epoch 114/150\n",
      "990/990 [==============================] - 0s 376us/step - loss: 0.0818\n",
      "Epoch 115/150\n",
      "990/990 [==============================] - 0s 364us/step - loss: 0.0961\n",
      "Epoch 116/150\n",
      "990/990 [==============================] - 0s 327us/step - loss: 0.0851\n",
      "Epoch 117/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 0.0979\n",
      "Epoch 118/150\n",
      "990/990 [==============================] - 0s 307us/step - loss: 0.0733\n",
      "Epoch 119/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.0534\n",
      "Epoch 120/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.0864\n",
      "Epoch 121/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.0949\n",
      "Epoch 122/150\n",
      "990/990 [==============================] - 0s 310us/step - loss: 0.0679\n",
      "Epoch 123/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.0770\n",
      "Epoch 124/150\n",
      "990/990 [==============================] - 0s 378us/step - loss: 0.0676\n",
      "Epoch 125/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 0.0994\n",
      "Epoch 126/150\n",
      "990/990 [==============================] - 0s 371us/step - loss: 0.0739\n",
      "Epoch 127/150\n",
      "990/990 [==============================] - 0s 365us/step - loss: 0.0856\n",
      "Epoch 128/150\n",
      "990/990 [==============================] - 0s 339us/step - loss: 0.0880\n",
      "Epoch 129/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.1152\n",
      "Epoch 130/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.1103\n",
      "Epoch 131/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.1130\n",
      "Epoch 132/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0663\n",
      "Epoch 133/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.0841\n",
      "Epoch 134/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.0506\n",
      "Epoch 135/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0832\n",
      "Epoch 136/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.1066\n",
      "Epoch 137/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0633\n",
      "Epoch 138/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.0524\n",
      "Epoch 139/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 0.0974\n",
      "Epoch 140/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0902\n",
      "Epoch 141/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.1060\n",
      "Epoch 142/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 0.0559\n",
      "Epoch 143/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.0542\n",
      "Epoch 144/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0736\n",
      "Epoch 145/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.0977\n",
      "Epoch 146/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.0703\n",
      "Epoch 147/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.0964\n",
      "Epoch 148/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.1274\n",
      "Epoch 149/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.0966\n",
      "Epoch 150/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.0832\n",
      "Epoch 1/150\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 4.5977\n",
      "Epoch 2/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 4.5964\n",
      "Epoch 3/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 4.5964\n",
      "Epoch 4/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 4.5964\n",
      "Epoch 5/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 4.5961\n",
      "Epoch 6/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 4.5960\n",
      "Epoch 7/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 4.5962\n",
      "Epoch 8/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 4.5960\n",
      "Epoch 9/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 4.5964\n",
      "Epoch 10/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 4.5963\n",
      "Epoch 11/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 4.5960\n",
      "Epoch 12/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 4.5957\n",
      "Epoch 13/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 4.5955\n",
      "Epoch 14/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 4.5937\n",
      "Epoch 15/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 4.5924\n",
      "Epoch 16/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 4.5853\n",
      "Epoch 17/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 4.5707\n",
      "Epoch 18/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 4.5470\n",
      "Epoch 19/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 4.5019\n",
      "Epoch 20/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 4.4431\n",
      "Epoch 21/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 4.3603\n",
      "Epoch 22/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 4.2862\n",
      "Epoch 23/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 4.2176\n",
      "Epoch 24/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 4.1376\n",
      "Epoch 25/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 4.0842\n",
      "Epoch 26/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 4.0205\n",
      "Epoch 27/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.9803\n",
      "Epoch 28/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.9439\n",
      "Epoch 29/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.9140\n",
      "Epoch 30/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.8831\n",
      "Epoch 31/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 3.8477\n",
      "Epoch 32/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.8346\n",
      "Epoch 33/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.8270\n",
      "Epoch 34/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.8005\n",
      "Epoch 35/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.7959\n",
      "Epoch 36/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.7752\n",
      "Epoch 37/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.7768\n",
      "Epoch 38/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.7505\n",
      "Epoch 39/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.7417\n",
      "Epoch 40/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.7365\n",
      "Epoch 41/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.7301\n",
      "Epoch 42/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.7141\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 0s 284us/step - loss: 3.6967\n",
      "Epoch 44/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.6988\n",
      "Epoch 45/150\n",
      "990/990 [==============================] - 0s 277us/step - loss: 3.7050\n",
      "Epoch 46/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 3.6678\n",
      "Epoch 47/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6984\n",
      "Epoch 48/150\n",
      "990/990 [==============================] - 0s 276us/step - loss: 3.7004\n",
      "Epoch 49/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6784\n",
      "Epoch 50/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.6753\n",
      "Epoch 51/150\n",
      "990/990 [==============================] - 0s 276us/step - loss: 3.6420\n",
      "Epoch 52/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6519\n",
      "Epoch 53/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 3.6636\n",
      "Epoch 54/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6637\n",
      "Epoch 55/150\n",
      "990/990 [==============================] - 0s 276us/step - loss: 3.6607\n",
      "Epoch 56/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.6517\n",
      "Epoch 57/150\n",
      "990/990 [==============================] - 0s 295us/step - loss: 3.6213\n",
      "Epoch 58/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6320\n",
      "Epoch 59/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6346\n",
      "Epoch 60/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6442\n",
      "Epoch 61/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.6370\n",
      "Epoch 62/150\n",
      "990/990 [==============================] - 0s 276us/step - loss: 3.6164\n",
      "Epoch 63/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.6215\n",
      "Epoch 64/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.6198\n",
      "Epoch 65/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.6051\n",
      "Epoch 66/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.6123\n",
      "Epoch 67/150\n",
      "990/990 [==============================] - 0s 342us/step - loss: 3.5946\n",
      "Epoch 68/150\n",
      "990/990 [==============================] - 0s 339us/step - loss: 3.6045\n",
      "Epoch 69/150\n",
      "990/990 [==============================] - 0s 331us/step - loss: 3.6284\n",
      "Epoch 70/150\n",
      "990/990 [==============================] - 0s 336us/step - loss: 3.6285\n",
      "Epoch 71/150\n",
      "990/990 [==============================] - 0s 341us/step - loss: 3.6103\n",
      "Epoch 72/150\n",
      "990/990 [==============================] - 0s 312us/step - loss: 3.6074\n",
      "Epoch 73/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 3.5906\n",
      "Epoch 74/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.6011\n",
      "Epoch 75/150\n",
      "990/990 [==============================] - 0s 306us/step - loss: 3.5772\n",
      "Epoch 76/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.6119\n",
      "Epoch 77/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.5952\n",
      "Epoch 78/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5906\n",
      "Epoch 79/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 3.5984\n",
      "Epoch 80/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.5905\n",
      "Epoch 81/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5821\n",
      "Epoch 82/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.5721\n",
      "Epoch 83/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5877\n",
      "Epoch 84/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.5713\n",
      "Epoch 85/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.5751\n",
      "Epoch 86/150\n",
      "990/990 [==============================] - 0s 304us/step - loss: 3.5900\n",
      "Epoch 87/150\n",
      "990/990 [==============================] - 0s 428us/step - loss: 3.5656\n",
      "Epoch 88/150\n",
      "990/990 [==============================] - 0s 303us/step - loss: 3.5448\n",
      "Epoch 89/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.5712\n",
      "Epoch 90/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 3.5717\n",
      "Epoch 91/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.5674\n",
      "Epoch 92/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5287\n",
      "Epoch 93/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 3.5596\n",
      "Epoch 94/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.5727\n",
      "Epoch 95/150\n",
      "990/990 [==============================] - 0s 305us/step - loss: 3.5585\n",
      "Epoch 96/150\n",
      "990/990 [==============================] - 0s 296us/step - loss: 3.5739\n",
      "Epoch 97/150\n",
      "990/990 [==============================] - 0s 300us/step - loss: 3.5397\n",
      "Epoch 98/150\n",
      "990/990 [==============================] - 0s 305us/step - loss: 3.5666\n",
      "Epoch 99/150\n",
      "990/990 [==============================] - 0s 299us/step - loss: 3.5385\n",
      "Epoch 100/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.5536\n",
      "Epoch 101/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 3.5586\n",
      "Epoch 102/150\n",
      "990/990 [==============================] - 0s 298us/step - loss: 3.5453\n",
      "Epoch 103/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.5402\n",
      "Epoch 104/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 3.5354\n",
      "Epoch 105/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.5280\n",
      "Epoch 106/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5463\n",
      "Epoch 107/150\n",
      "990/990 [==============================] - 0s 293us/step - loss: 3.5420\n",
      "Epoch 108/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.5271\n",
      "Epoch 109/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.5270\n",
      "Epoch 110/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 3.5122\n",
      "Epoch 111/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5705\n",
      "Epoch 112/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5352\n",
      "Epoch 113/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 3.5499\n",
      "Epoch 114/150\n",
      "990/990 [==============================] - 0s 296us/step - loss: 3.5417\n",
      "Epoch 115/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5575\n",
      "Epoch 116/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5373\n",
      "Epoch 117/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5205\n",
      "Epoch 118/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5355\n",
      "Epoch 119/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 3.5429\n",
      "Epoch 120/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 3.5384\n",
      "Epoch 121/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.5309\n",
      "Epoch 122/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.5376\n",
      "Epoch 123/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.5303\n",
      "Epoch 124/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.5076\n",
      "Epoch 125/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 3.5238\n",
      "Epoch 126/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.5267\n",
      "Epoch 127/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 3.4991\n",
      "Epoch 128/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 3.5251\n",
      "Epoch 129/150\n",
      "990/990 [==============================] - 0s 298us/step - loss: 3.5044\n",
      "Epoch 130/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 3.5168\n",
      "Epoch 131/150\n",
      "990/990 [==============================] - 0s 340us/step - loss: 3.5324\n",
      "Epoch 132/150\n",
      "990/990 [==============================] - 0s 343us/step - loss: 3.5091\n",
      "Epoch 133/150\n",
      "990/990 [==============================] - 0s 391us/step - loss: 3.5159\n",
      "Epoch 134/150\n",
      "990/990 [==============================] - 0s 434us/step - loss: 3.5309\n",
      "Epoch 135/150\n",
      "990/990 [==============================] - 0s 340us/step - loss: 3.5135\n",
      "Epoch 136/150\n",
      "990/990 [==============================] - 0s 338us/step - loss: 3.5193\n",
      "Epoch 137/150\n",
      "990/990 [==============================] - 0s 350us/step - loss: 3.4996\n",
      "Epoch 138/150\n",
      "990/990 [==============================] - 0s 354us/step - loss: 3.5004\n",
      "Epoch 139/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 0s 347us/step - loss: 3.5106\n",
      "Epoch 140/150\n",
      "990/990 [==============================] - 0s 310us/step - loss: 3.5284\n",
      "Epoch 141/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 3.5205\n",
      "Epoch 142/150\n",
      "990/990 [==============================] - 0s 304us/step - loss: 3.5176\n",
      "Epoch 143/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.4953\n",
      "Epoch 144/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.5068\n",
      "Epoch 145/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.5226\n",
      "Epoch 146/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 3.4786\n",
      "Epoch 147/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 3.5287\n",
      "Epoch 148/150\n",
      "990/990 [==============================] - 0s 295us/step - loss: 3.4854\n",
      "Epoch 149/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 3.5109\n",
      "Epoch 150/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 3.4976\n",
      "Epoch 1/150\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 4.5842\n",
      "Epoch 2/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 4.4843\n",
      "Epoch 3/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 4.0990\n",
      "Epoch 4/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 3.5103\n",
      "Epoch 5/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 3.0333\n",
      "Epoch 6/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 2.7225\n",
      "Epoch 7/150\n",
      "990/990 [==============================] - 0s 277us/step - loss: 2.4591\n",
      "Epoch 8/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 2.2437\n",
      "Epoch 9/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 2.1022\n",
      "Epoch 10/150\n",
      "990/990 [==============================] - 0s 293us/step - loss: 1.9957\n",
      "Epoch 11/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 1.8964\n",
      "Epoch 12/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 1.7824\n",
      "Epoch 13/150\n",
      "990/990 [==============================] - 0s 312us/step - loss: 1.6775\n",
      "Epoch 14/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 1.6092\n",
      "Epoch 15/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 1.5812\n",
      "Epoch 16/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 1.5435\n",
      "Epoch 17/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.4777\n",
      "Epoch 18/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 1.3815\n",
      "Epoch 19/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 1.3398\n",
      "Epoch 20/150\n",
      "990/990 [==============================] - 0s 296us/step - loss: 1.3112\n",
      "Epoch 21/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 1.2892\n",
      "Epoch 22/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 1.2212\n",
      "Epoch 23/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 1.2113\n",
      "Epoch 24/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 1.1626\n",
      "Epoch 25/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 1.1582\n",
      "Epoch 26/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 1.0918\n",
      "Epoch 27/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 1.1116\n",
      "Epoch 28/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 1.0595\n",
      "Epoch 29/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 1.0124\n",
      "Epoch 30/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 1.0055\n",
      "Epoch 31/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 1.0336\n",
      "Epoch 32/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.9739\n",
      "Epoch 33/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.9406\n",
      "Epoch 34/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.9310\n",
      "Epoch 35/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.9173\n",
      "Epoch 36/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.8960\n",
      "Epoch 37/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.9017\n",
      "Epoch 38/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.8476\n",
      "Epoch 39/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.8237\n",
      "Epoch 40/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.8511\n",
      "Epoch 41/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.8229\n",
      "Epoch 42/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7789\n",
      "Epoch 43/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.7746\n",
      "Epoch 44/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7744\n",
      "Epoch 45/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.7765\n",
      "Epoch 46/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.7361\n",
      "Epoch 47/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7158\n",
      "Epoch 48/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.7048\n",
      "Epoch 49/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.6757\n",
      "Epoch 50/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.6854\n",
      "Epoch 51/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 0.6618\n",
      "Epoch 52/150\n",
      "990/990 [==============================] - 0s 277us/step - loss: 0.6871\n",
      "Epoch 53/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.6882\n",
      "Epoch 54/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.6370\n",
      "Epoch 55/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.6729\n",
      "Epoch 56/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.6414\n",
      "Epoch 57/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.6354\n",
      "Epoch 58/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.6181\n",
      "Epoch 59/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.6231\n",
      "Epoch 60/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.6055\n",
      "Epoch 61/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.5844\n",
      "Epoch 62/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.5751\n",
      "Epoch 63/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.5807\n",
      "Epoch 64/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.5791\n",
      "Epoch 65/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.5532\n",
      "Epoch 66/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.5437\n",
      "Epoch 67/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.5472\n",
      "Epoch 68/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.5061\n",
      "Epoch 69/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.5536\n",
      "Epoch 70/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.5222\n",
      "Epoch 71/150\n",
      "990/990 [==============================] - 0s 294us/step - loss: 0.5153\n",
      "Epoch 72/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.4869\n",
      "Epoch 73/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.4792\n",
      "Epoch 74/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.5204\n",
      "Epoch 75/150\n",
      "990/990 [==============================] - 0s 292us/step - loss: 0.4540\n",
      "Epoch 76/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.4833\n",
      "Epoch 77/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.4662\n",
      "Epoch 78/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4675\n",
      "Epoch 79/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.4673\n",
      "Epoch 80/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4546\n",
      "Epoch 81/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.4508\n",
      "Epoch 82/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.4190\n",
      "Epoch 83/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.4342\n",
      "Epoch 84/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.4663\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 0s 279us/step - loss: 0.4419\n",
      "Epoch 86/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.4274\n",
      "Epoch 87/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.4164\n",
      "Epoch 88/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.4006\n",
      "Epoch 89/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.4075\n",
      "Epoch 90/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.4276\n",
      "Epoch 91/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.3993\n",
      "Epoch 92/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.3901\n",
      "Epoch 93/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.3837\n",
      "Epoch 94/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.3788\n",
      "Epoch 95/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.3884\n",
      "Epoch 96/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.3511\n",
      "Epoch 97/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.3921\n",
      "Epoch 98/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.3650\n",
      "Epoch 99/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.3461\n",
      "Epoch 100/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.3766\n",
      "Epoch 101/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.3545\n",
      "Epoch 102/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.3557\n",
      "Epoch 103/150\n",
      "990/990 [==============================] - 0s 277us/step - loss: 0.3541\n",
      "Epoch 104/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.3600\n",
      "Epoch 105/150\n",
      "990/990 [==============================] - 0s 277us/step - loss: 0.3463\n",
      "Epoch 106/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.3490\n",
      "Epoch 107/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.3105\n",
      "Epoch 108/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.3323\n",
      "Epoch 109/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.3256\n",
      "Epoch 110/150\n",
      "990/990 [==============================] - 0s 297us/step - loss: 0.3251\n",
      "Epoch 111/150\n",
      "990/990 [==============================] - 0s 303us/step - loss: 0.3045\n",
      "Epoch 112/150\n",
      "990/990 [==============================] - 0s 296us/step - loss: 0.3125\n",
      "Epoch 113/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.3378\n",
      "Epoch 114/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.2956\n",
      "Epoch 115/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2705\n",
      "Epoch 116/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.2928\n",
      "Epoch 117/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.2915\n",
      "Epoch 118/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.3079\n",
      "Epoch 119/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.2781\n",
      "Epoch 120/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.2744\n",
      "Epoch 121/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.3124\n",
      "Epoch 122/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.3070\n",
      "Epoch 123/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.2892\n",
      "Epoch 124/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2676\n",
      "Epoch 125/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2700\n",
      "Epoch 126/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.2571\n",
      "Epoch 127/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2734\n",
      "Epoch 128/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.2798\n",
      "Epoch 129/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2748\n",
      "Epoch 130/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.2647\n",
      "Epoch 131/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2598\n",
      "Epoch 132/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.2678\n",
      "Epoch 133/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2501\n",
      "Epoch 134/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 0.2445\n",
      "Epoch 135/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.2451\n",
      "Epoch 136/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.2481\n",
      "Epoch 137/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.2121\n",
      "Epoch 138/150\n",
      "990/990 [==============================] - 0s 296us/step - loss: 0.2424\n",
      "Epoch 139/150\n",
      "990/990 [==============================] - 0s 277us/step - loss: 0.2218\n",
      "Epoch 140/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.2450\n",
      "Epoch 141/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.2285\n",
      "Epoch 142/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.2382\n",
      "Epoch 143/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.2273\n",
      "Epoch 144/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.2294\n",
      "Epoch 145/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.2182\n",
      "Epoch 146/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.1956\n",
      "Epoch 147/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.2114\n",
      "Epoch 148/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.2016\n",
      "Epoch 149/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.1983\n",
      "Epoch 150/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.2196\n",
      "Epoch 1/150\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 4.5864\n",
      "Epoch 2/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 4.4543\n",
      "Epoch 3/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 3.9705\n",
      "Epoch 4/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 3.4922\n",
      "Epoch 5/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 3.1390\n",
      "Epoch 6/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 2.8618\n",
      "Epoch 7/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 2.6169\n",
      "Epoch 8/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 2.4004\n",
      "Epoch 9/150\n",
      "990/990 [==============================] - 0s 298us/step - loss: 2.2370\n",
      "Epoch 10/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 2.0915\n",
      "Epoch 11/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 1.9700\n",
      "Epoch 12/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.8769\n",
      "Epoch 13/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 1.7718\n",
      "Epoch 14/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 1.6933\n",
      "Epoch 15/150\n",
      "990/990 [==============================] - 0s 295us/step - loss: 1.5989\n",
      "Epoch 16/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.5453\n",
      "Epoch 17/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 1.5004\n",
      "Epoch 18/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 1.4556\n",
      "Epoch 19/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 1.4246\n",
      "Epoch 20/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.3812\n",
      "Epoch 21/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.3249\n",
      "Epoch 22/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 1.3225\n",
      "Epoch 23/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.2905\n",
      "Epoch 24/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.2775\n",
      "Epoch 25/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 1.2143\n",
      "Epoch 26/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 1.2150\n",
      "Epoch 27/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 1.2198\n",
      "Epoch 28/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 1.1627\n",
      "Epoch 29/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 1.1493\n",
      "Epoch 30/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 1.1247\n",
      "Epoch 31/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 0s 283us/step - loss: 1.1012\n",
      "Epoch 32/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 1.0994\n",
      "Epoch 33/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 1.0811\n",
      "Epoch 34/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 1.0438\n",
      "Epoch 35/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 1.0274\n",
      "Epoch 36/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 1.0461\n",
      "Epoch 37/150\n",
      "990/990 [==============================] - 0s 393us/step - loss: 1.0343\n",
      "Epoch 38/150\n",
      "990/990 [==============================] - 0s 347us/step - loss: 0.9924\n",
      "Epoch 39/150\n",
      "990/990 [==============================] - 0s 342us/step - loss: 0.9838\n",
      "Epoch 40/150\n",
      "990/990 [==============================] - 0s 341us/step - loss: 0.9620\n",
      "Epoch 41/150\n",
      "990/990 [==============================] - 0s 346us/step - loss: 0.9841\n",
      "Epoch 42/150\n",
      "990/990 [==============================] - 0s 343us/step - loss: 0.9430\n",
      "Epoch 43/150\n",
      "990/990 [==============================] - 0s 335us/step - loss: 0.9198\n",
      "Epoch 44/150\n",
      "990/990 [==============================] - 0s 338us/step - loss: 0.9196\n",
      "Epoch 45/150\n",
      "990/990 [==============================] - 0s 350us/step - loss: 0.9492\n",
      "Epoch 46/150\n",
      "990/990 [==============================] - 0s 330us/step - loss: 0.9327\n",
      "Epoch 47/150\n",
      "990/990 [==============================] - 0s 358us/step - loss: 0.9299\n",
      "Epoch 48/150\n",
      "990/990 [==============================] - 0s 347us/step - loss: 0.8845\n",
      "Epoch 49/150\n",
      "990/990 [==============================] - 0s 299us/step - loss: 0.8990\n",
      "Epoch 50/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.8821\n",
      "Epoch 51/150\n",
      "990/990 [==============================] - 0s 305us/step - loss: 0.8666\n",
      "Epoch 52/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.8761\n",
      "Epoch 53/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.8500\n",
      "Epoch 54/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.8522\n",
      "Epoch 55/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.8191\n",
      "Epoch 56/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.8544\n",
      "Epoch 57/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.8296\n",
      "Epoch 58/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.8451\n",
      "Epoch 59/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.8111\n",
      "Epoch 60/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.8159\n",
      "Epoch 61/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.7994\n",
      "Epoch 62/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7784\n",
      "Epoch 63/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7743\n",
      "Epoch 64/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7630\n",
      "Epoch 65/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7694\n",
      "Epoch 66/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.7511\n",
      "Epoch 67/150\n",
      "990/990 [==============================] - 0s 298us/step - loss: 0.7563\n",
      "Epoch 68/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.7262\n",
      "Epoch 69/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.7309\n",
      "Epoch 70/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.7325\n",
      "Epoch 71/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.7490\n",
      "Epoch 72/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.7076\n",
      "Epoch 73/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.6915\n",
      "Epoch 74/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.7061\n",
      "Epoch 75/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.6956\n",
      "Epoch 76/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.7108\n",
      "Epoch 77/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.6953\n",
      "Epoch 78/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.6576\n",
      "Epoch 79/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.6859\n",
      "Epoch 80/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.6550\n",
      "Epoch 81/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.6528\n",
      "Epoch 82/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.6347\n",
      "Epoch 83/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.6552\n",
      "Epoch 84/150\n",
      "990/990 [==============================] - 0s 289us/step - loss: 0.6086\n",
      "Epoch 85/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.6398\n",
      "Epoch 86/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.6413\n",
      "Epoch 87/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.6247\n",
      "Epoch 88/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.6197\n",
      "Epoch 89/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.5938\n",
      "Epoch 90/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.6164\n",
      "Epoch 91/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.5990\n",
      "Epoch 92/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.6221\n",
      "Epoch 93/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.6031\n",
      "Epoch 94/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.5947\n",
      "Epoch 95/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.5891\n",
      "Epoch 96/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.5551\n",
      "Epoch 97/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.6295\n",
      "Epoch 98/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.5704\n",
      "Epoch 99/150\n",
      "990/990 [==============================] - 0s 285us/step - loss: 0.5730\n",
      "Epoch 100/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.5678\n",
      "Epoch 101/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.5986\n",
      "Epoch 102/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.5594\n",
      "Epoch 103/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.5586\n",
      "Epoch 104/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.5621\n",
      "Epoch 105/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.5292\n",
      "Epoch 106/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.5303\n",
      "Epoch 107/150\n",
      "990/990 [==============================] - 0s 291us/step - loss: 0.5207\n",
      "Epoch 108/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.5048\n",
      "Epoch 109/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.5033\n",
      "Epoch 110/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.5115\n",
      "Epoch 111/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.5201\n",
      "Epoch 112/150\n",
      "990/990 [==============================] - 0s 288us/step - loss: 0.5107\n",
      "Epoch 113/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.5087\n",
      "Epoch 114/150\n",
      "990/990 [==============================] - 0s 297us/step - loss: 0.4964\n",
      "Epoch 115/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.5104\n",
      "Epoch 116/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.5264\n",
      "Epoch 117/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.4904\n",
      "Epoch 118/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.4789\n",
      "Epoch 119/150\n",
      "990/990 [==============================] - 0s 290us/step - loss: 0.4713\n",
      "Epoch 120/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.4984\n",
      "Epoch 121/150\n",
      "990/990 [==============================] - 0s 286us/step - loss: 0.5053\n",
      "Epoch 122/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4736\n",
      "Epoch 123/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4807\n",
      "Epoch 124/150\n",
      "990/990 [==============================] - 0s 287us/step - loss: 0.4510\n",
      "Epoch 125/150\n",
      "990/990 [==============================] - 0s 283us/step - loss: 0.4662\n",
      "Epoch 126/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.4640\n",
      "Epoch 127/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 0s 282us/step - loss: 0.4691\n",
      "Epoch 128/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.4762\n",
      "Epoch 129/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.4394\n",
      "Epoch 130/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4614\n",
      "Epoch 131/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.4663\n",
      "Epoch 132/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.4368\n",
      "Epoch 133/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.4184\n",
      "Epoch 134/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4437\n",
      "Epoch 135/150\n",
      "990/990 [==============================] - 0s 284us/step - loss: 0.4768\n",
      "Epoch 136/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.4246\n",
      "Epoch 137/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.4283\n",
      "Epoch 138/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.4268\n",
      "Epoch 139/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4302\n",
      "Epoch 140/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.3839\n",
      "Epoch 141/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.3910\n",
      "Epoch 142/150\n",
      "990/990 [==============================] - 0s 280us/step - loss: 0.4339\n",
      "Epoch 143/150\n",
      "990/990 [==============================] - 0s 279us/step - loss: 0.4033\n",
      "Epoch 144/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.3990\n",
      "Epoch 145/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.3935\n",
      "Epoch 146/150\n",
      "990/990 [==============================] - 0s 282us/step - loss: 0.3821\n",
      "Epoch 147/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.3835\n",
      "Epoch 148/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4005\n",
      "Epoch 149/150\n",
      "990/990 [==============================] - 0s 281us/step - loss: 0.4190\n",
      "Epoch 150/150\n",
      "990/990 [==============================] - 0s 278us/step - loss: 0.4020\n",
      "Epoch 1/150\n",
      "990/990 [==============================] - 1s 1ms/step - loss: 4.5719\n",
      "Epoch 2/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 4.3060\n",
      "Epoch 3/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 3.5833\n",
      "Epoch 4/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 2.9213\n",
      "Epoch 5/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 2.5425\n",
      "Epoch 6/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 2.1696\n",
      "Epoch 7/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 1.9697\n",
      "Epoch 8/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 1.7810\n",
      "Epoch 9/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 1.6600\n",
      "Epoch 10/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 1.5008\n",
      "Epoch 11/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 1.4027\n",
      "Epoch 12/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 1.3191\n",
      "Epoch 13/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 1.2247\n",
      "Epoch 14/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 1.1537\n",
      "Epoch 15/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 1.0995\n",
      "Epoch 16/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 1.0267\n",
      "Epoch 17/150\n",
      "990/990 [==============================] - 0s 323us/step - loss: 0.9455\n",
      "Epoch 18/150\n",
      "990/990 [==============================] - 0s 359us/step - loss: 0.9193\n",
      "Epoch 19/150\n",
      "990/990 [==============================] - 0s 474us/step - loss: 0.8834\n",
      "Epoch 20/150\n",
      "990/990 [==============================] - 0s 348us/step - loss: 0.8542\n",
      "Epoch 21/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.7643\n",
      "Epoch 22/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.7790\n",
      "Epoch 23/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 0.7407\n",
      "Epoch 24/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.6996\n",
      "Epoch 25/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 0.6463\n",
      "Epoch 26/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.6177\n",
      "Epoch 27/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.6099\n",
      "Epoch 28/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.6011\n",
      "Epoch 29/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.5271\n",
      "Epoch 30/150\n",
      "990/990 [==============================] - 0s 322us/step - loss: 0.5257\n",
      "Epoch 31/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.5249\n",
      "Epoch 32/150\n",
      "990/990 [==============================] - 0s 327us/step - loss: 0.4701\n",
      "Epoch 33/150\n",
      "990/990 [==============================] - 0s 339us/step - loss: 0.4787\n",
      "Epoch 34/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.4636\n",
      "Epoch 35/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 0.4301\n",
      "Epoch 36/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.4591\n",
      "Epoch 37/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.4000\n",
      "Epoch 38/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.4151\n",
      "Epoch 39/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.3969\n",
      "Epoch 40/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.3822\n",
      "Epoch 41/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.3791\n",
      "Epoch 42/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.3688\n",
      "Epoch 43/150\n",
      "990/990 [==============================] - 0s 312us/step - loss: 0.3381\n",
      "Epoch 44/150\n",
      "990/990 [==============================] - 0s 324us/step - loss: 0.3562\n",
      "Epoch 45/150\n",
      "990/990 [==============================] - 0s 322us/step - loss: 0.3265\n",
      "Epoch 46/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.3179\n",
      "Epoch 47/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.3065\n",
      "Epoch 48/150\n",
      "990/990 [==============================] - 0s 312us/step - loss: 0.2974\n",
      "Epoch 49/150\n",
      "990/990 [==============================] - 0s 326us/step - loss: 0.3066\n",
      "Epoch 50/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.2885\n",
      "Epoch 51/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.2533\n",
      "Epoch 52/150\n",
      "990/990 [==============================] - 0s 350us/step - loss: 0.2670\n",
      "Epoch 53/150\n",
      "990/990 [==============================] - 0s 340us/step - loss: 0.2601\n",
      "Epoch 54/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.2441\n",
      "Epoch 55/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.2231\n",
      "Epoch 56/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 0.2403\n",
      "Epoch 57/150\n",
      "990/990 [==============================] - 0s 351us/step - loss: 0.2398\n",
      "Epoch 58/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 0.2104\n",
      "Epoch 59/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.2084\n",
      "Epoch 60/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.2154\n",
      "Epoch 61/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.2054\n",
      "Epoch 62/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.2015\n",
      "Epoch 63/150\n",
      "990/990 [==============================] - 0s 335us/step - loss: 0.2002\n",
      "Epoch 64/150\n",
      "990/990 [==============================] - 0s 345us/step - loss: 0.1892\n",
      "Epoch 65/150\n",
      "990/990 [==============================] - 0s 326us/step - loss: 0.1648\n",
      "Epoch 66/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.1844\n",
      "Epoch 67/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 0.1612\n",
      "Epoch 68/150\n",
      "990/990 [==============================] - 0s 322us/step - loss: 0.1686\n",
      "Epoch 69/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.1637\n",
      "Epoch 70/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.1829\n",
      "Epoch 71/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.1725\n",
      "Epoch 72/150\n",
      "990/990 [==============================] - 0s 323us/step - loss: 0.1413\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990/990 [==============================] - 0s 316us/step - loss: 0.1470\n",
      "Epoch 74/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.1448\n",
      "Epoch 75/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.1440\n",
      "Epoch 76/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.1529\n",
      "Epoch 77/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 0.1523\n",
      "Epoch 78/150\n",
      "990/990 [==============================] - 0s 314us/step - loss: 0.1388\n",
      "Epoch 79/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.1295\n",
      "Epoch 80/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.1285\n",
      "Epoch 81/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.1304\n",
      "Epoch 82/150\n",
      "990/990 [==============================] - 0s 313us/step - loss: 0.1095\n",
      "Epoch 83/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.1231\n",
      "Epoch 84/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.1096\n",
      "Epoch 85/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.1157\n",
      "Epoch 86/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.1181\n",
      "Epoch 87/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.1143\n",
      "Epoch 88/150\n",
      "990/990 [==============================] - 0s 314us/step - loss: 0.0998\n",
      "Epoch 89/150\n",
      "990/990 [==============================] - 0s 314us/step - loss: 0.0976\n",
      "Epoch 90/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.1088\n",
      "Epoch 91/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0888\n",
      "Epoch 92/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0924\n",
      "Epoch 93/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0914\n",
      "Epoch 94/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0906\n",
      "Epoch 95/150\n",
      "990/990 [==============================] - 0s 330us/step - loss: 0.1002\n",
      "Epoch 96/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.1135\n",
      "Epoch 97/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0972\n",
      "Epoch 98/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0954\n",
      "Epoch 99/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.0821\n",
      "Epoch 100/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.0793\n",
      "Epoch 101/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0922\n",
      "Epoch 102/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.0830\n",
      "Epoch 103/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.0768\n",
      "Epoch 104/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.1015\n",
      "Epoch 105/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0686\n",
      "Epoch 106/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.0871\n",
      "Epoch 107/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0775\n",
      "Epoch 108/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0713\n",
      "Epoch 109/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0641\n",
      "Epoch 110/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0767\n",
      "Epoch 111/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.0516\n",
      "Epoch 112/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.0645\n",
      "Epoch 113/150\n",
      "990/990 [==============================] - 0s 374us/step - loss: 0.0704\n",
      "Epoch 114/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0788\n",
      "Epoch 115/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0729\n",
      "Epoch 116/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.0584\n",
      "Epoch 117/150\n",
      "990/990 [==============================] - 0s 331us/step - loss: 0.0591\n",
      "Epoch 118/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0597\n",
      "Epoch 119/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 0.0565\n",
      "Epoch 120/150\n",
      "990/990 [==============================] - 0s 330us/step - loss: 0.0710\n",
      "Epoch 121/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0590\n",
      "Epoch 122/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0725\n",
      "Epoch 123/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0602\n",
      "Epoch 124/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0531\n",
      "Epoch 125/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.0490\n",
      "Epoch 126/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 0.0556\n",
      "Epoch 127/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0518\n",
      "Epoch 128/150\n",
      "990/990 [==============================] - 0s 315us/step - loss: 0.0494\n",
      "Epoch 129/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0689\n",
      "Epoch 130/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0605\n",
      "Epoch 131/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.0614\n",
      "Epoch 132/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0644\n",
      "Epoch 133/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0455\n",
      "Epoch 134/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0491\n",
      "Epoch 135/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0428\n",
      "Epoch 136/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 0.0410\n",
      "Epoch 137/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0361\n",
      "Epoch 138/150\n",
      "990/990 [==============================] - 0s 320us/step - loss: 0.0391\n",
      "Epoch 139/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0303\n",
      "Epoch 140/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0488\n",
      "Epoch 141/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0383\n",
      "Epoch 142/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0617\n",
      "Epoch 143/150\n",
      "990/990 [==============================] - 0s 317us/step - loss: 0.0438\n",
      "Epoch 144/150\n",
      "990/990 [==============================] - 0s 318us/step - loss: 0.0429\n",
      "Epoch 145/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.0427\n",
      "Epoch 146/150\n",
      "990/990 [==============================] - 0s 322us/step - loss: 0.0528\n",
      "Epoch 147/150\n",
      "990/990 [==============================] - 0s 334us/step - loss: 0.0427\n",
      "Epoch 148/150\n",
      "990/990 [==============================] - 0s 316us/step - loss: 0.0532\n",
      "Epoch 149/150\n",
      "990/990 [==============================] - 0s 321us/step - loss: 0.0481\n",
      "Epoch 150/150\n",
      "990/990 [==============================] - 0s 319us/step - loss: 0.0410\n"
     ]
    }
   ],
   "source": [
    "print probs_cnn_train.shape\n",
    "print Y_labels.shape\n",
    "deep_alex.fit(probs_cnn_train,Y_labels,batch_size=16,epochs=150)\n",
    "deep_shape.fit(train_shape_data,Y_labels,batch_size=16,epochs=150)\n",
    "deep_texture.fit(train_texture_data,Y_labels,batch_size=16,epochs=150)\n",
    "deep_margin.fit(train_margin_data,Y_labels,batch_size=16,epochs=150)\n",
    "deep_sift.fit(vocab_train,Y_labels,batch_size=16,epochs=150)\n",
    "\n",
    "\n",
    "deep_input = np.array(deep_shape.predict(train_shape_data))\n",
    "deep_input = np.array(np.append(deep_input, deep_texture.predict(train_texture_data), axis=1))\n",
    "deep_input = np.array(np.append(deep_input, deep_margin.predict(train_margin_data), axis=1))\n",
    "deep_input = np.array(np.append(deep_input, deep_sift.predict(vocab_train), axis=1))\n",
    "deep_input = np.array(np.append(deep_input, deep_alex.predict(probs_cnn_train), axis=1))\n",
    "deep_input = np.array(np.append(deep_input, small_probs_cnn_train, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "# deep_final.fit(deep_input,Y_labels,batch_size=16,epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_model = Sequential([\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dense(1024, activation='relu'),\n",
    "#     Dropout(0.4),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(99, activation='softmax')\n",
    "# ])\n",
    "\n",
    "deep_model = Sequential([\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(99, activation='softmax')\n",
    "])\n",
    "\n",
    "deep_model.compile(loss='categorical_crossentropy',optimizer='adam',metric=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 4.1596 - val_loss: 3.4617\n",
      "Epoch 2/200\n",
      "792/792 [==============================] - 0s 598us/step - loss: 2.1285 - val_loss: 1.0078\n",
      "Epoch 3/200\n",
      "792/792 [==============================] - 0s 605us/step - loss: 0.2289 - val_loss: 0.0540\n",
      "Epoch 4/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 0.0204 - val_loss: 0.0149\n",
      "Epoch 5/200\n",
      "792/792 [==============================] - 0s 596us/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 6/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 7/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 0.0052 - val_loss: 0.0057\n",
      "Epoch 8/200\n",
      "792/792 [==============================] - 0s 612us/step - loss: 0.0042 - val_loss: 0.0046\n",
      "Epoch 9/200\n",
      "792/792 [==============================] - 0s 610us/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 10/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 11/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 12/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 13/200\n",
      "792/792 [==============================] - 0s 599us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 14/200\n",
      "792/792 [==============================] - 0s 596us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 15/200\n",
      "792/792 [==============================] - 0s 597us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 16/200\n",
      "792/792 [==============================] - 0s 601us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 17/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 18/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 19/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 20/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 9.8338e-04 - val_loss: 0.0010\n",
      "Epoch 21/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 8.7974e-04 - val_loss: 9.3689e-04\n",
      "Epoch 22/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 7.9110e-04 - val_loss: 8.7285e-04\n",
      "Epoch 23/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 7.7356e-04 - val_loss: 8.0712e-04\n",
      "Epoch 24/200\n",
      "792/792 [==============================] - 0s 630us/step - loss: 7.0850e-04 - val_loss: 7.3981e-04\n",
      "Epoch 25/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 6.6436e-04 - val_loss: 6.8794e-04\n",
      "Epoch 26/200\n",
      "792/792 [==============================] - 0s 598us/step - loss: 6.3022e-04 - val_loss: 6.3878e-04\n",
      "Epoch 27/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 5.6397e-04 - val_loss: 5.9998e-04\n",
      "Epoch 28/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 5.3953e-04 - val_loss: 5.5967e-04\n",
      "Epoch 29/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 4.9653e-04 - val_loss: 5.2549e-04\n",
      "Epoch 30/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 4.9289e-04 - val_loss: 4.9410e-04\n",
      "Epoch 31/200\n",
      "792/792 [==============================] - 0s 597us/step - loss: 4.4516e-04 - val_loss: 4.6537e-04\n",
      "Epoch 32/200\n",
      "792/792 [==============================] - 0s 597us/step - loss: 4.2058e-04 - val_loss: 4.3949e-04\n",
      "Epoch 33/200\n",
      "792/792 [==============================] - 0s 600us/step - loss: 4.1336e-04 - val_loss: 4.1545e-04\n",
      "Epoch 34/200\n",
      "792/792 [==============================] - 0s 601us/step - loss: 3.8288e-04 - val_loss: 3.9470e-04\n",
      "Epoch 35/200\n",
      "792/792 [==============================] - 0s 600us/step - loss: 3.5862e-04 - val_loss: 3.7320e-04\n",
      "Epoch 36/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 3.5436e-04 - val_loss: 3.5524e-04\n",
      "Epoch 37/200\n",
      "792/792 [==============================] - 0s 621us/step - loss: 3.3268e-04 - val_loss: 3.3740e-04\n",
      "Epoch 38/200\n",
      "792/792 [==============================] - 1s 635us/step - loss: 2.9584e-04 - val_loss: 3.2195e-04\n",
      "Epoch 39/200\n",
      "792/792 [==============================] - 1s 668us/step - loss: 2.9176e-04 - val_loss: 3.0837e-04\n",
      "Epoch 40/200\n",
      "792/792 [==============================] - 1s 781us/step - loss: 2.9070e-04 - val_loss: 2.9313e-04\n",
      "Epoch 41/200\n",
      "792/792 [==============================] - 1s 667us/step - loss: 2.6810e-04 - val_loss: 2.7794e-04\n",
      "Epoch 42/200\n",
      "792/792 [==============================] - 0s 598us/step - loss: 2.7158e-04 - val_loss: 2.6377e-04\n",
      "Epoch 43/200\n",
      "792/792 [==============================] - 1s 638us/step - loss: 2.4376e-04 - val_loss: 2.5235e-04\n",
      "Epoch 44/200\n",
      "792/792 [==============================] - 0s 618us/step - loss: 2.5192e-04 - val_loss: 2.4067e-04\n",
      "Epoch 45/200\n",
      "792/792 [==============================] - 0s 618us/step - loss: 2.2980e-04 - val_loss: 2.3028e-04\n",
      "Epoch 46/200\n",
      "792/792 [==============================] - 0s 610us/step - loss: 2.3355e-04 - val_loss: 2.2126e-04\n",
      "Epoch 47/200\n",
      "792/792 [==============================] - 0s 608us/step - loss: 2.2465e-04 - val_loss: 2.1019e-04\n",
      "Epoch 48/200\n",
      "792/792 [==============================] - 0s 609us/step - loss: 1.9670e-04 - val_loss: 2.0179e-04\n",
      "Epoch 49/200\n",
      "792/792 [==============================] - 0s 613us/step - loss: 1.8346e-04 - val_loss: 1.9491e-04\n",
      "Epoch 50/200\n",
      "792/792 [==============================] - 0s 612us/step - loss: 1.7610e-04 - val_loss: 1.8709e-04\n",
      "Epoch 51/200\n",
      "792/792 [==============================] - 0s 609us/step - loss: 1.8093e-04 - val_loss: 1.7965e-04\n",
      "Epoch 52/200\n",
      "792/792 [==============================] - 0s 617us/step - loss: 1.7760e-04 - val_loss: 1.7308e-04\n",
      "Epoch 53/200\n",
      "792/792 [==============================] - 0s 621us/step - loss: 1.7300e-04 - val_loss: 1.6658e-04\n",
      "Epoch 54/200\n",
      "792/792 [==============================] - 0s 631us/step - loss: 1.6665e-04 - val_loss: 1.5993e-04\n",
      "Epoch 55/200\n",
      "792/792 [==============================] - 0s 599us/step - loss: 1.6169e-04 - val_loss: 1.5408e-04\n",
      "Epoch 56/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 1.5544e-04 - val_loss: 1.4822e-04\n",
      "Epoch 57/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 1.4178e-04 - val_loss: 1.4309e-04\n",
      "Epoch 58/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 1.4199e-04 - val_loss: 1.3787e-04\n",
      "Epoch 59/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.2829e-04 - val_loss: 1.3325e-04\n",
      "Epoch 60/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 1.3745e-04 - val_loss: 1.2860e-04\n",
      "Epoch 61/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 1.2475e-04 - val_loss: 1.2454e-04\n",
      "Epoch 62/200\n",
      "792/792 [==============================] - 0s 600us/step - loss: 1.1297e-04 - val_loss: 1.2059e-04\n",
      "Epoch 63/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.1247e-04 - val_loss: 1.1732e-04\n",
      "Epoch 64/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 1.1034e-04 - val_loss: 1.1338e-04\n",
      "Epoch 65/200\n",
      "792/792 [==============================] - 0s 609us/step - loss: 1.0793e-04 - val_loss: 1.0967e-04\n",
      "Epoch 66/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 1.1133e-04 - val_loss: 1.0619e-04\n",
      "Epoch 67/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 9.7285e-05 - val_loss: 1.0304e-04\n",
      "Epoch 68/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 9.9249e-05 - val_loss: 9.9724e-05\n",
      "Epoch 69/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 9.5386e-05 - val_loss: 9.6761e-05\n",
      "Epoch 70/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 9.2478e-05 - val_loss: 9.3788e-05\n",
      "Epoch 71/200\n",
      "792/792 [==============================] - 0s 599us/step - loss: 9.6409e-05 - val_loss: 9.0851e-05\n",
      "Epoch 72/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 8.9228e-05 - val_loss: 8.7964e-05\n",
      "Epoch 73/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 8.7266e-05 - val_loss: 8.5250e-05\n",
      "Epoch 74/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 7.9706e-05 - val_loss: 8.3070e-05\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 0s 600us/step - loss: 7.9769e-05 - val_loss: 8.0610e-05\n",
      "Epoch 76/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 8.0456e-05 - val_loss: 7.8128e-05\n",
      "Epoch 77/200\n",
      "792/792 [==============================] - 1s 642us/step - loss: 7.8241e-05 - val_loss: 7.5620e-05\n",
      "Epoch 78/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 7.2059e-05 - val_loss: 7.3558e-05\n",
      "Epoch 79/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 6.9434e-05 - val_loss: 7.1392e-05\n",
      "Epoch 80/200\n",
      "792/792 [==============================] - 0s 602us/step - loss: 7.1824e-05 - val_loss: 6.9655e-05\n",
      "Epoch 81/200\n",
      "792/792 [==============================] - 0s 609us/step - loss: 6.8942e-05 - val_loss: 6.7743e-05\n",
      "Epoch 82/200\n",
      "792/792 [==============================] - 0s 603us/step - loss: 7.4259e-05 - val_loss: 6.5792e-05\n",
      "Epoch 83/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 6.5892e-05 - val_loss: 6.4118e-05\n",
      "Epoch 84/200\n",
      "792/792 [==============================] - 0s 599us/step - loss: 6.2972e-05 - val_loss: 6.2482e-05\n",
      "Epoch 85/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 6.7410e-05 - val_loss: 6.0831e-05\n",
      "Epoch 86/200\n",
      "792/792 [==============================] - 0s 606us/step - loss: 5.8317e-05 - val_loss: 5.9063e-05\n",
      "Epoch 87/200\n",
      "792/792 [==============================] - 1s 686us/step - loss: 5.8852e-05 - val_loss: 5.7549e-05\n",
      "Epoch 88/200\n",
      "792/792 [==============================] - 1s 663us/step - loss: 5.9225e-05 - val_loss: 5.6156e-05\n",
      "Epoch 89/200\n",
      "792/792 [==============================] - 0s 616us/step - loss: 5.6517e-05 - val_loss: 5.4739e-05\n",
      "Epoch 90/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 5.6081e-05 - val_loss: 5.3488e-05\n",
      "Epoch 91/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 5.6747e-05 - val_loss: 5.1977e-05\n",
      "Epoch 92/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 5.5952e-05 - val_loss: 5.0642e-05\n",
      "Epoch 93/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 4.9499e-05 - val_loss: 4.9356e-05\n",
      "Epoch 94/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 5.3665e-05 - val_loss: 4.8124e-05\n",
      "Epoch 95/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 4.5125e-05 - val_loss: 4.6964e-05\n",
      "Epoch 96/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 4.8884e-05 - val_loss: 4.5833e-05\n",
      "Epoch 97/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 5.0854e-05 - val_loss: 4.4707e-05\n",
      "Epoch 98/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 4.3417e-05 - val_loss: 4.3701e-05\n",
      "Epoch 99/200\n",
      "792/792 [==============================] - 1s 647us/step - loss: 4.6940e-05 - val_loss: 4.2658e-05\n",
      "Epoch 100/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 4.2222e-05 - val_loss: 4.1745e-05\n",
      "Epoch 101/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 4.5826e-05 - val_loss: 4.0608e-05\n",
      "Epoch 102/200\n",
      "792/792 [==============================] - 0s 611us/step - loss: 4.0908e-05 - val_loss: 3.9683e-05\n",
      "Epoch 103/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 4.2174e-05 - val_loss: 3.8864e-05\n",
      "Epoch 104/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 4.0907e-05 - val_loss: 3.8039e-05\n",
      "Epoch 105/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 3.8597e-05 - val_loss: 3.7185e-05\n",
      "Epoch 106/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 4.0137e-05 - val_loss: 3.6342e-05\n",
      "Epoch 107/200\n",
      "792/792 [==============================] - 0s 601us/step - loss: 4.0025e-05 - val_loss: 3.5441e-05\n",
      "Epoch 108/200\n",
      "792/792 [==============================] - 0s 616us/step - loss: 3.6207e-05 - val_loss: 3.4632e-05\n",
      "Epoch 109/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 3.5050e-05 - val_loss: 3.3961e-05\n",
      "Epoch 110/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 3.6749e-05 - val_loss: 3.3216e-05\n",
      "Epoch 111/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 3.5531e-05 - val_loss: 3.2407e-05\n",
      "Epoch 112/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 3.5541e-05 - val_loss: 3.1794e-05\n",
      "Epoch 113/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 3.1950e-05 - val_loss: 3.1161e-05\n",
      "Epoch 114/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 3.2673e-05 - val_loss: 3.0426e-05\n",
      "Epoch 115/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 2.9825e-05 - val_loss: 2.9785e-05\n",
      "Epoch 116/200\n",
      "792/792 [==============================] - 0s 599us/step - loss: 3.2395e-05 - val_loss: 2.9186e-05\n",
      "Epoch 117/200\n",
      "792/792 [==============================] - 1s 648us/step - loss: 3.0421e-05 - val_loss: 2.8498e-05\n",
      "Epoch 118/200\n",
      "792/792 [==============================] - 0s 617us/step - loss: 3.1041e-05 - val_loss: 2.7874e-05\n",
      "Epoch 119/200\n",
      "792/792 [==============================] - 0s 596us/step - loss: 2.8912e-05 - val_loss: 2.7282e-05\n",
      "Epoch 120/200\n",
      "792/792 [==============================] - 0s 598us/step - loss: 2.8830e-05 - val_loss: 2.6795e-05\n",
      "Epoch 121/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 2.7591e-05 - val_loss: 2.6241e-05\n",
      "Epoch 122/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 2.6231e-05 - val_loss: 2.5734e-05\n",
      "Epoch 123/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 2.7923e-05 - val_loss: 2.5152e-05\n",
      "Epoch 124/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 2.8371e-05 - val_loss: 2.4586e-05\n",
      "Epoch 125/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 2.6654e-05 - val_loss: 2.4006e-05\n",
      "Epoch 126/200\n",
      "792/792 [==============================] - 0s 588us/step - loss: 2.7224e-05 - val_loss: 2.3584e-05\n",
      "Epoch 127/200\n",
      "792/792 [==============================] - 0s 588us/step - loss: 2.4668e-05 - val_loss: 2.3122e-05\n",
      "Epoch 128/200\n",
      "792/792 [==============================] - 0s 597us/step - loss: 2.6602e-05 - val_loss: 2.2672e-05\n",
      "Epoch 129/200\n",
      "792/792 [==============================] - 0s 586us/step - loss: 2.4099e-05 - val_loss: 2.2200e-05\n",
      "Epoch 130/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 2.5098e-05 - val_loss: 2.1730e-05\n",
      "Epoch 131/200\n",
      "792/792 [==============================] - 0s 591us/step - loss: 2.3910e-05 - val_loss: 2.1350e-05\n",
      "Epoch 132/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 2.1679e-05 - val_loss: 2.0818e-05\n",
      "Epoch 133/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 2.2131e-05 - val_loss: 2.0459e-05\n",
      "Epoch 134/200\n",
      "792/792 [==============================] - 0s 628us/step - loss: 2.4483e-05 - val_loss: 2.0061e-05\n",
      "Epoch 135/200\n",
      "792/792 [==============================] - 0s 610us/step - loss: 2.2530e-05 - val_loss: 1.9613e-05\n",
      "Epoch 136/200\n",
      "792/792 [==============================] - 0s 608us/step - loss: 2.2316e-05 - val_loss: 1.9232e-05\n",
      "Epoch 137/200\n",
      "792/792 [==============================] - 0s 607us/step - loss: 2.1018e-05 - val_loss: 1.8850e-05\n",
      "Epoch 138/200\n",
      "792/792 [==============================] - 0s 601us/step - loss: 2.1239e-05 - val_loss: 1.8534e-05\n",
      "Epoch 139/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 2.1487e-05 - val_loss: 1.8214e-05\n",
      "Epoch 140/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 1.9032e-05 - val_loss: 1.7875e-05\n",
      "Epoch 141/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 1.7817e-05 - val_loss: 1.7549e-05\n",
      "Epoch 142/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.8360e-05 - val_loss: 1.7228e-05\n",
      "Epoch 143/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 1.9127e-05 - val_loss: 1.6886e-05\n",
      "Epoch 144/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 1.8891e-05 - val_loss: 1.6551e-05\n",
      "Epoch 145/200\n",
      "792/792 [==============================] - 0s 585us/step - loss: 1.8116e-05 - val_loss: 1.6259e-05\n",
      "Epoch 146/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 2.0453e-05 - val_loss: 1.5929e-05\n",
      "Epoch 147/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 1.6337e-05 - val_loss: 1.5582e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "792/792 [==============================] - 0s 601us/step - loss: 1.7362e-05 - val_loss: 1.5330e-05\n",
      "Epoch 149/200\n",
      "792/792 [==============================] - 0s 610us/step - loss: 1.8266e-05 - val_loss: 1.5081e-05\n",
      "Epoch 150/200\n",
      "792/792 [==============================] - 1s 639us/step - loss: 1.6887e-05 - val_loss: 1.4784e-05\n",
      "Epoch 151/200\n",
      "792/792 [==============================] - 0s 610us/step - loss: 1.7269e-05 - val_loss: 1.4458e-05\n",
      "Epoch 152/200\n",
      "792/792 [==============================] - 0s 604us/step - loss: 1.5846e-05 - val_loss: 1.4224e-05\n",
      "Epoch 153/200\n",
      "792/792 [==============================] - 0s 597us/step - loss: 1.5594e-05 - val_loss: 1.3978e-05\n",
      "Epoch 154/200\n",
      "792/792 [==============================] - 0s 588us/step - loss: 1.6167e-05 - val_loss: 1.3769e-05\n",
      "Epoch 155/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 1.5462e-05 - val_loss: 1.3535e-05\n",
      "Epoch 156/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 1.4465e-05 - val_loss: 1.3305e-05\n",
      "Epoch 157/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 1.5113e-05 - val_loss: 1.3103e-05\n",
      "Epoch 158/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.4672e-05 - val_loss: 1.2857e-05\n",
      "Epoch 159/200\n",
      "792/792 [==============================] - 0s 587us/step - loss: 1.4064e-05 - val_loss: 1.2620e-05\n",
      "Epoch 160/200\n",
      "792/792 [==============================] - 0s 598us/step - loss: 1.3325e-05 - val_loss: 1.2384e-05\n",
      "Epoch 161/200\n",
      "792/792 [==============================] - 0s 596us/step - loss: 1.3774e-05 - val_loss: 1.2174e-05\n",
      "Epoch 162/200\n",
      "792/792 [==============================] - 0s 618us/step - loss: 1.3052e-05 - val_loss: 1.1946e-05\n",
      "Epoch 163/200\n",
      "792/792 [==============================] - 0s 601us/step - loss: 1.2460e-05 - val_loss: 1.1725e-05\n",
      "Epoch 164/200\n",
      "792/792 [==============================] - 0s 604us/step - loss: 1.3578e-05 - val_loss: 1.1521e-05\n",
      "Epoch 165/200\n",
      "792/792 [==============================] - 0s 627us/step - loss: 1.2657e-05 - val_loss: 1.1300e-05\n",
      "Epoch 166/200\n",
      "792/792 [==============================] - 0s 597us/step - loss: 1.2280e-05 - val_loss: 1.1093e-05\n",
      "Epoch 167/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 1.3296e-05 - val_loss: 1.0913e-05\n",
      "Epoch 168/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 1.2313e-05 - val_loss: 1.0744e-05\n",
      "Epoch 169/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.2032e-05 - val_loss: 1.0564e-05\n",
      "Epoch 170/200\n",
      "792/792 [==============================] - 0s 594us/step - loss: 1.1667e-05 - val_loss: 1.0382e-05\n",
      "Epoch 171/200\n",
      "792/792 [==============================] - 0s 620us/step - loss: 1.1112e-05 - val_loss: 1.0250e-05\n",
      "Epoch 172/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.1467e-05 - val_loss: 1.0069e-05\n",
      "Epoch 173/200\n",
      "792/792 [==============================] - 0s 599us/step - loss: 1.1591e-05 - val_loss: 9.8807e-06\n",
      "Epoch 174/200\n",
      "792/792 [==============================] - 0s 604us/step - loss: 1.0862e-05 - val_loss: 9.7157e-06\n",
      "Epoch 175/200\n",
      "792/792 [==============================] - 0s 603us/step - loss: 1.0200e-05 - val_loss: 9.5655e-06\n",
      "Epoch 176/200\n",
      "792/792 [==============================] - 0s 604us/step - loss: 1.0426e-05 - val_loss: 9.4403e-06\n",
      "Epoch 177/200\n",
      "792/792 [==============================] - 0s 601us/step - loss: 1.0478e-05 - val_loss: 9.2783e-06\n",
      "Epoch 178/200\n",
      "792/792 [==============================] - 0s 584us/step - loss: 1.1067e-05 - val_loss: 9.1082e-06\n",
      "Epoch 179/200\n",
      "792/792 [==============================] - 0s 588us/step - loss: 9.8387e-06 - val_loss: 8.9399e-06\n",
      "Epoch 180/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.0015e-05 - val_loss: 8.8126e-06\n",
      "Epoch 181/200\n",
      "792/792 [==============================] - 0s 589us/step - loss: 1.0210e-05 - val_loss: 8.6503e-06\n",
      "Epoch 182/200\n",
      "792/792 [==============================] - 0s 593us/step - loss: 1.0137e-05 - val_loss: 8.5185e-06\n",
      "Epoch 183/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 1.0444e-05 - val_loss: 8.3719e-06\n",
      "Epoch 184/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 9.4880e-06 - val_loss: 8.2126e-06\n",
      "Epoch 185/200\n",
      "792/792 [==============================] - 0s 606us/step - loss: 9.4981e-06 - val_loss: 8.0582e-06\n",
      "Epoch 186/200\n",
      "792/792 [==============================] - 0s 603us/step - loss: 9.2887e-06 - val_loss: 7.9176e-06\n",
      "Epoch 187/200\n",
      "792/792 [==============================] - 0s 612us/step - loss: 9.8289e-06 - val_loss: 7.7601e-06\n",
      "Epoch 188/200\n",
      "792/792 [==============================] - 0s 612us/step - loss: 8.1948e-06 - val_loss: 7.6358e-06\n",
      "Epoch 189/200\n",
      "792/792 [==============================] - 0s 602us/step - loss: 9.8196e-06 - val_loss: 7.5097e-06\n",
      "Epoch 190/200\n",
      "792/792 [==============================] - 0s 588us/step - loss: 8.4726e-06 - val_loss: 7.3610e-06\n",
      "Epoch 191/200\n",
      "792/792 [==============================] - 0s 599us/step - loss: 8.9614e-06 - val_loss: 7.2568e-06\n",
      "Epoch 192/200\n",
      "792/792 [==============================] - 0s 596us/step - loss: 8.6550e-06 - val_loss: 7.1403e-06\n",
      "Epoch 193/200\n",
      "792/792 [==============================] - 0s 595us/step - loss: 8.0076e-06 - val_loss: 7.0012e-06\n",
      "Epoch 194/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 8.1862e-06 - val_loss: 6.8895e-06\n",
      "Epoch 195/200\n",
      "792/792 [==============================] - 0s 590us/step - loss: 7.9833e-06 - val_loss: 6.7779e-06\n",
      "Epoch 196/200\n",
      "792/792 [==============================] - 0s 592us/step - loss: 7.7430e-06 - val_loss: 6.6619e-06\n",
      "Epoch 197/200\n",
      "792/792 [==============================] - 0s 596us/step - loss: 7.9463e-06 - val_loss: 6.5632e-06\n",
      "Epoch 198/200\n",
      "792/792 [==============================] - 0s 604us/step - loss: 8.1796e-06 - val_loss: 6.4434e-06\n",
      "Epoch 199/200\n",
      "792/792 [==============================] - 0s 606us/step - loss: 8.5155e-06 - val_loss: 6.3492e-06\n",
      "Epoch 200/200\n",
      "792/792 [==============================] - 0s 609us/step - loss: 7.2837e-06 - val_loss: 6.2543e-06\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               304640    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 99)                50787     \n",
      "=================================================================\n",
      "Total params: 618,083\n",
      "Trainable params: 618,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_model.fit(deep_input,Y_labels,batch_size=32,epochs=200, validation_split=0.2)\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_input_test = np.array(deep_shape.predict(test_shape_data))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_texture.predict(test_texture_data), axis=1))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_margin.predict(test_margin_data), axis=1))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_sift.predict(vocab_test), axis=1))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_alex.predict(probs_cnn_test), axis=1))\n",
    "deep_input_test = np.array(np.append(deep_input_test, small_probs_cnn_test, axis=1))\n",
    "\n",
    "# Generate out file\n",
    "out_file = deep_model.predict(deep_input_test)\n",
    "out_file = np.append(np.array(test_ids).reshape(-1,1),out_file,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# out_file = generateSubmission(test_ids, test_data,svm_model,99)\n",
    "headerRow=np.array(['id'] + le.inverse_transform(range(99)).tolist())\n",
    "df = pd.DataFrame(data=out_file, columns = headerRow)\n",
    "df['id'] = df['id'].astype(np.int)\n",
    "df=df.set_index('id')\n",
    "#print df.head()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print out_file\n",
    "df.to_csv('output/18_12_18_001(ALexNet15DeepFull).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
