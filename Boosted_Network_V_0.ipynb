{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import keras\n",
    "import nolearn# import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images first\n",
    "numImages = len(glob.glob('./images/*jpg'))\n",
    "images = [None for i in xrange(numImages)]\n",
    "for fileName in glob.glob('./images/*jpg'):\n",
    "    fileNum = int(fileName[9:][:-4])\n",
    "    images[fileNum-1] = np.array(cv2.imread(fileName, 0))\n",
    "images = np.array(images)\n",
    "\n",
    "# Load csv data next\n",
    "train_data = pd.read_csv('data/train.csv').drop(['species'], axis=1).values\n",
    "train_labels = pd.read_csv('data/train.csv')['species'].values\n",
    "train_images = [images[int(data[0]-1)] for data in train_data]\n",
    "train_ids = [data[0] for data in train_data]\n",
    "train_data = np.delete(train_data, 0, 1)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv').values\n",
    "test_images = [images[int(data[0]-1)] for data in test_data]\n",
    "test_ids = [data[0] for data in test_data]\n",
    "test_data = np.delete(test_data, 0, 1)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Various Manual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the manual 3 feature histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le= preprocessing.LabelEncoder()\n",
    "#encode train labels\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded=le.transform(train_labels)\n",
    "\n",
    "#separate the 3 histograms\n",
    "train_margin_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'margin1':'margin64']).values\n",
    "train_shape_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'shape1':'shape64']).values\n",
    "train_texture_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "test_margin_data=((pd.read_csv('data/test.csv')).loc[:,'margin1':'margin64']).values\n",
    "test_shape_data=((pd.read_csv('data/test.csv')).loc[:,'shape1':'shape64']).values\n",
    "test_texture_data=((pd.read_csv('data/test.csv')).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "#print train_margin_data.head()\n",
    "#print train_shape_data.head()\n",
    "#print train_texture_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for clustering/finding descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(images, dense=False):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    des_per_Img = np.array([sift.detectAndCompute(img,None)[1] for img in images])\n",
    "    return des_per_Img\n",
    "        \n",
    "def get_clusters(descriptors, vocabSize):\n",
    "    des_list = np.concatenate(descriptors)\n",
    "\n",
    "    kmeans = MiniBatchKMeans(vocabSize, batch_size=100)\n",
    "    kmeans.fit(np.array(des_list))\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def get_vocabulary(descriptors, clusters, vocabSize):\n",
    "    return np.array([normalize(np.histogram(clusters.predict(dscrs), bins=range(vocabSize))[0].reshape(1,-1)).ravel() for dscrs in descriptors])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the descriptors for the training/testing of SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_list_train = get_descriptor(train_images)\n",
    "des_list_test = get_descriptor(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the descriptors into a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed in 3.969930 seconds\n"
     ]
    }
   ],
   "source": [
    "clustering_start_time=time.time()\n",
    "clusters = get_clusters(des_list_train,150)\n",
    "clustering_end_time=time.time()\n",
    "print \"Clustering completed in {:2f} seconds\".format(clustering_end_time-clustering_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the vocabulary for test/train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = get_vocabulary(des_list_train,clusters,150)\n",
    "vocab_test = get_vocabulary(des_list_test,clusters,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 192)\n",
      "(990, 149)\n",
      "(990, 341)\n",
      "(594, 341)\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape\n",
    "print vocab_train.shape\n",
    "train_set = np.append(train_data,vocab_train,axis=1)\n",
    "test_set =np.append(test_data,vocab_test,axis=1)\n",
    "print train_set.shape\n",
    "print test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models trained on base histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNearestNeighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1 : KNN\n",
    "knn_model = KNeighborsClassifier(len((np.unique(train_labels))))\n",
    "#Training on all 192 cols\n",
    "knn_train_start_time=time.time()\n",
    "knn_model.fit(train_data,train_labels_encoded)\n",
    "knn_train_end_time=time.time()\n",
    "print \"KNN trained in {:2f} seconds\".format(knn_train_end_time-knn_train_start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2 : SVM\n",
    "svm_model = OneVsRestClassifier(SVC(kernel= 'linear',C=0.8,probability=True))\n",
    "#Training on all 192 cols\n",
    "svm_train_start_time=time.time()\n",
    "svm_model.fit(train_data,train_labels_encoded)\n",
    "svm_train_end_time=time.time()\n",
    "print \"SVM trained in {:2f} seconds\".format(knn_train_end_time-knn_train_start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = OneVsRestClassifier(SVC(kernel= 'linear',C=0.8,probability=True))\n",
    "svm_model.fit(vocab_train,train_labels_encoded)\n",
    "out_file = generateSubmission(test_ids,vocab_test,svm_model,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = OneVsRestClassifier(SVC(kernel= 'linear',C=0.8,probability=True))\n",
    "svm_model.fit(train_set,train_labels_encoded)\n",
    "out_file = generateSubmission(test_ids,test_set,svm_model,99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=xgboost.XGBClassifier()\n",
    "xgb.fit(train_data,train_labels_encoded)\n",
    "out_file = generateSubmission(test_ids,test_data,xgb,99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlc_model = MLPClassifier(learning_rate='adaptive')\n",
    "mlc_model = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(400,))\n",
    "mlc_model.fit(train_set, train_labels_encoded)\n",
    "out_file = generateSubmission(test_ids, test_set, mlc_model, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-815f9456214e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(180, input_dim=192))\n",
    "model.add(Dense(8, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models trained based on outputs of other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layered FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayeredFNN(object):\n",
    "    def __init__(self):\n",
    "        self.model_sift = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "        self.model_shape = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "        self.model_texture = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "        self.model_margin = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "        self.model_second = MLPClassifier(learning_rate='constant', max_iter=8000,hidden_layer_sizes=(400,))\n",
    "     \n",
    "    def genSecondInput(self, sift, shape, texture, margin):\n",
    "        level_input = self.model_margin.predict_proba(margin)\n",
    "        level_input = np.array(np.append(level_input, self.model_texture.predict_proba(texture), axis=1))\n",
    "        level_input = np.array(np.append(level_input, self.model_shape.predict_proba(shape),     axis=1))\n",
    "        level_input = np.array(np.append(level_input, self.model_sift.predict_proba(sift),       axis=1))\n",
    "        return level_input\n",
    "    \n",
    "    def fit(self, sift, shape, texture, margin, labels):\n",
    "        self.model_sift.fit(sift, labels)\n",
    "        self.model_shape.fit(shape, labels)\n",
    "        self.model_texture.fit(texture, labels)\n",
    "        self.model_margin.fit(margin, labels)\n",
    "\n",
    "        level_input = self.genSecondInput(sift, shape, texture, margin)\n",
    "\n",
    "        self.model_second.fit(level_input, labels)\n",
    "         \n",
    "    def predict(self, sift, shape, texture, margin):\n",
    "        level_input = self.genSecondInput(sift, shape, texture, margin)\n",
    "        return self.model_second.predict(level_input)\n",
    "        \n",
    "LFNN = LayeredFNN()\n",
    "LFNN.fit(vocab_train, train_shape_data, train_texture_data, train_margin_data, train_labels_encoded)\n",
    "out = LFNN.predict(vocab_test, test_shape_data, test_texture_data, test_margin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layered FNN with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3f080b0350>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f081335d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.resize(train_images[442], (25, 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5951 - acc: 0.0051 - val_loss: 4.5952 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "792/792 [==============================] - 7s 9ms/step - loss: 4.5951 - acc: 0.0126 - val_loss: 4.5953 - val_acc: 0.0152\n",
      "Epoch 3/20\n",
      "792/792 [==============================] - 7s 9ms/step - loss: 4.5951 - acc: 0.0101 - val_loss: 4.5953 - val_acc: 0.0051\n",
      "Epoch 4/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5951 - acc: 0.0051 - val_loss: 4.5954 - val_acc: 0.0051\n",
      "Epoch 5/20\n",
      "792/792 [==============================] - 8s 9ms/step - loss: 4.5951 - acc: 0.0076 - val_loss: 4.5955 - val_acc: 0.0051\n",
      "Epoch 6/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5951 - acc: 0.0101 - val_loss: 4.5956 - val_acc: 0.0051\n",
      "Epoch 7/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5950 - acc: 0.0063 - val_loss: 4.5956 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5950 - acc: 0.0076 - val_loss: 4.5957 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5950 - acc: 0.0126 - val_loss: 4.5958 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5950 - acc: 0.0114 - val_loss: 4.5959 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5950 - acc: 0.0126 - val_loss: 4.5960 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5949 - acc: 0.0114 - val_loss: 4.5961 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5949 - acc: 0.0088 - val_loss: 4.5961 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5949 - acc: 0.0126 - val_loss: 4.5962 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5949 - acc: 0.0126 - val_loss: 4.5963 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5949 - acc: 0.0101 - val_loss: 4.5964 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5948 - acc: 0.0088 - val_loss: 4.5965 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5948 - acc: 0.0126 - val_loss: 4.5966 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5948 - acc: 0.0126 - val_loss: 4.5966 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "792/792 [==============================] - 8s 10ms/step - loss: 4.5948 - acc: 0.0076 - val_loss: 4.5967 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc94c14ef50>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = 64, 64\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(x, y, 1)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1000, activation='relu'))\n",
    "model.add(keras.layers.Dense(99, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "reduced_train_images = np.concatenate(np.array([cv2.normalize(cv2.resize(img, (x, y)), dst=np.array([])).reshape(1, x, y, 1) for img in train_images]))\n",
    "\n",
    "train_labels_categorical = keras.utils.to_categorical(train_labels_encoded, 99)\n",
    "\n",
    "model.fit(reduced_train_images, train_labels_categorical, batch_size=128, epochs=20, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 64, 64, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_train_images.shape\n",
    "# for i in reduced_train_images:\n",
    "#     reduced_train_images[i] = reduced_train_images[i] / (reduced_train_images[i].sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 25)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.resize(train_images[0], (25,25)).reshape((1, 25, 25)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7de462599cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mcnnClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# cnnClassifier.fit(train_images, train_labels_encoded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mcnnClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-7de462599cc0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemon/Documents/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lemon/Documents/anaconda2/lib/python2.7/site-packages/torch/nn/modules/loss.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[1;32m    759\u001b[0m                                self.ignore_index, self.reduce)\n",
      "\u001b[0;32m/home/lemon/Documents/anaconda2/lib/python2.7/site-packages/torch/nn/modules/loss.pyc\u001b[0m in \u001b[0;36m_assert_no_grad\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"nn criterions don't compute the gradient w.r.t. targets - please \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m\"mark these tensors as not requiring gradients\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'requires_grad'"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(9, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 99)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class CNNClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.model = CNN()\n",
    "        \n",
    "    def fit(self, X, y, epochs=2):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(zip(X, y), 0):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                \n",
    "                inputs = torch.FloatTensor(cv2.resize(inputs, (22, 22)).reshape(1, 1, 22, 22))\n",
    "                                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 2000))\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "cnnClassifier = CNNClassifier()\n",
    "# cnnClassifier.fit(train_images, train_labels_encoded)\n",
    "cnnClassifier.fit(train_images, keras.utils.to_categoritrain_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]), np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3]])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict using KNN\n",
    "# preds_knn=knn_model.predict(test_data)\n",
    "# #predict using SVM\n",
    "# preds_svm=svm_model.decision_function(test_data)\n",
    "# print np.array(preds_svm).shape\n",
    "\n",
    "def generateSubmission(ids, test, model, num_classes):\n",
    "    num_test = len(test)\n",
    "    predictions = model.predict(test)\n",
    "    confidence = model.predict_proba(test)\n",
    "    output = np.zeros((num_test, num_classes+1))\n",
    "    \n",
    "    for i in xrange(num_test):\n",
    "        p = predictions[i]\n",
    "        c = confidence[i][p]\n",
    "        #prob = c\n",
    "        prob = min(max(10e-15, c), 1-10e-15)\n",
    "        logLoss = np.log(prob)\n",
    "        output[i][p+1] = -logLoss / num_test\n",
    "        output[i][0] = ids[i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_file = generateSubmission(test_ids, test_data,svm_model,99)\n",
    "headerRow=np.array(['id'] + le.inverse_transform(range(99)).tolist())\n",
    "df = pd.DataFrame(data=out_file, columns = headerRow)\n",
    "df['id'] = df['id'].astype(np.int)\n",
    "df=df.set_index('id')\n",
    "#print df.head()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print out_file\n",
    "df.to_csv('output/9_11_18_17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
