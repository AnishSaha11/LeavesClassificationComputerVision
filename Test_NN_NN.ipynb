{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images first\n",
    "numImages = len(glob.glob('./images/*jpg'))\n",
    "images = [None for i in xrange(numImages)]\n",
    "for fileName in glob.glob('./images/*jpg'):\n",
    "    fileNum = int(fileName[9:][:-4])\n",
    "    images[fileNum-1] = np.array(cv2.imread(fileName, 0))\n",
    "images = np.array(images)\n",
    "\n",
    "# Load csv data next\n",
    "train_data = pd.read_csv('data/train.csv').drop(['species'], axis=1).values\n",
    "train_labels = pd.read_csv('data/train.csv')['species'].values\n",
    "train_images = [images[int(data[0]-1)] for data in train_data]\n",
    "train_ids = [data[0] for data in train_data]\n",
    "train_data = np.delete(train_data, 0, 1)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv').values\n",
    "test_images = [images[int(data[0]-1)] for data in test_data]\n",
    "test_ids = [data[0] for data in test_data]\n",
    "test_data = np.delete(test_data, 0, 1)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le= preprocessing.LabelEncoder()\n",
    "#encode train labels\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded=le.transform(train_labels)\n",
    "\n",
    "#separate the 3 histograms\n",
    "train_margin_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'margin1':'margin64']).values\n",
    "train_shape_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'shape1':'shape64']).values\n",
    "train_texture_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "test_margin_data=((pd.read_csv('data/test.csv')).loc[:,'margin1':'margin64']).values\n",
    "test_shape_data=((pd.read_csv('data/test.csv')).loc[:,'shape1':'shape64']).values\n",
    "test_texture_data=((pd.read_csv('data/test.csv')).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "#print train_margin_data.head()\n",
    "#print train_shape_data.head()\n",
    "#print train_texture_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute descriptors,clusters and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(images, dense=False):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    des_per_Img = np.array([sift.detectAndCompute(img,None)[1] for img in images])\n",
    "    return des_per_Img\n",
    "        \n",
    "def get_clusters(descriptors, vocabSize):\n",
    "    des_list = np.concatenate(descriptors)\n",
    "\n",
    "    kmeans = MiniBatchKMeans(vocabSize, batch_size=100)\n",
    "    kmeans.fit(np.array(des_list))\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def get_vocabulary(descriptors, clusters, vocabSize):\n",
    "    return np.array([normalize(np.histogram(clusters.predict(dscrs), bins=range(vocabSize))[0].reshape(1,-1)).ravel() for dscrs in descriptors])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors computed in 126.814249 seconds\n"
     ]
    }
   ],
   "source": [
    "des_start_time =time.time()\n",
    "des_list_train = get_descriptor(train_images)\n",
    "des_list_test = get_descriptor(test_images)\n",
    "des_end_time =time.time()\n",
    "print \"Descriptors computed in {:2f} seconds\".format(des_end_time-des_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed in 3.232382 seconds\n"
     ]
    }
   ],
   "source": [
    "clustering_start_time=time.time()\n",
    "clusters = get_clusters(des_list_train,150)\n",
    "clustering_end_time=time.time()\n",
    "print \"Clustering completed in {:2f} seconds\".format(clustering_end_time-clustering_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 149)\n"
     ]
    }
   ],
   "source": [
    "vocab_train = get_vocabulary(des_list_train,clusters,150)\n",
    "vocab_test = get_vocabulary(des_list_test,clusters,150)\n",
    "\n",
    "print vocab_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_margin = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_margin.fit(train_margin_data, train_labels_encoded)\n",
    "mlp_train_margin_pred = mlp_train_margin.predict_proba(train_margin_data)\n",
    "\n",
    "mlp_train_texture = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_texture.fit(train_texture_data, train_labels_encoded)\n",
    "mlp_train_texture_pred = mlp_train_texture.predict_proba(train_texture_data)\n",
    "\n",
    "mlp_train_shape = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_shape.fit(train_shape_data, train_labels_encoded)\n",
    "mlp_train_shape_pred = mlp_train_shape.predict_proba(train_shape_data)\n",
    "\n",
    "mlp_train_sift_bof = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_sift_bof.fit(vocab_train, train_labels_encoded)\n",
    "mlp_train_sift_bof_pred = mlp_train_sift_bof.predict_proba(vocab_train)\n",
    "\n",
    "second_level_input = np.array(np.append(mlp_train_margin_pred,mlp_train_texture_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,mlp_train_shape_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,mlp_train_sift_bof_pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(400,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=8000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlc_model = MLPClassifier(learning_rate='constant', max_iter=8000,hidden_layer_sizes=(400,))\n",
    "mlc_model.fit(second_level_input, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 396)\n"
     ]
    }
   ],
   "source": [
    "out_file = generateDeepSubmission(test_ids,test_margin_data,test_texture_data,test_shape_data,vocab_test,mlp_train_margin, mlp_train_texture,mlp_train_shape,mlp_train_sift_bof,mlc_model,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDeepSubmission(ids,test_l1,test_l2,test_l3,test_l4\n",
    "                           ,model_b1,model_b2,model_b3,model_b4,model_top,num_classes):\n",
    "    num_test = len(test_l1)\n",
    "    block1_pred = model_b1.predict_proba(test_l1)\n",
    "    block2_pred = model_b2.predict_proba(test_l2)\n",
    "    block3_pred = model_b3.predict_proba(test_l3)\n",
    "    block4_pred = model_b4.predict_proba(test_l4)\n",
    "    final_input = np.array(np.append(block1_pred,block2_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,block3_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,block4_pred,axis=1))\n",
    "    print final_input.shape\n",
    "    final_pred = model_top.predict(final_input)\n",
    "    final_confidence =  model_top.predict_proba(final_input)\n",
    "    final_confidence = np.append(np.array(ids).reshape(-1,1),final_confidence,axis=1)\n",
    "    \n",
    "    return final_confidence\n",
    "    #for i in xrange()\n",
    "    #return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# out_file = generateSubmission(test_ids, test_data,svm_model,99)\n",
    "headerRow=np.array(['id'] + le.inverse_transform(range(99)).tolist())\n",
    "df = pd.DataFrame(data=out_file, columns = headerRow)\n",
    "df['id'] = df['id'].astype(np.int)\n",
    "df=df.set_index('id')\n",
    "#print df.head()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print out_file\n",
    "df.to_csv('output/22_11_18_002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
