{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images first\n",
    "numImages = len(glob.glob('./images/*jpg'))\n",
    "images = [None for i in xrange(numImages)]\n",
    "for fileName in glob.glob('./images/*jpg'):\n",
    "    fileNum = int(fileName[9:][:-4])\n",
    "    images[fileNum-1] = np.array(cv2.imread(fileName, 0))\n",
    "images = np.array(images)\n",
    "\n",
    "# Load csv data next\n",
    "train_data = pd.read_csv('data/train.csv').drop(['species'], axis=1).values\n",
    "train_labels = pd.read_csv('data/train.csv')['species'].values\n",
    "labels=train_labels.tolist()\n",
    "train_images = [images[int(data[0]-1)] for data in train_data]\n",
    "train_ids = [data[0] for data in train_data]\n",
    "train_data = np.delete(train_data, 0, 1)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv').values\n",
    "test_images = [images[int(data[0]-1)] for data in test_data]\n",
    "test_ids = [data[0] for data in test_data]\n",
    "test_data = np.delete(test_data, 0, 1)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Normalization and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN train data\n",
    "def img_norm(img):\n",
    "    t= 2 * (np.float32(img) / 255 - 0.5) # normalize img pixels to [-1, 1]\n",
    "    return t\n",
    "def minibatchData(data,labels_encoded,img_size,channel_num=1,batch_num=30):\n",
    "    images=[]\n",
    "    for img in data:\n",
    "        images.append(img_norm(cv2.resize(img,img_size)))\n",
    "    \n",
    "    channel_num=1\n",
    "    if batch_num > 1:\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        print(len(images))\n",
    "        print(batch_num)\n",
    "        \n",
    "        for i in range(int(len(images) / batch_num)):\n",
    "            minibatch_d = images[i*batch_num: (i+1)*batch_num]\n",
    "            minibatch_d = np.reshape(minibatch_d, (batch_num, channel_num,img_size[0],img_size[1]))\n",
    "            batch_data.append(torch.from_numpy(minibatch_d))\n",
    "            if labels_encoded is not None:\n",
    "                minibatch_l = labels_encoded[i*batch_num: (i+1)*batch_num]\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "            else:\n",
    "                minibatch_l = np.zeros(batch_num)\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "        #data, labels = batch_data, batch_labels \n",
    "        \n",
    "    return zip(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le= preprocessing.LabelEncoder()\n",
    "#encode train labels\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded=le.transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "30\n",
      "594\n",
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a25668d90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_size=(128,128)\n",
    "cnn_train_data = list(minibatchData(train_images,train_labels_encoded,img_size))\n",
    "#plt.imshow(cnn_train_data[0][0][3][0])\n",
    "cnn_test_data = list(minibatchData(test_images,None,img_size))\n",
    "#print cnn_train_data.size\n",
    "plt.imshow(cnn_test_data[0][0][10][0],'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Test, and Train a Convolutional Network In PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(4, 4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        #self.conv3 = nn.Conv2d(32, 64, 4)\n",
    "        self.fc1 = nn.Linear(32*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 99)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print x.shape\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print x.shape\n",
    "        x=self.dropout(x)\n",
    "        x = x.view(-1, 32*6*6)\n",
    "        #print x.shape\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #print x.shape\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net =Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 0.139\n",
      "[2,    30] loss: 0.139\n",
      "[3,    30] loss: 0.139\n",
      "[4,    30] loss: 0.139\n",
      "[5,    30] loss: 0.139\n",
      "[6,    30] loss: 0.139\n",
      "[7,    30] loss: 0.139\n",
      "[8,    30] loss: 0.139\n",
      "[9,    30] loss: 0.139\n",
      "[10,    30] loss: 0.139\n",
      "[11,    30] loss: 0.139\n",
      "[12,    30] loss: 0.139\n",
      "[13,    30] loss: 0.139\n",
      "[14,    30] loss: 0.139\n",
      "[15,    30] loss: 0.139\n",
      "[16,    30] loss: 0.139\n",
      "[17,    30] loss: 0.139\n",
      "[18,    30] loss: 0.139\n",
      "[19,    30] loss: 0.139\n",
      "[20,    30] loss: 0.139\n",
      "[21,    30] loss: 0.139\n",
      "[22,    30] loss: 0.139\n",
      "[23,    30] loss: 0.138\n",
      "[24,    30] loss: 0.138\n",
      "[25,    30] loss: 0.138\n",
      "[26,    30] loss: 0.138\n",
      "[27,    30] loss: 0.138\n",
      "[28,    30] loss: 0.138\n",
      "[29,    30] loss: 0.137\n",
      "[30,    30] loss: 0.137\n",
      "[31,    30] loss: 0.136\n",
      "[32,    30] loss: 0.135\n",
      "[33,    30] loss: 0.134\n",
      "[34,    30] loss: 0.133\n",
      "[35,    30] loss: 0.131\n",
      "[36,    30] loss: 0.127\n",
      "[37,    30] loss: 0.123\n",
      "[38,    30] loss: 0.118\n",
      "[39,    30] loss: 0.112\n",
      "[40,    30] loss: 0.107\n",
      "[41,    30] loss: 0.102\n",
      "[42,    30] loss: 0.097\n",
      "[43,    30] loss: 0.092\n",
      "[44,    30] loss: 0.088\n",
      "[45,    30] loss: 0.084\n",
      "[46,    30] loss: 0.080\n",
      "[47,    30] loss: 0.076\n",
      "[48,    30] loss: 0.074\n",
      "[49,    30] loss: 0.072\n",
      "[50,    30] loss: 0.070\n",
      "[51,    30] loss: 0.066\n",
      "[52,    30] loss: 0.065\n",
      "[53,    30] loss: 0.061\n",
      "[54,    30] loss: 0.061\n",
      "[55,    30] loss: 0.059\n",
      "[56,    30] loss: 0.060\n",
      "[57,    30] loss: 0.057\n",
      "[58,    30] loss: 0.055\n",
      "[59,    30] loss: 0.054\n",
      "[60,    30] loss: 0.054\n",
      "[61,    30] loss: 0.051\n",
      "[62,    30] loss: 0.052\n",
      "[63,    30] loss: 0.051\n",
      "[64,    30] loss: 0.049\n",
      "[65,    30] loss: 0.049\n",
      "[66,    30] loss: 0.048\n",
      "[67,    30] loss: 0.048\n",
      "[68,    30] loss: 0.047\n",
      "[69,    30] loss: 0.047\n",
      "[70,    30] loss: 0.045\n",
      "[71,    30] loss: 0.043\n",
      "[72,    30] loss: 0.045\n",
      "[73,    30] loss: 0.044\n",
      "[74,    30] loss: 0.044\n",
      "[75,    30] loss: 0.043\n",
      "[76,    30] loss: 0.041\n",
      "[77,    30] loss: 0.041\n",
      "[78,    30] loss: 0.041\n",
      "[79,    30] loss: 0.040\n",
      "[80,    30] loss: 0.040\n",
      "[81,    30] loss: 0.038\n",
      "[82,    30] loss: 0.038\n",
      "[83,    30] loss: 0.038\n",
      "[84,    30] loss: 0.036\n",
      "[85,    30] loss: 0.036\n",
      "[86,    30] loss: 0.035\n",
      "[87,    30] loss: 0.034\n",
      "[88,    30] loss: 0.035\n",
      "[89,    30] loss: 0.034\n",
      "[90,    30] loss: 0.035\n",
      "[91,    30] loss: 0.034\n",
      "[92,    30] loss: 0.033\n",
      "[93,    30] loss: 0.033\n",
      "[94,    30] loss: 0.033\n",
      "[95,    30] loss: 0.032\n",
      "[96,    30] loss: 0.032\n",
      "[97,    30] loss: 0.032\n",
      "[98,    30] loss: 0.033\n",
      "[99,    30] loss: 0.031\n",
      "[100,    30] loss: 0.030\n",
      "[101,    30] loss: 0.029\n",
      "[102,    30] loss: 0.028\n",
      "[103,    30] loss: 0.029\n",
      "[104,    30] loss: 0.030\n",
      "[105,    30] loss: 0.028\n",
      "[106,    30] loss: 0.028\n",
      "[107,    30] loss: 0.028\n",
      "[108,    30] loss: 0.028\n",
      "[109,    30] loss: 0.026\n",
      "[110,    30] loss: 0.026\n",
      "[111,    30] loss: 0.026\n",
      "[112,    30] loss: 0.026\n",
      "[113,    30] loss: 0.024\n",
      "[114,    30] loss: 0.024\n",
      "[115,    30] loss: 0.025\n",
      "[116,    30] loss: 0.024\n",
      "[117,    30] loss: 0.024\n",
      "[118,    30] loss: 0.024\n",
      "[119,    30] loss: 0.023\n",
      "[120,    30] loss: 0.025\n",
      "[121,    30] loss: 0.024\n",
      "[122,    30] loss: 0.023\n",
      "[123,    30] loss: 0.024\n",
      "[124,    30] loss: 0.022\n",
      "[125,    30] loss: 0.022\n",
      "[126,    30] loss: 0.021\n",
      "[127,    30] loss: 0.022\n",
      "[128,    30] loss: 0.021\n",
      "[129,    30] loss: 0.021\n",
      "[130,    30] loss: 0.020\n",
      "[131,    30] loss: 0.020\n",
      "[132,    30] loss: 0.020\n",
      "[133,    30] loss: 0.019\n",
      "[134,    30] loss: 0.020\n",
      "[135,    30] loss: 0.020\n",
      "[136,    30] loss: 0.018\n",
      "[137,    30] loss: 0.019\n",
      "[138,    30] loss: 0.018\n",
      "[139,    30] loss: 0.019\n",
      "[140,    30] loss: 0.019\n",
      "[141,    30] loss: 0.017\n",
      "[142,    30] loss: 0.019\n",
      "[143,    30] loss: 0.018\n",
      "[144,    30] loss: 0.018\n",
      "[145,    30] loss: 0.018\n",
      "[146,    30] loss: 0.017\n",
      "[147,    30] loss: 0.018\n",
      "[148,    30] loss: 0.017\n",
      "[149,    30] loss: 0.017\n",
      "[150,    30] loss: 0.015\n",
      "[151,    30] loss: 0.017\n",
      "[152,    30] loss: 0.015\n",
      "[153,    30] loss: 0.015\n",
      "[154,    30] loss: 0.015\n",
      "[155,    30] loss: 0.016\n",
      "[156,    30] loss: 0.015\n",
      "[157,    30] loss: 0.015\n",
      "[158,    30] loss: 0.015\n",
      "[159,    30] loss: 0.015\n",
      "[160,    30] loss: 0.015\n",
      "[161,    30] loss: 0.015\n",
      "[162,    30] loss: 0.014\n",
      "[163,    30] loss: 0.014\n",
      "[164,    30] loss: 0.014\n",
      "[165,    30] loss: 0.016\n",
      "[166,    30] loss: 0.014\n",
      "[167,    30] loss: 0.013\n",
      "[168,    30] loss: 0.014\n",
      "[169,    30] loss: 0.013\n",
      "[170,    30] loss: 0.013\n",
      "[171,    30] loss: 0.013\n",
      "[172,    30] loss: 0.013\n",
      "[173,    30] loss: 0.013\n",
      "[174,    30] loss: 0.012\n",
      "[175,    30] loss: 0.010\n",
      "[176,    30] loss: 0.013\n",
      "[177,    30] loss: 0.012\n",
      "[178,    30] loss: 0.012\n",
      "[179,    30] loss: 0.012\n",
      "[180,    30] loss: 0.012\n",
      "[181,    30] loss: 0.011\n",
      "[182,    30] loss: 0.011\n",
      "[183,    30] loss: 0.011\n",
      "[184,    30] loss: 0.012\n",
      "[185,    30] loss: 0.011\n",
      "[186,    30] loss: 0.009\n",
      "[187,    30] loss: 0.010\n",
      "[188,    30] loss: 0.011\n",
      "[189,    30] loss: 0.011\n",
      "[190,    30] loss: 0.010\n",
      "[191,    30] loss: 0.012\n",
      "[192,    30] loss: 0.011\n",
      "[193,    30] loss: 0.010\n",
      "[194,    30] loss: 0.010\n",
      "[195,    30] loss: 0.010\n",
      "[196,    30] loss: 0.009\n",
      "[197,    30] loss: 0.012\n",
      "[198,    30] loss: 0.010\n",
      "[199,    30] loss: 0.009\n",
      "[200,    30] loss: 0.009\n",
      "('Finished Training in:', 919.6592879295349)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(cnn_train_data,0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        #print inputs\n",
    "        #print inputs.shape\n",
    "        optimizer_ft.zero_grad()\n",
    "       # print inputs.type('torch.DoubleTensor')\n",
    "        #inputs=inputs.type('torch.DoubleTensor')\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print outputs.shape\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 30 == 29:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 990))   \n",
    "            running_loss = 0.0\n",
    "end_time = time.time()\n",
    "\n",
    "print('Finished Training in:',end_time-start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 99)\n"
     ]
    }
   ],
   "source": [
    "probs_cnn_train=np.empty([0,99])\n",
    "probs_cnn_test=np.empty([0,99])\n",
    "sm = torch.nn.Softmax()\n",
    "for data in cnn_train_data:\n",
    "    images,labels=data\n",
    "    probs_cnn_train=np.append(probs_cnn_train,sm(net(images)).data.numpy(),axis=0)\n",
    "print probs_cnn_train.shape\n",
    "    \n",
    "for data in cnn_test_data:\n",
    "    images, labels = data\n",
    "    probs_cnn_test=np.append(probs_cnn_test,sm(net(images)).data.numpy(),axis=0)\n",
    "    #outputs=np.append(outputs,net(images).data.numpy(),axis=0)\n",
    "    #print probs\n",
    "    \n",
    "\n",
    "print probs_cnn_test.shape\n",
    "\n",
    "#sm = torch.nn.Softmax()\n",
    "#probabilities = sm(output) \n",
    "#print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Partitioned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the 3 histograms\n",
    "train_margin_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'margin1':'margin64']).values\n",
    "train_shape_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'shape1':'shape64']).values\n",
    "train_texture_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "test_margin_data=((pd.read_csv('data/test.csv')).loc[:,'margin1':'margin64']).values\n",
    "test_shape_data=((pd.read_csv('data/test.csv')).loc[:,'shape1':'shape64']).values\n",
    "test_texture_data=((pd.read_csv('data/test.csv')).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "#print train_margin_data.head()\n",
    "#print train_shape_data.head()\n",
    "#print train_texture_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(images, dense=False):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    des_per_Img = np.array([sift.detectAndCompute(img,None)[1] for img in images])\n",
    "    return des_per_Img\n",
    "        \n",
    "def get_clusters(descriptors, vocabSize):\n",
    "    des_list = np.concatenate(descriptors)\n",
    "\n",
    "    kmeans = MiniBatchKMeans(vocabSize, batch_size=100)\n",
    "    kmeans.fit(np.array(des_list))\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def get_vocabulary(descriptors, clusters, vocabSize):\n",
    "    return np.array([normalize(np.histogram(clusters.predict(dscrs), bins=range(vocabSize))[0].reshape(1,-1)).ravel() for dscrs in descriptors])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors computed in 129.263396 seconds\n"
     ]
    }
   ],
   "source": [
    "des_start_time =time.time()\n",
    "des_list_train = get_descriptor(train_images)\n",
    "des_list_test = get_descriptor(test_images)\n",
    "des_end_time =time.time()\n",
    "print \"Descriptors computed in {:2f} seconds\".format(des_end_time-des_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed in 2.878999 seconds\n"
     ]
    }
   ],
   "source": [
    "clustering_start_time=time.time()\n",
    "clusters = get_clusters(des_list_train,150)\n",
    "clustering_end_time=time.time()\n",
    "print \"Clustering completed in {:2f} seconds\".format(clustering_end_time-clustering_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 149)\n"
     ]
    }
   ],
   "source": [
    "vocab_train = get_vocabulary(des_list_train,clusters,150)\n",
    "vocab_test = get_vocabulary(des_list_test,clusters,150)\n",
    "\n",
    "print vocab_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Weak Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 495)\n"
     ]
    }
   ],
   "source": [
    "mlp_train_margin = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_margin.fit(train_margin_data, train_labels_encoded)\n",
    "mlp_train_margin_pred = mlp_train_margin.predict_proba(train_margin_data)\n",
    "\n",
    "mlp_train_texture = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_texture.fit(train_texture_data, train_labels_encoded)\n",
    "mlp_train_texture_pred = mlp_train_texture.predict_proba(train_texture_data)\n",
    "\n",
    "mlp_train_shape = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_shape.fit(train_shape_data, train_labels_encoded)\n",
    "mlp_train_shape_pred = mlp_train_shape.predict_proba(train_shape_data)\n",
    "\n",
    "mlp_train_sift_bof = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_sift_bof.fit(vocab_train, train_labels_encoded)\n",
    "mlp_train_sift_bof_pred = mlp_train_sift_bof.predict_proba(vocab_train)\n",
    "\n",
    "second_level_input = np.array(np.append(mlp_train_margin_pred,mlp_train_texture_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,mlp_train_shape_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,mlp_train_sift_bof_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,probs_cnn_train,axis=1))\n",
    "print second_level_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the basic boosted network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(520,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=8000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlc_model = MLPClassifier(learning_rate='constant', max_iter=8000,hidden_layer_sizes=(520,))\n",
    "mlc_model.fit(second_level_input, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 495)\n"
     ]
    }
   ],
   "source": [
    "out_file = generateDeepSubmission(test_ids,test_margin_data,test_texture_data,test_shape_data,vocab_test,mlp_train_margin, mlp_train_texture,mlp_train_shape,mlp_train_sift_bof,probs_cnn_test,mlc_model,99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Export the Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDeepSubmission(ids,test_l1,test_l2,test_l3,test_l4\n",
    "                           ,model_b1,model_b2,model_b3,model_b4,cnn_test_data,model_top,num_classes):\n",
    "    num_test = len(test_l1)\n",
    "    block1_pred = model_b1.predict_proba(test_l1)\n",
    "    block2_pred = model_b2.predict_proba(test_l2)\n",
    "    block3_pred = model_b3.predict_proba(test_l3)\n",
    "    block4_pred = model_b4.predict_proba(test_l4)\n",
    "    final_input = np.array(np.append(block1_pred,block2_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,block3_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,block4_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,cnn_test_data,axis=1))\n",
    "    print final_input.shape\n",
    "    final_pred = model_top.predict(final_input)\n",
    "    final_confidence =  model_top.predict_proba(final_input)\n",
    "    final_confidence = np.append(np.array(ids).reshape(-1,1),final_confidence,axis=1)\n",
    "    \n",
    "    return final_confidence\n",
    "    #for i in xrange()\n",
    "    #return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict using KNN\n",
    "# preds_knn=knn_model.predict(test_data)\n",
    "# #predict using SVM\n",
    "# preds_svm=svm_model.decision_function(test_data)\n",
    "# print np.array(preds_svm).shape\n",
    "\n",
    "def generateSubmission(ids, test, model, num_classes):\n",
    "    num_test = len(test)\n",
    "    predictions = model.predict(test)\n",
    "    confidence = model.predict_proba(test)\n",
    "    output = np.zeros((num_test, num_classes+1))\n",
    "    \n",
    "    for i in xrange(num_test):\n",
    "        p = predictions[i]\n",
    "        c = confidence[i][p]\n",
    "        #prob = c\n",
    "        prob = min(max(10e-15, c), 1-10e-15)\n",
    "        logLoss = np.log(prob)\n",
    "        output[i][p+1] = -logLoss / num_test\n",
    "        output[i][0] = ids[i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# out_file = generateSubmission(test_ids, test_data,svm_model,99)\n",
    "headerRow=np.array(['id'] + le.inverse_transform(range(99)).tolist())\n",
    "df = pd.DataFrame(data=out_file, columns = headerRow)\n",
    "df['id'] = df['id'].astype(np.int)\n",
    "df=df.set_index('id')\n",
    "#print df.head()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print out_file\n",
    "df.to_csv('output/26_11_18_003(cnn).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
