{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the relevant Packages\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import pickle\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images first\n",
    "numImages = len(glob.glob('./images/*jpg'))\n",
    "# Create the empty images array\n",
    "images = [None for i in xrange(numImages)]\n",
    "# Find and load all the images relevant to the leaves \n",
    "for fileName in glob.glob('./images/*jpg'):\n",
    "    fileNum = int(fileName[9:][:-4])\n",
    "    images[fileNum-1] = np.array(cv2.imread(fileName))\n",
    "# Make sure the numpy array is an array\n",
    "images = np.array(images)\n",
    "\n",
    "# Load csv data next\n",
    "train_data = pd.read_csv('data/train.csv').drop(['species'], axis=1).values\n",
    "train_labels = pd.read_csv('data/train.csv')['species'].values\n",
    "\n",
    "# Load the training data from the trawled data\n",
    "labels=train_labels.tolist()\n",
    "train_images = [images[int(data[0]-1)] for data in train_data]\n",
    "train_ids = [data[0] for data in train_data]\n",
    "train_data = np.delete(train_data, 0, 1)\n",
    "\n",
    "# Load the test data from the trawled data\n",
    "test_data = pd.read_csv('data/test.csv').values\n",
    "test_images = [images[int(data[0]-1)] for data in test_data]\n",
    "test_ids = [data[0] for data in test_data]\n",
    "test_data = np.delete(test_data, 0, 1)\n",
    "\n",
    "# Delete the physical array of loaded images\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN train data\n",
    "\n",
    "# Define the image normalization\n",
    "def img_norm(img):\n",
    "    t= 2 * (np.float32(img) / 255 - 0.5) # normalize img pixels to [-1, 1]\n",
    "    return t\n",
    "\n",
    "# Define a function which loads data in batches\n",
    "def minibatchData(data,labels_encoded,img_size,channel_num=3,batch_num=30):\n",
    "    images=[]\n",
    "    for img in data:\n",
    "        images.append(np.transpose(img_norm(cv2.resize(img,img_size)),[2,0,1]))\n",
    "    \n",
    "    \n",
    "    if batch_num > 1:\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        print(len(images))\n",
    "        print(batch_num)\n",
    "        \n",
    "        for i in range(int(len(images) / batch_num)):\n",
    "            minibatch_d = images[i*batch_num: (i+1)*batch_num]\n",
    "            minibatch_d = np.reshape(minibatch_d, (batch_num, channel_num,img_size[0],img_size[1]))\n",
    "            batch_data.append(torch.from_numpy(minibatch_d))\n",
    "            if labels_encoded is not None:\n",
    "                minibatch_l = labels_encoded[i*batch_num: (i+1)*batch_num]\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "            else:\n",
    "                minibatch_l = np.zeros(batch_num)\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "        #data, labels = batch_data, batch_labels \n",
    "        \n",
    "    return zip(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels for preprocessing\n",
    "\n",
    "le= preprocessing.LabelEncoder()\n",
    "#encode train labels\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded=le.transform(train_labels)\n",
    "Y_labels = np_utils.to_categorical(train_labels_encoded,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Create the minibatch dataset based on the train images\n",
    "\n",
    "img_size=(224,224)\n",
    "cnn_train_data = list(minibatchData(train_images,train_labels_encoded,img_size))\n",
    "#plt.imshow(cnn_train_data[0][0][3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a82b6dc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the minibatch with different paramters\n",
    "\n",
    "cnn_test_data = list(minibatchData(test_images,None,img_size,batch_num=2))\n",
    "#print cnn_train_data.size\n",
    "plt.imshow(cnn_test_data[0][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tuned alex-network into the RAM directly from the pickled instance\n",
    "\n",
    "filename='tunedAlex.sav'\n",
    "model_ft = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 99)\n"
     ]
    }
   ],
   "source": [
    "# Create the input space / labels for the CNN\n",
    "\n",
    "probs_cnn_train=np.empty([0,99])\n",
    "#sm = torch.nn.Softmax()\n",
    "for data in cnn_train_data:\n",
    "    images,labels=data\n",
    "    probs_cnn_train=np.append(probs_cnn_train,(model_ft(images)).data.numpy(),axis=0)\n",
    "print probs_cnn_train.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 99)\n"
     ]
    }
   ],
   "source": [
    "# Create the input space for the testing of the CNN\n",
    "\n",
    "probs_cnn_test=np.empty([0,99])\n",
    "for data in cnn_test_data:\n",
    "    images, labels = data\n",
    "    probs_cnn_test=np.append(probs_cnn_test,(model_ft(images)).data.numpy(),axis=0)\n",
    "    #outputs=np.append(outputs,net(images).data.numpy(),axis=0)\n",
    "    #print probs\n",
    "    \n",
    "\n",
    "print probs_cnn_test.shape\n",
    "\n",
    "#sm = torch.nn.Softmax()\n",
    "#probabilities = sm(output) \n",
    "#print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate the 3 histograms given to us in the data for training\n",
    "\n",
    "train_margin_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'margin1':'margin64']).values\n",
    "train_shape_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'shape1':'shape64']).values\n",
    "train_texture_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "pca_shape = PCA(n_components=0.99, whiten=True)\n",
    "pca_shape = pca_shape.fit(train_shape_data)\n",
    "\n",
    "train_shape_data = pca_shape.transform(train_shape_data)\n",
    "train_shape_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the 3 histograms iven to us in the data for testing\n",
    "\n",
    "test_margin_data=((pd.read_csv('data/test.csv')).loc[:,'margin1':'margin64']).values\n",
    "test_shape_data=((pd.read_csv('data/test.csv')).loc[:,'shape1':'shape64']).values\n",
    "test_texture_data=((pd.read_csv('data/test.csv')).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "\n",
    "test_shape_data = pca_shape.transform(test_shape_data)\n",
    "test_shape_data.shape\n",
    "#print train_margin_data.head()\n",
    "#print train_shape_data.head()\n",
    "#print train_texture_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BagOfWords SIFT Descriptors\n",
    "\n",
    "def get_descriptor(images, dense=False):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    des_per_Img = np.array([sift.detectAndCompute(img,None)[1] for img in images])\n",
    "    return des_per_Img\n",
    "        \n",
    "def get_clusters(descriptors, vocabSize):\n",
    "    des_list = np.concatenate(descriptors)\n",
    "\n",
    "    kmeans = MiniBatchKMeans(vocabSize, batch_size=100)\n",
    "    kmeans.fit(np.array(des_list))\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def get_vocabulary(descriptors, clusters, vocabSize):\n",
    "    return np.array([normalize(np.histogram(clusters.predict(dscrs), bins=range(vocabSize))[0].reshape(1,-1)).ravel() for dscrs in descriptors])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors computed in 139.248525 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create and run the vocabulary for testing/training images\n",
    "\n",
    "des_start_time =time.time()\n",
    "des_list_train = get_descriptor(train_images)\n",
    "\n",
    "des_list_test = get_descriptor(test_images)\n",
    "des_end_time =time.time()\n",
    "print \"Descriptors computed in {:2f} seconds\".format(des_end_time-des_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed in 3.207614 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the clusters for the BagOfWords\n",
    "\n",
    "clustering_start_time=time.time()\n",
    "clusters = get_clusters(des_list_train,150)\n",
    "clustering_end_time=time.time()\n",
    "print \"Clustering completed in {:2f} seconds\".format(clustering_end_time-clustering_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 149)\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary for each feature for the test/train space\n",
    "\n",
    "vocab_train = get_vocabulary(des_list_train,clusters,150)\n",
    "vocab_test = get_vocabulary(des_list_test,clusters,150)\n",
    "\n",
    "print vocab_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the vocabulary\n",
    "\n",
    "pca_vocab = PCA(n_components=0.99, whiten=True)\n",
    "pca_vocab.fit(vocab_train)\n",
    "vocab_train = pca_vocab.transform(vocab_train)\n",
    "vocab_test = pca_vocab.transform(vocab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the packages for KERAS\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "from keras.utils import np_utils\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weak learner networks for each of the logical partitions of data\n",
    "\n",
    "deep_shape = Sequential([Dense(64, activation='relu'), Dropout(0.7), Dense(128, activation='relu'), Dense(99, activation='softmax')])\n",
    "deep_texture = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dropout(0.5), Dense(99, activation='softmax')])\n",
    "deep_margin = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dense(99, activation='softmax')])\n",
    "deep_sift = Sequential([Dense(128, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dropout(0.5), Dense(99, activation='softmax')])\n",
    "deep_alex = Sequential([Dense(256, activation='relu'), Dropout(0.5), Dense(128, activation='relu'), Dropout(0.5), Dense(99, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the weak learner networks\n",
    "\n",
    "deep_shape.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_texture.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_margin.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_sift.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])\n",
    "deep_alex.compile(loss='categorical_crossentropy', optimizer='adam', metric=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/15\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 5.0431 - val_loss: 4.6459\n",
      "Epoch 2/15\n",
      "792/792 [==============================] - 0s 302us/step - loss: 4.5984 - val_loss: 4.5855\n",
      "Epoch 3/15\n",
      "792/792 [==============================] - 0s 322us/step - loss: 4.4754 - val_loss: 4.5383\n",
      "Epoch 4/15\n",
      "792/792 [==============================] - 0s 304us/step - loss: 4.3096 - val_loss: 4.4851\n",
      "Epoch 5/15\n",
      "792/792 [==============================] - 0s 395us/step - loss: 4.1779 - val_loss: 4.4145\n",
      "Epoch 6/15\n",
      "792/792 [==============================] - 0s 316us/step - loss: 3.9534 - val_loss: 4.3246\n",
      "Epoch 7/15\n",
      "792/792 [==============================] - 0s 298us/step - loss: 3.8303 - val_loss: 4.1984\n",
      "Epoch 8/15\n",
      "792/792 [==============================] - 0s 302us/step - loss: 3.5194 - val_loss: 4.0365\n",
      "Epoch 9/15\n",
      "792/792 [==============================] - 0s 324us/step - loss: 3.3264 - val_loss: 3.8740\n",
      "Epoch 10/15\n",
      "792/792 [==============================] - 0s 362us/step - loss: 3.0795 - val_loss: 3.7323\n",
      "Epoch 11/15\n",
      "792/792 [==============================] - 0s 353us/step - loss: 2.8482 - val_loss: 3.5547\n",
      "Epoch 12/15\n",
      "792/792 [==============================] - 0s 354us/step - loss: 2.6676 - val_loss: 3.3948\n",
      "Epoch 13/15\n",
      "792/792 [==============================] - 0s 366us/step - loss: 2.5559 - val_loss: 3.2977\n",
      "Epoch 14/15\n",
      "792/792 [==============================] - 0s 312us/step - loss: 2.4382 - val_loss: 3.1479\n",
      "Epoch 15/15\n",
      "792/792 [==============================] - 0s 333us/step - loss: 2.1434 - val_loss: 3.0358\n",
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/40\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 4.6189 - val_loss: 4.4285\n",
      "Epoch 2/40\n",
      "792/792 [==============================] - 1s 663us/step - loss: 4.2729 - val_loss: 4.1651\n",
      "Epoch 3/40\n",
      "792/792 [==============================] - 1s 747us/step - loss: 3.9871 - val_loss: 3.8105\n",
      "Epoch 4/40\n",
      "792/792 [==============================] - 1s 706us/step - loss: 3.5824 - val_loss: 3.3975\n",
      "Epoch 5/40\n",
      "792/792 [==============================] - 1s 733us/step - loss: 3.3106 - val_loss: 3.0834\n",
      "Epoch 6/40\n",
      "792/792 [==============================] - 1s 690us/step - loss: 3.1282 - val_loss: 2.8941\n",
      "Epoch 7/40\n",
      "792/792 [==============================] - 1s 653us/step - loss: 3.0063 - val_loss: 2.7502\n",
      "Epoch 8/40\n",
      "792/792 [==============================] - 1s 675us/step - loss: 2.9282 - val_loss: 2.6829\n",
      "Epoch 9/40\n",
      "792/792 [==============================] - 1s 726us/step - loss: 2.8229 - val_loss: 2.5939\n",
      "Epoch 10/40\n",
      "792/792 [==============================] - 1s 737us/step - loss: 2.7440 - val_loss: 2.4966\n",
      "Epoch 11/40\n",
      "792/792 [==============================] - 1s 842us/step - loss: 2.6973 - val_loss: 2.3964\n",
      "Epoch 12/40\n",
      "792/792 [==============================] - 1s 774us/step - loss: 2.6214 - val_loss: 2.4077\n",
      "Epoch 13/40\n",
      "792/792 [==============================] - 0s 613us/step - loss: 2.5791 - val_loss: 2.3434\n",
      "Epoch 14/40\n",
      "792/792 [==============================] - 1s 643us/step - loss: 2.5364 - val_loss: 2.3037\n",
      "Epoch 15/40\n",
      "792/792 [==============================] - 1s 749us/step - loss: 2.5093 - val_loss: 2.3027\n",
      "Epoch 16/40\n",
      "792/792 [==============================] - 0s 617us/step - loss: 2.4240 - val_loss: 2.2936\n",
      "Epoch 17/40\n",
      "792/792 [==============================] - 0s 603us/step - loss: 2.4463 - val_loss: 2.2055\n",
      "Epoch 18/40\n",
      "792/792 [==============================] - 1s 670us/step - loss: 2.3618 - val_loss: 2.2090\n",
      "Epoch 19/40\n",
      "792/792 [==============================] - 1s 674us/step - loss: 2.3717 - val_loss: 2.1904\n",
      "Epoch 20/40\n",
      "792/792 [==============================] - 1s 644us/step - loss: 2.3679 - val_loss: 2.1688\n",
      "Epoch 21/40\n",
      "792/792 [==============================] - 0s 603us/step - loss: 2.3114 - val_loss: 2.1622\n",
      "Epoch 22/40\n",
      "792/792 [==============================] - 0s 610us/step - loss: 2.3019 - val_loss: 2.1224\n",
      "Epoch 23/40\n",
      "792/792 [==============================] - 0s 617us/step - loss: 2.2843 - val_loss: 2.1397\n",
      "Epoch 24/40\n",
      "792/792 [==============================] - 0s 597us/step - loss: 2.2527 - val_loss: 2.1478\n",
      "Epoch 25/40\n",
      "792/792 [==============================] - 0s 601us/step - loss: 2.2743 - val_loss: 2.0762\n",
      "Epoch 26/40\n",
      "792/792 [==============================] - 0s 603us/step - loss: 2.2447 - val_loss: 2.0967\n",
      "Epoch 27/40\n",
      "792/792 [==============================] - 0s 603us/step - loss: 2.1865 - val_loss: 2.0468\n",
      "Epoch 28/40\n",
      "792/792 [==============================] - 0s 608us/step - loss: 2.2248 - val_loss: 2.0561\n",
      "Epoch 29/40\n",
      "792/792 [==============================] - 0s 604us/step - loss: 2.1519 - val_loss: 2.0171\n",
      "Epoch 30/40\n",
      "792/792 [==============================] - 0s 611us/step - loss: 2.1385 - val_loss: 1.9731\n",
      "Epoch 31/40\n",
      "792/792 [==============================] - 0s 604us/step - loss: 2.1062 - val_loss: 2.0165\n",
      "Epoch 32/40\n",
      "792/792 [==============================] - 0s 609us/step - loss: 2.0616 - val_loss: 2.0299\n",
      "Epoch 33/40\n",
      "792/792 [==============================] - 0s 607us/step - loss: 2.0678 - val_loss: 2.0076\n",
      "Epoch 34/40\n",
      "792/792 [==============================] - 0s 600us/step - loss: 2.1369 - val_loss: 1.9342\n",
      "Epoch 35/40\n",
      "792/792 [==============================] - 1s 672us/step - loss: 2.0709 - val_loss: 1.9523\n",
      "Epoch 36/40\n",
      "792/792 [==============================] - 1s 885us/step - loss: 2.0728 - val_loss: 1.9363\n",
      "Epoch 37/40\n",
      "792/792 [==============================] - 1s 661us/step - loss: 2.1198 - val_loss: 1.9983\n",
      "Epoch 38/40\n",
      "792/792 [==============================] - 1s 775us/step - loss: 1.9796 - val_loss: 1.9756\n",
      "Epoch 39/40\n",
      "792/792 [==============================] - 1s 828us/step - loss: 2.0377 - val_loss: 2.0172\n",
      "Epoch 40/40\n",
      "792/792 [==============================] - 1s 796us/step - loss: 2.0234 - val_loss: 1.8785\n",
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/150\n",
      "792/792 [==============================] - 2s 3ms/step - loss: 4.5954 - val_loss: 4.5942\n",
      "Epoch 2/150\n",
      "792/792 [==============================] - 0s 399us/step - loss: 4.5782 - val_loss: 4.5823\n",
      "Epoch 3/150\n",
      "792/792 [==============================] - 0s 397us/step - loss: 4.4522 - val_loss: 4.3824\n",
      "Epoch 4/150\n",
      "792/792 [==============================] - 0s 400us/step - loss: 4.0632 - val_loss: 4.0731\n",
      "Epoch 5/150\n",
      "792/792 [==============================] - 0s 392us/step - loss: 3.7066 - val_loss: 3.6189\n",
      "Epoch 6/150\n",
      "792/792 [==============================] - 0s 400us/step - loss: 3.3050 - val_loss: 3.1330\n",
      "Epoch 7/150\n",
      "792/792 [==============================] - 0s 394us/step - loss: 3.0543 - val_loss: 2.8874\n",
      "Epoch 8/150\n",
      "792/792 [==============================] - 0s 395us/step - loss: 2.8692 - val_loss: 2.6980\n",
      "Epoch 9/150\n",
      "792/792 [==============================] - 0s 460us/step - loss: 2.6779 - val_loss: 2.4846\n",
      "Epoch 10/150\n",
      "792/792 [==============================] - 0s 554us/step - loss: 2.5776 - val_loss: 2.4374\n",
      "Epoch 11/150\n",
      "792/792 [==============================] - 0s 489us/step - loss: 2.4196 - val_loss: 2.2077\n",
      "Epoch 12/150\n",
      "792/792 [==============================] - 0s 465us/step - loss: 2.2728 - val_loss: 2.1260\n",
      "Epoch 13/150\n",
      "792/792 [==============================] - 0s 473us/step - loss: 2.1531 - val_loss: 1.9603\n",
      "Epoch 14/150\n",
      "792/792 [==============================] - 0s 441us/step - loss: 2.0909 - val_loss: 1.8966\n",
      "Epoch 15/150\n",
      "792/792 [==============================] - 0s 396us/step - loss: 2.0843 - val_loss: 1.8898\n",
      "Epoch 16/150\n",
      "792/792 [==============================] - 0s 482us/step - loss: 1.9485 - val_loss: 1.7006\n",
      "Epoch 17/150\n",
      "792/792 [==============================] - 0s 515us/step - loss: 1.9348 - val_loss: 1.6536\n",
      "Epoch 18/150\n",
      "792/792 [==============================] - 0s 443us/step - loss: 1.7909 - val_loss: 1.5819\n",
      "Epoch 19/150\n",
      "792/792 [==============================] - 0s 419us/step - loss: 1.7804 - val_loss: 1.5564\n",
      "Epoch 20/150\n",
      "792/792 [==============================] - 0s 554us/step - loss: 1.6996 - val_loss: 1.5105\n",
      "Epoch 21/150\n",
      "792/792 [==============================] - 0s 406us/step - loss: 1.7082 - val_loss: 1.4721\n",
      "Epoch 22/150\n",
      "792/792 [==============================] - 0s 409us/step - loss: 1.6400 - val_loss: 1.4349\n",
      "Epoch 23/150\n",
      "792/792 [==============================] - 0s 541us/step - loss: 1.5689 - val_loss: 1.3792\n",
      "Epoch 24/150\n",
      "792/792 [==============================] - 0s 456us/step - loss: 1.6125 - val_loss: 1.3890\n",
      "Epoch 25/150\n",
      "792/792 [==============================] - 0s 384us/step - loss: 1.5761 - val_loss: 1.3547\n",
      "Epoch 26/150\n",
      "792/792 [==============================] - 0s 414us/step - loss: 1.4962 - val_loss: 1.3843\n",
      "Epoch 27/150\n",
      "792/792 [==============================] - 0s 542us/step - loss: 1.5167 - val_loss: 1.3280\n",
      "Epoch 28/150\n",
      "792/792 [==============================] - 0s 492us/step - loss: 1.4643 - val_loss: 1.3211\n",
      "Epoch 29/150\n",
      "792/792 [==============================] - 0s 560us/step - loss: 1.4295 - val_loss: 1.2965\n",
      "Epoch 30/150\n",
      "792/792 [==============================] - 0s 442us/step - loss: 1.4650 - val_loss: 1.2897\n",
      "Epoch 31/150\n",
      "792/792 [==============================] - 0s 411us/step - loss: 1.3966 - val_loss: 1.3072\n",
      "Epoch 32/150\n",
      "792/792 [==============================] - 0s 473us/step - loss: 1.4234 - val_loss: 1.2078\n",
      "Epoch 33/150\n",
      "792/792 [==============================] - 0s 492us/step - loss: 1.3726 - val_loss: 1.1848\n",
      "Epoch 34/150\n",
      "792/792 [==============================] - 0s 399us/step - loss: 1.3150 - val_loss: 1.2067\n",
      "Epoch 35/150\n",
      "792/792 [==============================] - 0s 413us/step - loss: 1.3619 - val_loss: 1.1520\n",
      "Epoch 36/150\n",
      "792/792 [==============================] - 0s 387us/step - loss: 1.3306 - val_loss: 1.1519\n",
      "Epoch 37/150\n",
      "792/792 [==============================] - 0s 398us/step - loss: 1.2562 - val_loss: 1.1658\n",
      "Epoch 38/150\n",
      "792/792 [==============================] - 0s 384us/step - loss: 1.2492 - val_loss: 1.1322\n",
      "Epoch 39/150\n",
      "792/792 [==============================] - 0s 394us/step - loss: 1.2601 - val_loss: 1.1226\n",
      "Epoch 40/150\n",
      "792/792 [==============================] - 0s 391us/step - loss: 1.2078 - val_loss: 1.0965\n",
      "Epoch 41/150\n",
      "792/792 [==============================] - 0s 394us/step - loss: 1.2141 - val_loss: 1.0817\n",
      "Epoch 42/150\n",
      "792/792 [==============================] - 0s 392us/step - loss: 1.2195 - val_loss: 1.0853\n",
      "Epoch 43/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 1.2055 - val_loss: 1.0967\n",
      "Epoch 44/150\n",
      "792/792 [==============================] - 0s 389us/step - loss: 1.1973 - val_loss: 1.0656\n",
      "Epoch 45/150\n",
      "792/792 [==============================] - 0s 392us/step - loss: 1.1455 - val_loss: 1.0725\n",
      "Epoch 46/150\n",
      "792/792 [==============================] - 0s 394us/step - loss: 1.1068 - val_loss: 1.0606\n",
      "Epoch 47/150\n",
      "792/792 [==============================] - 0s 488us/step - loss: 1.1338 - val_loss: 1.0361\n",
      "Epoch 48/150\n",
      "792/792 [==============================] - 0s 469us/step - loss: 1.1934 - val_loss: 1.0859\n",
      "Epoch 49/150\n",
      "792/792 [==============================] - 0s 418us/step - loss: 1.0978 - val_loss: 1.0211\n",
      "Epoch 50/150\n",
      "792/792 [==============================] - 0s 391us/step - loss: 1.0952 - val_loss: 1.0443\n",
      "Epoch 51/150\n",
      "792/792 [==============================] - 0s 420us/step - loss: 1.0804 - val_loss: 1.0486\n",
      "Epoch 52/150\n",
      "792/792 [==============================] - 0s 492us/step - loss: 1.0391 - val_loss: 1.0260\n",
      "Epoch 53/150\n",
      "792/792 [==============================] - 0s 509us/step - loss: 1.0914 - val_loss: 1.0783\n",
      "Epoch 54/150\n",
      "792/792 [==============================] - 0s 518us/step - loss: 1.0467 - val_loss: 1.0324\n",
      "Epoch 55/150\n",
      "792/792 [==============================] - 0s 433us/step - loss: 1.0654 - val_loss: 1.0320\n",
      "Epoch 56/150\n",
      "792/792 [==============================] - 0s 502us/step - loss: 1.0809 - val_loss: 1.0073\n",
      "Epoch 57/150\n",
      "792/792 [==============================] - 0s 496us/step - loss: 1.0308 - val_loss: 0.9772\n",
      "Epoch 58/150\n",
      "792/792 [==============================] - 0s 434us/step - loss: 1.0382 - val_loss: 0.9066\n",
      "Epoch 59/150\n",
      "792/792 [==============================] - 0s 449us/step - loss: 0.9840 - val_loss: 0.9742\n",
      "Epoch 60/150\n",
      "792/792 [==============================] - 0s 404us/step - loss: 1.0304 - val_loss: 0.9417\n",
      "Epoch 61/150\n",
      "792/792 [==============================] - 0s 531us/step - loss: 1.0049 - val_loss: 1.0074\n",
      "Epoch 62/150\n",
      "792/792 [==============================] - 0s 468us/step - loss: 1.0011 - val_loss: 0.9731\n",
      "Epoch 63/150\n",
      "792/792 [==============================] - 0s 517us/step - loss: 0.9693 - val_loss: 0.9702\n",
      "Epoch 64/150\n",
      "792/792 [==============================] - 0s 503us/step - loss: 0.9691 - val_loss: 0.9074\n",
      "Epoch 65/150\n",
      "792/792 [==============================] - 0s 416us/step - loss: 0.9735 - val_loss: 0.9410\n",
      "Epoch 66/150\n",
      "792/792 [==============================] - 0s 482us/step - loss: 0.9501 - val_loss: 0.9090\n",
      "Epoch 67/150\n",
      "792/792 [==============================] - 0s 515us/step - loss: 0.9824 - val_loss: 0.9694\n",
      "Epoch 68/150\n",
      "792/792 [==============================] - 0s 464us/step - loss: 0.9338 - val_loss: 0.9560\n",
      "Epoch 69/150\n",
      "792/792 [==============================] - 0s 473us/step - loss: 0.9430 - val_loss: 0.9304\n",
      "Epoch 70/150\n",
      "792/792 [==============================] - 0s 472us/step - loss: 0.9580 - val_loss: 0.8902\n",
      "Epoch 71/150\n",
      "792/792 [==============================] - 0s 431us/step - loss: 0.8872 - val_loss: 0.9135\n",
      "Epoch 72/150\n",
      "792/792 [==============================] - 0s 415us/step - loss: 0.8819 - val_loss: 0.8872\n",
      "Epoch 73/150\n",
      "792/792 [==============================] - 0s 481us/step - loss: 0.9330 - val_loss: 0.9993\n",
      "Epoch 74/150\n",
      "792/792 [==============================] - 0s 497us/step - loss: 0.8527 - val_loss: 0.9285\n",
      "Epoch 75/150\n",
      "792/792 [==============================] - 0s 458us/step - loss: 0.9294 - val_loss: 0.9473\n",
      "Epoch 76/150\n",
      "792/792 [==============================] - 0s 406us/step - loss: 0.9256 - val_loss: 0.9491\n",
      "Epoch 77/150\n",
      "792/792 [==============================] - 0s 519us/step - loss: 0.8696 - val_loss: 0.9079\n",
      "Epoch 78/150\n",
      "792/792 [==============================] - 0s 456us/step - loss: 0.8652 - val_loss: 0.9385\n",
      "Epoch 79/150\n",
      "792/792 [==============================] - 0s 470us/step - loss: 0.8619 - val_loss: 0.9467\n",
      "Epoch 80/150\n",
      "792/792 [==============================] - 0s 450us/step - loss: 0.8661 - val_loss: 0.8984\n",
      "Epoch 81/150\n",
      "792/792 [==============================] - 0s 533us/step - loss: 0.8549 - val_loss: 0.8708\n",
      "Epoch 82/150\n",
      "792/792 [==============================] - 0s 428us/step - loss: 0.8434 - val_loss: 0.8690\n",
      "Epoch 83/150\n",
      "792/792 [==============================] - 0s 449us/step - loss: 0.8505 - val_loss: 0.9068\n",
      "Epoch 84/150\n",
      "792/792 [==============================] - 0s 441us/step - loss: 0.8906 - val_loss: 0.9039\n",
      "Epoch 85/150\n",
      "792/792 [==============================] - 0s 492us/step - loss: 0.8114 - val_loss: 0.8851\n",
      "Epoch 86/150\n",
      "792/792 [==============================] - 0s 478us/step - loss: 0.8037 - val_loss: 0.9045\n",
      "Epoch 87/150\n",
      "792/792 [==============================] - 0s 506us/step - loss: 0.8512 - val_loss: 0.9542\n",
      "Epoch 88/150\n",
      "792/792 [==============================] - 0s 446us/step - loss: 0.8240 - val_loss: 0.8999\n",
      "Epoch 89/150\n",
      "792/792 [==============================] - 0s 386us/step - loss: 0.7881 - val_loss: 0.9054\n",
      "Epoch 90/150\n",
      "792/792 [==============================] - 0s 414us/step - loss: 0.8119 - val_loss: 0.8501\n",
      "Epoch 91/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.7965 - val_loss: 0.8907\n",
      "Epoch 92/150\n",
      "792/792 [==============================] - 0s 392us/step - loss: 0.7918 - val_loss: 0.9288\n",
      "Epoch 93/150\n",
      "792/792 [==============================] - 0s 396us/step - loss: 0.7561 - val_loss: 0.9118\n",
      "Epoch 94/150\n",
      "792/792 [==============================] - 0s 386us/step - loss: 0.8181 - val_loss: 0.8845\n",
      "Epoch 95/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.7756 - val_loss: 0.8457\n",
      "Epoch 96/150\n",
      "792/792 [==============================] - 0s 394us/step - loss: 0.7612 - val_loss: 0.9101\n",
      "Epoch 97/150\n",
      "792/792 [==============================] - 0s 530us/step - loss: 0.8133 - val_loss: 0.8394\n",
      "Epoch 98/150\n",
      "792/792 [==============================] - 0s 394us/step - loss: 0.7604 - val_loss: 0.8584\n",
      "Epoch 99/150\n",
      "792/792 [==============================] - 0s 530us/step - loss: 0.7266 - val_loss: 0.9117\n",
      "Epoch 100/150\n",
      "792/792 [==============================] - 0s 395us/step - loss: 0.7269 - val_loss: 0.9261\n",
      "Epoch 101/150\n",
      "792/792 [==============================] - 0s 557us/step - loss: 0.7564 - val_loss: 0.9019\n",
      "Epoch 102/150\n",
      "792/792 [==============================] - 0s 402us/step - loss: 0.7336 - val_loss: 0.8597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/150\n",
      "792/792 [==============================] - 0s 400us/step - loss: 0.7185 - val_loss: 0.8835\n",
      "Epoch 104/150\n",
      "792/792 [==============================] - 0s 532us/step - loss: 0.7044 - val_loss: 0.8620\n",
      "Epoch 105/150\n",
      "792/792 [==============================] - 0s 493us/step - loss: 0.7187 - val_loss: 0.8576\n",
      "Epoch 106/150\n",
      "792/792 [==============================] - 0s 557us/step - loss: 0.7470 - val_loss: 0.8679\n",
      "Epoch 107/150\n",
      "792/792 [==============================] - 0s 420us/step - loss: 0.7053 - val_loss: 0.9281\n",
      "Epoch 108/150\n",
      "792/792 [==============================] - 0s 389us/step - loss: 0.6839 - val_loss: 0.9067\n",
      "Epoch 109/150\n",
      "792/792 [==============================] - 0s 513us/step - loss: 0.7549 - val_loss: 0.8948\n",
      "Epoch 110/150\n",
      "792/792 [==============================] - 0s 479us/step - loss: 0.6827 - val_loss: 0.8965\n",
      "Epoch 111/150\n",
      "792/792 [==============================] - 0s 458us/step - loss: 0.7054 - val_loss: 0.8807\n",
      "Epoch 112/150\n",
      "792/792 [==============================] - 0s 393us/step - loss: 0.6689 - val_loss: 0.8261\n",
      "Epoch 113/150\n",
      "792/792 [==============================] - 0s 414us/step - loss: 0.6966 - val_loss: 0.8391\n",
      "Epoch 114/150\n",
      "792/792 [==============================] - 0s 422us/step - loss: 0.7058 - val_loss: 0.8474\n",
      "Epoch 115/150\n",
      "792/792 [==============================] - 0s 399us/step - loss: 0.6661 - val_loss: 0.8786\n",
      "Epoch 116/150\n",
      "792/792 [==============================] - 0s 413us/step - loss: 0.6532 - val_loss: 0.8186\n",
      "Epoch 117/150\n",
      "792/792 [==============================] - 0s 407us/step - loss: 0.6589 - val_loss: 0.8887\n",
      "Epoch 118/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 0.6714 - val_loss: 0.8222\n",
      "Epoch 119/150\n",
      "792/792 [==============================] - 0s 397us/step - loss: 0.6593 - val_loss: 0.8683\n",
      "Epoch 120/150\n",
      "792/792 [==============================] - 0s 428us/step - loss: 0.7229 - val_loss: 0.8636\n",
      "Epoch 121/150\n",
      "792/792 [==============================] - 0s 410us/step - loss: 0.6484 - val_loss: 0.8093\n",
      "Epoch 122/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.6840 - val_loss: 0.9098\n",
      "Epoch 123/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.6386 - val_loss: 0.9560\n",
      "Epoch 124/150\n",
      "792/792 [==============================] - 0s 384us/step - loss: 0.6252 - val_loss: 0.8499\n",
      "Epoch 125/150\n",
      "792/792 [==============================] - 0s 384us/step - loss: 0.6524 - val_loss: 0.8419\n",
      "Epoch 126/150\n",
      "792/792 [==============================] - 0s 385us/step - loss: 0.6681 - val_loss: 0.8860\n",
      "Epoch 127/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.6231 - val_loss: 0.8471\n",
      "Epoch 128/150\n",
      "792/792 [==============================] - 0s 407us/step - loss: 0.6457 - val_loss: 0.8609\n",
      "Epoch 129/150\n",
      "792/792 [==============================] - 0s 403us/step - loss: 0.6383 - val_loss: 0.9005\n",
      "Epoch 130/150\n",
      "792/792 [==============================] - 0s 398us/step - loss: 0.6258 - val_loss: 0.8400\n",
      "Epoch 131/150\n",
      "792/792 [==============================] - 0s 387us/step - loss: 0.5802 - val_loss: 0.9165\n",
      "Epoch 132/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 0.5872 - val_loss: 0.8899\n",
      "Epoch 133/150\n",
      "792/792 [==============================] - 0s 387us/step - loss: 0.5892 - val_loss: 0.8616\n",
      "Epoch 134/150\n",
      "792/792 [==============================] - 0s 387us/step - loss: 0.5401 - val_loss: 0.8471\n",
      "Epoch 135/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 0.5580 - val_loss: 0.9137\n",
      "Epoch 136/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.6004 - val_loss: 0.8847\n",
      "Epoch 137/150\n",
      "792/792 [==============================] - 0s 386us/step - loss: 0.5735 - val_loss: 0.8730\n",
      "Epoch 138/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.5883 - val_loss: 0.8753\n",
      "Epoch 139/150\n",
      "792/792 [==============================] - 0s 409us/step - loss: 0.6367 - val_loss: 0.8732\n",
      "Epoch 140/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 0.5968 - val_loss: 0.8904\n",
      "Epoch 141/150\n",
      "792/792 [==============================] - 0s 426us/step - loss: 0.5501 - val_loss: 0.8420\n",
      "Epoch 142/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.5521 - val_loss: 0.8962\n",
      "Epoch 143/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.5920 - val_loss: 0.8593\n",
      "Epoch 144/150\n",
      "792/792 [==============================] - 0s 393us/step - loss: 0.5726 - val_loss: 0.8704\n",
      "Epoch 145/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 0.5896 - val_loss: 0.8016\n",
      "Epoch 146/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.5659 - val_loss: 0.8479\n",
      "Epoch 147/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 0.5632 - val_loss: 0.8611\n",
      "Epoch 148/150\n",
      "792/792 [==============================] - 0s 400us/step - loss: 0.5703 - val_loss: 0.8518\n",
      "Epoch 149/150\n",
      "792/792 [==============================] - 0s 504us/step - loss: 0.5747 - val_loss: 0.8470\n",
      "Epoch 150/150\n",
      "792/792 [==============================] - 0s 506us/step - loss: 0.5458 - val_loss: 0.9269\n",
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/150\n",
      "792/792 [==============================] - 2s 3ms/step - loss: 7.6184 - val_loss: 4.1047\n",
      "Epoch 2/150\n",
      "792/792 [==============================] - 0s 474us/step - loss: 4.5903 - val_loss: 3.9193\n",
      "Epoch 3/150\n",
      "792/792 [==============================] - 0s 357us/step - loss: 3.9874 - val_loss: 3.4317\n",
      "Epoch 4/150\n",
      "792/792 [==============================] - 0s 405us/step - loss: 3.5336 - val_loss: 2.9747\n",
      "Epoch 5/150\n",
      "792/792 [==============================] - 0s 402us/step - loss: 3.0255 - val_loss: 2.3765\n",
      "Epoch 6/150\n",
      "792/792 [==============================] - 0s 461us/step - loss: 2.6538 - val_loss: 1.9037\n",
      "Epoch 7/150\n",
      "792/792 [==============================] - 0s 412us/step - loss: 2.4324 - val_loss: 1.5338\n",
      "Epoch 8/150\n",
      "792/792 [==============================] - 0s 414us/step - loss: 2.2031 - val_loss: 1.3340\n",
      "Epoch 9/150\n",
      "792/792 [==============================] - 0s 427us/step - loss: 1.9045 - val_loss: 1.0848\n",
      "Epoch 10/150\n",
      "792/792 [==============================] - 0s 449us/step - loss: 1.7336 - val_loss: 0.9223\n",
      "Epoch 11/150\n",
      "792/792 [==============================] - 0s 506us/step - loss: 1.5388 - val_loss: 0.7538\n",
      "Epoch 12/150\n",
      "792/792 [==============================] - 0s 416us/step - loss: 1.4101 - val_loss: 0.7129\n",
      "Epoch 13/150\n",
      "792/792 [==============================] - 0s 436us/step - loss: 1.3338 - val_loss: 0.6736\n",
      "Epoch 14/150\n",
      "792/792 [==============================] - 0s 451us/step - loss: 1.3299 - val_loss: 0.6438\n",
      "Epoch 15/150\n",
      "792/792 [==============================] - 0s 453us/step - loss: 1.2037 - val_loss: 0.5316\n",
      "Epoch 16/150\n",
      "792/792 [==============================] - 0s 393us/step - loss: 1.0610 - val_loss: 0.4833\n",
      "Epoch 17/150\n",
      "792/792 [==============================] - 0s 355us/step - loss: 1.0951 - val_loss: 0.5030\n",
      "Epoch 18/150\n",
      "792/792 [==============================] - 0s 383us/step - loss: 1.0232 - val_loss: 0.4468\n",
      "Epoch 19/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.9831 - val_loss: 0.4267\n",
      "Epoch 20/150\n",
      "792/792 [==============================] - 0s 418us/step - loss: 0.8190 - val_loss: 0.3928\n",
      "Epoch 21/150\n",
      "792/792 [==============================] - 0s 441us/step - loss: 0.8449 - val_loss: 0.3827\n",
      "Epoch 22/150\n",
      "792/792 [==============================] - 0s 477us/step - loss: 0.7408 - val_loss: 0.3638\n",
      "Epoch 23/150\n",
      "792/792 [==============================] - 0s 400us/step - loss: 0.7729 - val_loss: 0.3816\n",
      "Epoch 24/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.7503 - val_loss: 0.4121\n",
      "Epoch 25/150\n",
      "792/792 [==============================] - 0s 451us/step - loss: 0.7551 - val_loss: 0.3608\n",
      "Epoch 26/150\n",
      "792/792 [==============================] - 0s 498us/step - loss: 0.6032 - val_loss: 0.3322\n",
      "Epoch 27/150\n",
      "792/792 [==============================] - 0s 435us/step - loss: 0.7104 - val_loss: 0.2921\n",
      "Epoch 28/150\n",
      "792/792 [==============================] - 0s 385us/step - loss: 0.5850 - val_loss: 0.3369\n",
      "Epoch 29/150\n",
      "792/792 [==============================] - 0s 354us/step - loss: 0.6084 - val_loss: 0.3424\n",
      "Epoch 30/150\n",
      "792/792 [==============================] - 0s 441us/step - loss: 0.5755 - val_loss: 0.2917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/150\n",
      "792/792 [==============================] - 0s 429us/step - loss: 0.6028 - val_loss: 0.2793\n",
      "Epoch 32/150\n",
      "792/792 [==============================] - 0s 374us/step - loss: 0.5896 - val_loss: 0.2724\n",
      "Epoch 33/150\n",
      "792/792 [==============================] - 0s 357us/step - loss: 0.5636 - val_loss: 0.2920\n",
      "Epoch 34/150\n",
      "792/792 [==============================] - 0s 415us/step - loss: 0.4631 - val_loss: 0.3299\n",
      "Epoch 35/150\n",
      "792/792 [==============================] - 0s 445us/step - loss: 0.5441 - val_loss: 0.2551\n",
      "Epoch 36/150\n",
      "792/792 [==============================] - 0s 425us/step - loss: 0.5236 - val_loss: 0.2594\n",
      "Epoch 37/150\n",
      "792/792 [==============================] - 0s 389us/step - loss: 0.5263 - val_loss: 0.2960\n",
      "Epoch 38/150\n",
      "792/792 [==============================] - 0s 364us/step - loss: 0.5227 - val_loss: 0.3173\n",
      "Epoch 39/150\n",
      "792/792 [==============================] - 0s 423us/step - loss: 0.4594 - val_loss: 0.3326\n",
      "Epoch 40/150\n",
      "792/792 [==============================] - 0s 406us/step - loss: 0.5325 - val_loss: 0.2722\n",
      "Epoch 41/150\n",
      "792/792 [==============================] - 0s 355us/step - loss: 0.4611 - val_loss: 0.2778\n",
      "Epoch 42/150\n",
      "792/792 [==============================] - 0s 408us/step - loss: 0.4671 - val_loss: 0.2457\n",
      "Epoch 43/150\n",
      "792/792 [==============================] - 0s 436us/step - loss: 0.4485 - val_loss: 0.2688\n",
      "Epoch 44/150\n",
      "792/792 [==============================] - 0s 439us/step - loss: 0.4163 - val_loss: 0.2661\n",
      "Epoch 45/150\n",
      "792/792 [==============================] - 0s 435us/step - loss: 0.4153 - val_loss: 0.2814\n",
      "Epoch 46/150\n",
      "792/792 [==============================] - 0s 353us/step - loss: 0.3941 - val_loss: 0.2323\n",
      "Epoch 47/150\n",
      "792/792 [==============================] - 0s 421us/step - loss: 0.3543 - val_loss: 0.2358\n",
      "Epoch 48/150\n",
      "792/792 [==============================] - 0s 425us/step - loss: 0.3964 - val_loss: 0.2603\n",
      "Epoch 49/150\n",
      "792/792 [==============================] - 0s 423us/step - loss: 0.4040 - val_loss: 0.3638\n",
      "Epoch 50/150\n",
      "792/792 [==============================] - 0s 435us/step - loss: 0.3715 - val_loss: 0.2955\n",
      "Epoch 51/150\n",
      "792/792 [==============================] - 0s 352us/step - loss: 0.4535 - val_loss: 0.2944\n",
      "Epoch 52/150\n",
      "792/792 [==============================] - 0s 454us/step - loss: 0.3608 - val_loss: 0.2619\n",
      "Epoch 53/150\n",
      "792/792 [==============================] - 0s 424us/step - loss: 0.3733 - val_loss: 0.2690\n",
      "Epoch 54/150\n",
      "792/792 [==============================] - 0s 365us/step - loss: 0.3122 - val_loss: 0.2565\n",
      "Epoch 55/150\n",
      "792/792 [==============================] - 0s 358us/step - loss: 0.3191 - val_loss: 0.2972\n",
      "Epoch 56/150\n",
      "792/792 [==============================] - 0s 462us/step - loss: 0.3111 - val_loss: 0.3179\n",
      "Epoch 57/150\n",
      "792/792 [==============================] - 0s 431us/step - loss: 0.3231 - val_loss: 0.3146\n",
      "Epoch 58/150\n",
      "792/792 [==============================] - 0s 369us/step - loss: 0.3141 - val_loss: 0.3174\n",
      "Epoch 59/150\n",
      "792/792 [==============================] - 0s 435us/step - loss: 0.3223 - val_loss: 0.2460\n",
      "Epoch 60/150\n",
      "792/792 [==============================] - 0s 406us/step - loss: 0.3901 - val_loss: 0.2966\n",
      "Epoch 61/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.3231 - val_loss: 0.3373\n",
      "Epoch 62/150\n",
      "792/792 [==============================] - 0s 387us/step - loss: 0.3889 - val_loss: 0.3122\n",
      "Epoch 63/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.2986 - val_loss: 0.2551\n",
      "Epoch 64/150\n",
      "792/792 [==============================] - 0s 427us/step - loss: 0.3047 - val_loss: 0.2519\n",
      "Epoch 65/150\n",
      "792/792 [==============================] - 0s 457us/step - loss: 0.3804 - val_loss: 0.2319\n",
      "Epoch 66/150\n",
      "792/792 [==============================] - 0s 469us/step - loss: 0.3090 - val_loss: 0.2494\n",
      "Epoch 67/150\n",
      "792/792 [==============================] - 0s 354us/step - loss: 0.2947 - val_loss: 0.3371\n",
      "Epoch 68/150\n",
      "792/792 [==============================] - 0s 403us/step - loss: 0.2338 - val_loss: 0.3238\n",
      "Epoch 69/150\n",
      "792/792 [==============================] - 0s 533us/step - loss: 0.3379 - val_loss: 0.2814\n",
      "Epoch 70/150\n",
      "792/792 [==============================] - 0s 462us/step - loss: 0.3220 - val_loss: 0.2616\n",
      "Epoch 71/150\n",
      "792/792 [==============================] - 0s 411us/step - loss: 0.3185 - val_loss: 0.2300\n",
      "Epoch 72/150\n",
      "792/792 [==============================] - 0s 392us/step - loss: 0.3100 - val_loss: 0.2660\n",
      "Epoch 73/150\n",
      "792/792 [==============================] - 0s 438us/step - loss: 0.2393 - val_loss: 0.3265\n",
      "Epoch 74/150\n",
      "792/792 [==============================] - 0s 479us/step - loss: 0.2271 - val_loss: 0.2867\n",
      "Epoch 75/150\n",
      "792/792 [==============================] - 0s 529us/step - loss: 0.3064 - val_loss: 0.2224\n",
      "Epoch 76/150\n",
      "792/792 [==============================] - 0s 396us/step - loss: 0.2902 - val_loss: 0.2585\n",
      "Epoch 77/150\n",
      "792/792 [==============================] - 0s 390us/step - loss: 0.2486 - val_loss: 0.2580\n",
      "Epoch 78/150\n",
      "792/792 [==============================] - 0s 437us/step - loss: 0.3433 - val_loss: 0.2180\n",
      "Epoch 79/150\n",
      "792/792 [==============================] - 0s 399us/step - loss: 0.3207 - val_loss: 0.2684\n",
      "Epoch 80/150\n",
      "792/792 [==============================] - 0s 473us/step - loss: 0.3074 - val_loss: 0.2566\n",
      "Epoch 81/150\n",
      "792/792 [==============================] - 0s 476us/step - loss: 0.2662 - val_loss: 0.2715\n",
      "Epoch 82/150\n",
      "792/792 [==============================] - 0s 462us/step - loss: 0.2678 - val_loss: 0.2264\n",
      "Epoch 83/150\n",
      "792/792 [==============================] - 0s 461us/step - loss: 0.2657 - val_loss: 0.2219\n",
      "Epoch 84/150\n",
      "792/792 [==============================] - 0s 385us/step - loss: 0.2600 - val_loss: 0.3038\n",
      "Epoch 85/150\n",
      "792/792 [==============================] - 0s 388us/step - loss: 0.2167 - val_loss: 0.2723\n",
      "Epoch 86/150\n",
      "792/792 [==============================] - 0s 360us/step - loss: 0.2828 - val_loss: 0.2245\n",
      "Epoch 87/150\n",
      "792/792 [==============================] - 0s 366us/step - loss: 0.3036 - val_loss: 0.2324\n",
      "Epoch 88/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.3095 - val_loss: 0.2464\n",
      "Epoch 89/150\n",
      "792/792 [==============================] - 0s 352us/step - loss: 0.3149 - val_loss: 0.2970\n",
      "Epoch 90/150\n",
      "792/792 [==============================] - 0s 355us/step - loss: 0.2387 - val_loss: 0.2984\n",
      "Epoch 91/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.2792 - val_loss: 0.2899\n",
      "Epoch 92/150\n",
      "792/792 [==============================] - 0s 350us/step - loss: 0.2936 - val_loss: 0.2632\n",
      "Epoch 93/150\n",
      "792/792 [==============================] - 0s 350us/step - loss: 0.2755 - val_loss: 0.2649\n",
      "Epoch 94/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.3353 - val_loss: 0.3522\n",
      "Epoch 95/150\n",
      "792/792 [==============================] - 0s 350us/step - loss: 0.2858 - val_loss: 0.3059\n",
      "Epoch 96/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.2740 - val_loss: 0.2596\n",
      "Epoch 97/150\n",
      "792/792 [==============================] - 0s 349us/step - loss: 0.3305 - val_loss: 0.3029\n",
      "Epoch 98/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.2891 - val_loss: 0.3806\n",
      "Epoch 99/150\n",
      "792/792 [==============================] - 0s 350us/step - loss: 0.2839 - val_loss: 0.4204\n",
      "Epoch 100/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.2236 - val_loss: 0.2834\n",
      "Epoch 101/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.2325 - val_loss: 0.3274\n",
      "Epoch 102/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.2622 - val_loss: 0.2918\n",
      "Epoch 103/150\n",
      "792/792 [==============================] - 0s 349us/step - loss: 0.2365 - val_loss: 0.2976\n",
      "Epoch 104/150\n",
      "792/792 [==============================] - 0s 352us/step - loss: 0.2163 - val_loss: 0.2850\n",
      "Epoch 105/150\n",
      "792/792 [==============================] - 0s 349us/step - loss: 0.2489 - val_loss: 0.2849\n",
      "Epoch 106/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.2628 - val_loss: 0.2969\n",
      "Epoch 107/150\n",
      "792/792 [==============================] - 0s 349us/step - loss: 0.2622 - val_loss: 0.2633\n",
      "Epoch 108/150\n",
      "792/792 [==============================] - 0s 354us/step - loss: 0.2113 - val_loss: 0.2591\n",
      "Epoch 109/150\n",
      "792/792 [==============================] - 0s 353us/step - loss: 0.2631 - val_loss: 0.2627\n",
      "Epoch 110/150\n",
      "792/792 [==============================] - 0s 341us/step - loss: 0.2445 - val_loss: 0.2940\n",
      "Epoch 111/150\n",
      "792/792 [==============================] - 0s 341us/step - loss: 0.2470 - val_loss: 0.2695\n",
      "Epoch 112/150\n",
      "792/792 [==============================] - 0s 338us/step - loss: 0.2626 - val_loss: 0.2390\n",
      "Epoch 113/150\n",
      "792/792 [==============================] - 0s 340us/step - loss: 0.2337 - val_loss: 0.2510\n",
      "Epoch 114/150\n",
      "792/792 [==============================] - 0s 342us/step - loss: 0.2364 - val_loss: 0.2387\n",
      "Epoch 115/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.1967 - val_loss: 0.2488\n",
      "Epoch 116/150\n",
      "792/792 [==============================] - 0s 340us/step - loss: 0.2752 - val_loss: 0.2346\n",
      "Epoch 117/150\n",
      "792/792 [==============================] - 0s 339us/step - loss: 0.1870 - val_loss: 0.2797\n",
      "Epoch 118/150\n",
      "792/792 [==============================] - 0s 340us/step - loss: 0.2438 - val_loss: 0.2620\n",
      "Epoch 119/150\n",
      "792/792 [==============================] - 0s 342us/step - loss: 0.2856 - val_loss: 0.2298\n",
      "Epoch 120/150\n",
      "792/792 [==============================] - 0s 340us/step - loss: 0.2489 - val_loss: 0.2260\n",
      "Epoch 121/150\n",
      "792/792 [==============================] - 0s 340us/step - loss: 0.2760 - val_loss: 0.2335\n",
      "Epoch 122/150\n",
      "792/792 [==============================] - 0s 342us/step - loss: 0.2457 - val_loss: 0.2495\n",
      "Epoch 123/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.2084 - val_loss: 0.2564\n",
      "Epoch 124/150\n",
      "792/792 [==============================] - 0s 343us/step - loss: 0.1744 - val_loss: 0.2199\n",
      "Epoch 125/150\n",
      "792/792 [==============================] - 0s 383us/step - loss: 0.2330 - val_loss: 0.3220\n",
      "Epoch 126/150\n",
      "792/792 [==============================] - 0s 342us/step - loss: 0.1908 - val_loss: 0.2956\n",
      "Epoch 127/150\n",
      "792/792 [==============================] - 0s 346us/step - loss: 0.2110 - val_loss: 0.3376\n",
      "Epoch 128/150\n",
      "792/792 [==============================] - 0s 340us/step - loss: 0.1930 - val_loss: 0.2449\n",
      "Epoch 129/150\n",
      "792/792 [==============================] - 0s 347us/step - loss: 0.2010 - val_loss: 0.2693\n",
      "Epoch 130/150\n",
      "792/792 [==============================] - 0s 341us/step - loss: 0.2342 - val_loss: 0.2930\n",
      "Epoch 131/150\n",
      "792/792 [==============================] - 0s 409us/step - loss: 0.2696 - val_loss: 0.2395\n",
      "Epoch 132/150\n",
      "792/792 [==============================] - 0s 472us/step - loss: 0.2532 - val_loss: 0.2582\n",
      "Epoch 133/150\n",
      "792/792 [==============================] - 0s 431us/step - loss: 0.2121 - val_loss: 0.2268\n",
      "Epoch 134/150\n",
      "792/792 [==============================] - 0s 443us/step - loss: 0.2741 - val_loss: 0.2368\n",
      "Epoch 135/150\n",
      "792/792 [==============================] - 0s 396us/step - loss: 0.2711 - val_loss: 0.2864\n",
      "Epoch 136/150\n",
      "792/792 [==============================] - 0s 350us/step - loss: 0.2147 - val_loss: 0.3102\n",
      "Epoch 137/150\n",
      "792/792 [==============================] - 0s 375us/step - loss: 0.2127 - val_loss: 0.3245\n",
      "Epoch 138/150\n",
      "792/792 [==============================] - 0s 356us/step - loss: 0.2450 - val_loss: 0.3448\n",
      "Epoch 139/150\n",
      "792/792 [==============================] - 0s 349us/step - loss: 0.1789 - val_loss: 0.2084\n",
      "Epoch 140/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.1823 - val_loss: 0.2137\n",
      "Epoch 141/150\n",
      "792/792 [==============================] - 0s 346us/step - loss: 0.2946 - val_loss: 0.3493\n",
      "Epoch 142/150\n",
      "792/792 [==============================] - 0s 347us/step - loss: 0.2633 - val_loss: 0.2499\n",
      "Epoch 143/150\n",
      "792/792 [==============================] - 0s 357us/step - loss: 0.2236 - val_loss: 0.2365\n",
      "Epoch 144/150\n",
      "792/792 [==============================] - 0s 350us/step - loss: 0.2534 - val_loss: 0.2605\n",
      "Epoch 145/150\n",
      "792/792 [==============================] - 0s 346us/step - loss: 0.1930 - val_loss: 0.2007\n",
      "Epoch 146/150\n",
      "792/792 [==============================] - 0s 348us/step - loss: 0.1873 - val_loss: 0.2184\n",
      "Epoch 147/150\n",
      "792/792 [==============================] - 0s 351us/step - loss: 0.1873 - val_loss: 0.2463\n",
      "Epoch 148/150\n",
      "792/792 [==============================] - 0s 346us/step - loss: 0.2002 - val_loss: 0.2410\n",
      "Epoch 149/150\n",
      "792/792 [==============================] - 0s 352us/step - loss: 0.1896 - val_loss: 0.2290\n",
      "Epoch 150/150\n",
      "792/792 [==============================] - 0s 356us/step - loss: 0.2384 - val_loss: 0.2221\n",
      "Train on 792 samples, validate on 198 samples\n",
      "Epoch 1/50\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 4.5929 - val_loss: 4.5840\n",
      "Epoch 2/50\n",
      "792/792 [==============================] - 0s 348us/step - loss: 4.5623 - val_loss: 4.5561\n",
      "Epoch 3/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 4.4805 - val_loss: 4.4732\n",
      "Epoch 4/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 4.3064 - val_loss: 4.2724\n",
      "Epoch 5/50\n",
      "792/792 [==============================] - 0s 341us/step - loss: 4.0080 - val_loss: 3.9973\n",
      "Epoch 6/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 3.7411 - val_loss: 3.7129\n",
      "Epoch 7/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 3.4842 - val_loss: 3.4500\n",
      "Epoch 8/50\n",
      "792/792 [==============================] - 0s 341us/step - loss: 3.2563 - val_loss: 3.2236\n",
      "Epoch 9/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 3.0918 - val_loss: 2.9926\n",
      "Epoch 10/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 2.9691 - val_loss: 2.8369\n",
      "Epoch 11/50\n",
      "792/792 [==============================] - 0s 341us/step - loss: 2.8210 - val_loss: 2.7005\n",
      "Epoch 12/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 2.7499 - val_loss: 2.5950\n",
      "Epoch 13/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 2.6384 - val_loss: 2.4835\n",
      "Epoch 14/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 2.5181 - val_loss: 2.3750\n",
      "Epoch 15/50\n",
      "792/792 [==============================] - 0s 345us/step - loss: 2.4591 - val_loss: 2.3016\n",
      "Epoch 16/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 2.3287 - val_loss: 2.2352\n",
      "Epoch 17/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 2.3433 - val_loss: 2.1891\n",
      "Epoch 18/50\n",
      "792/792 [==============================] - 0s 344us/step - loss: 2.2522 - val_loss: 2.1019\n",
      "Epoch 19/50\n",
      "792/792 [==============================] - 0s 362us/step - loss: 2.1821 - val_loss: 2.0678\n",
      "Epoch 20/50\n",
      "792/792 [==============================] - 0s 341us/step - loss: 2.1406 - val_loss: 2.0350\n",
      "Epoch 21/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 2.0874 - val_loss: 1.9992\n",
      "Epoch 22/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 2.0423 - val_loss: 1.9145\n",
      "Epoch 23/50\n",
      "792/792 [==============================] - 0s 344us/step - loss: 1.9474 - val_loss: 1.8952\n",
      "Epoch 24/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 1.9704 - val_loss: 1.8540\n",
      "Epoch 25/50\n",
      "792/792 [==============================] - 0s 344us/step - loss: 1.9389 - val_loss: 1.8543\n",
      "Epoch 26/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 1.8907 - val_loss: 1.7918\n",
      "Epoch 27/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 1.8344 - val_loss: 1.7921\n",
      "Epoch 28/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 1.8209 - val_loss: 1.7625\n",
      "Epoch 29/50\n",
      "792/792 [==============================] - 0s 343us/step - loss: 1.7246 - val_loss: 1.7181\n",
      "Epoch 30/50\n",
      "792/792 [==============================] - 0s 344us/step - loss: 1.7217 - val_loss: 1.6779\n",
      "Epoch 31/50\n",
      "792/792 [==============================] - 0s 344us/step - loss: 1.7055 - val_loss: 1.6693\n",
      "Epoch 32/50\n",
      "792/792 [==============================] - 0s 354us/step - loss: 1.6983 - val_loss: 1.6372\n",
      "Epoch 33/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 1.6342 - val_loss: 1.6115\n",
      "Epoch 34/50\n",
      "792/792 [==============================] - 0s 344us/step - loss: 1.5513 - val_loss: 1.5985\n",
      "Epoch 35/50\n",
      "792/792 [==============================] - 0s 346us/step - loss: 1.5817 - val_loss: 1.5695\n",
      "Epoch 36/50\n",
      "792/792 [==============================] - 0s 342us/step - loss: 1.5861 - val_loss: 1.5265\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 0s 347us/step - loss: 1.5371 - val_loss: 1.5339\n",
      "Epoch 38/50\n",
      "792/792 [==============================] - 0s 339us/step - loss: 1.5356 - val_loss: 1.5363\n",
      "Epoch 39/50\n",
      "792/792 [==============================] - 0s 336us/step - loss: 1.4742 - val_loss: 1.4914\n",
      "Epoch 40/50\n",
      "792/792 [==============================] - 0s 338us/step - loss: 1.4996 - val_loss: 1.5008\n",
      "Epoch 41/50\n",
      "792/792 [==============================] - 0s 334us/step - loss: 1.4486 - val_loss: 1.4839\n",
      "Epoch 42/50\n",
      "792/792 [==============================] - 0s 336us/step - loss: 1.4188 - val_loss: 1.4933\n",
      "Epoch 43/50\n",
      "792/792 [==============================] - 0s 334us/step - loss: 1.3924 - val_loss: 1.4673\n",
      "Epoch 44/50\n",
      "792/792 [==============================] - 0s 334us/step - loss: 1.4340 - val_loss: 1.4546\n",
      "Epoch 45/50\n",
      "792/792 [==============================] - 0s 336us/step - loss: 1.3531 - val_loss: 1.4535\n",
      "Epoch 46/50\n",
      "792/792 [==============================] - 0s 334us/step - loss: 1.3322 - val_loss: 1.4124\n",
      "Epoch 47/50\n",
      "792/792 [==============================] - 0s 336us/step - loss: 1.3021 - val_loss: 1.4034\n",
      "Epoch 48/50\n",
      "792/792 [==============================] - 0s 334us/step - loss: 1.2788 - val_loss: 1.4021\n",
      "Epoch 49/50\n",
      "792/792 [==============================] - 0s 337us/step - loss: 1.2837 - val_loss: 1.4015\n",
      "Epoch 50/50\n",
      "792/792 [==============================] - 0s 337us/step - loss: 1.2640 - val_loss: 1.4168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b0f672a90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform training on each of the weak learners with hypter parameters that benefit the most\n",
    "\n",
    "# Loss 2\n",
    "deep_sift.fit(vocab_train,Y_labels,batch_size=16,epochs=15, validation_split=0.2)\n",
    "# Loss 2.0 PCA 10 Values\n",
    "deep_shape.fit(train_shape_data,Y_labels,batch_size=5,epochs=40, validation_split=0.2)\n",
    "# Loss .80\n",
    "deep_margin.fit(train_margin_data,Y_labels,batch_size=20,epochs=150, validation_split=0.2)\n",
    "# Loss .22\n",
    "deep_alex.fit(probs_cnn_train,Y_labels,batch_size=16,epochs=150, validation_split=0.2)\n",
    "# Loss 1.3\n",
    "deep_texture.fit(train_texture_data,Y_labels,batch_size=16,epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the input space for the test data\n",
    "\n",
    "deep_input = np.array(deep_shape.predict(train_shape_data))\n",
    "deep_input = np.array(np.append(deep_input, deep_texture.predict(train_texture_data), axis=1))\n",
    "deep_input = np.array(np.append(deep_input, deep_margin.predict(train_margin_data), axis=1))\n",
    "deep_input = np.array(np.append(deep_input, deep_sift.predict(vocab_train), axis=1))\n",
    "deep_input = np.array(np.append(deep_input, deep_alex.predict(probs_cnn_train), axis=1))\n",
    "\n",
    "\n",
    "# print(deep_input.shape)\n",
    "# pca_deep = PCA(n_components=0.999, whiten=True)\n",
    "# pca_deep.fit(deep_input)\n",
    "# deep_input = pca_deep.transform(deep_input)\n",
    "# print(deep_input.shape)\n",
    "\n",
    "# deep_final.fit(deep_input,Y_labels,batch_size=16,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_model = Sequential([\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dense(1024, activation='relu'),\n",
    "#     Dropout(0.4),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(99, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# Setup the model for the deep network\n",
    "\n",
    "deep_model = Sequential([\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(99, activation='softmax')\n",
    "])\n",
    "\n",
    "deep_model.compile(loss='categorical_crossentropy',optimizer='adam',metric=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_input=np.array(np.append(train_margin_data,train_texture_data,axis=1))\n",
    "# deep_input=np.array(np.append(deep_input,train_shape_data,axis=1))\n",
    "# deep_input=np.array(np.append(deep_input,vocab_train,axis=1))\n",
    "# deep_input=np.array(np.append(deep_input,probs_cnn_train,axis=1))\n",
    "# Y_labels = np_utils.to_categorical(train_labels_encoded,99)\n",
    "# print deep_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 693 samples, validate on 297 samples\n",
      "Epoch 1/10\n",
      "693/693 [==============================] - 6s 9ms/step - loss: 4.4789 - val_loss: 4.2671\n",
      "Epoch 2/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 2.9144 - val_loss: 1.9793\n",
      "Epoch 3/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.8129 - val_loss: 0.4913\n",
      "Epoch 4/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.2630 - val_loss: 0.1719\n",
      "Epoch 5/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.1378 - val_loss: 0.0769\n",
      "Epoch 6/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.0852 - val_loss: 0.0418\n",
      "Epoch 7/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.0601 - val_loss: 0.0472\n",
      "Epoch 8/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.0377 - val_loss: 0.0434\n",
      "Epoch 9/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.0263 - val_loss: 0.0383\n",
      "Epoch 10/10\n",
      "693/693 [==============================] - 4s 6ms/step - loss: 0.0342 - val_loss: 0.0356\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 512)               253952    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 99)                25443     \n",
      "=================================================================\n",
      "Total params: 1,067,107\n",
      "Trainable params: 1,067,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fit the deep model on th einput space\n",
    "\n",
    "deep_model.fit(deep_input,Y_labels,batch_size=5,epochs=10, validation_split=0.3)\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_input = np.array(deep_shape.predict(train_shape_data))\n",
    "# deep_input = np.array(np.append(deep_input, deep_texture.predict(train_texture_data), axis=1))\n",
    "# deep_input = np.array(np.append(deep_input, deep_margin.predict(train_margin_data), axis=1))\n",
    "# deep_input = np.array(np.append(deep_input, deep_sift.predict(vocab_train), axis=1))\n",
    "# deep_input = np.array(np.append(deep_input, deep_alex.predict(probs_cnn_train), axis=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the test input space for the deep network\n",
    "\n",
    "deep_input_test = np.array(deep_shape.predict(test_shape_data))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_texture.predict(test_texture_data), axis=1))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_margin.predict(test_margin_data), axis=1))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_sift.predict(vocab_test), axis=1))\n",
    "deep_input_test = np.array(np.append(deep_input_test, deep_alex.predict(probs_cnn_test), axis=1))\n",
    "\n",
    "\n",
    "out_file = deep_model.predict(deep_input_test)\n",
    "out_file = np.append(np.array(test_ids).reshape(-1,1),out_file,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# out_file = generateSubmission(test_ids, test_data,svm_model,99)\n",
    "headerRow=np.array(['id'] + le.inverse_transform(range(99)).tolist())\n",
    "df = pd.DataFrame(data=out_file, columns = headerRow)\n",
    "df['id'] = df['id'].astype(np.int)\n",
    "df=df.set_index('id')\n",
    "#print df.head()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print out_file\n",
    "df.to_csv('output/15_12_18_003(ALexNet15Deep).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
