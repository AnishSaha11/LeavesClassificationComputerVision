{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images first\n",
    "numImages = len(glob.glob('./images/*jpg'))\n",
    "images = [None for i in xrange(numImages)]\n",
    "for fileName in glob.glob('./images/*jpg'):\n",
    "    fileNum = int(fileName[9:][:-4])\n",
    "    images[fileNum-1] = np.array(cv2.imread(fileName))\n",
    "images = np.array(images)\n",
    "\n",
    "# Load csv data next\n",
    "train_data = pd.read_csv('data/train.csv').drop(['species'], axis=1).values\n",
    "train_labels = pd.read_csv('data/train.csv')['species'].values\n",
    "labels=train_labels.tolist()\n",
    "train_images = [images[int(data[0]-1)] for data in train_data]\n",
    "train_ids = [data[0] for data in train_data]\n",
    "train_data = np.delete(train_data, 0, 1)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv').values\n",
    "test_images = [images[int(data[0]-1)] for data in test_data]\n",
    "test_ids = [data[0] for data in test_data]\n",
    "test_data = np.delete(test_data, 0, 1)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN train data\n",
    "def img_norm(img):\n",
    "    t= 2 * (np.float32(img) / 255 - 0.5) # normalize img pixels to [-1, 1]\n",
    "    return t\n",
    "def minibatchData(data,labels_encoded,img_size,channel_num=3,batch_num=30):\n",
    "    images=[]\n",
    "    for img in data:\n",
    "        images.append(np.transpose(img_norm(cv2.resize(img,img_size)),[2,0,1]))\n",
    "    \n",
    "    \n",
    "    if batch_num > 1:\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        print(len(images))\n",
    "        print(batch_num)\n",
    "        \n",
    "        for i in range(int(len(images) / batch_num)):\n",
    "            minibatch_d = images[i*batch_num: (i+1)*batch_num]\n",
    "            minibatch_d = np.reshape(minibatch_d, (batch_num, channel_num,img_size[0],img_size[1]))\n",
    "            batch_data.append(torch.from_numpy(minibatch_d))\n",
    "            if labels_encoded is not None:\n",
    "                minibatch_l = labels_encoded[i*batch_num: (i+1)*batch_num]\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "            else:\n",
    "                minibatch_l = np.zeros(batch_num)\n",
    "                batch_labels.append(torch.LongTensor(minibatch_l))\n",
    "        #data, labels = batch_data, batch_labels \n",
    "        \n",
    "    return zip(batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le= preprocessing.LabelEncoder()\n",
    "#encode train labels\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded=le.transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "img_size=(224,224)\n",
    "cnn_train_data = list(minibatchData(train_images,train_labels_encoded,img_size))\n",
    "#plt.imshow(cnn_train_data[0][0][3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a6f88e610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHwtJREFUeJzt3XmcFNXZ6PHfMz2sggKKOIMsA4IKiqNyAcEIxGWQqKA3GniJEkKCIua6JBq3a8x7b65vTDC+iYLBDaIEQRMQAUGCGhdANpFFBFl1ZERFFBRFZua5f3QNdM/0MD1dVV3V3c/385lPd5+u5ZnpqadPnTp1jqgqxhhTJS/oAIwx4WJJwRgTx5KCMSaOJQVjTBxLCsaYOJYUjDFxfEsKIjJIRDaKyGYRud2v/RhjvCV+9FMQkQiwCbgQKAWWA8NV9V3Pd2aM8ZRfNYVewGZV3aqq3wHPAEN82pcxxkP5Pm23LfBhzOtSoHdtCzeURtqYo3wKxRgDsI89n6lq67qW8yspSIKyuPMUERkDjAFoTFN6y/k+hWKMAfiXPrcjmeX8On0oBdrFvD4R2Bm7gKpOUtWeqtqzAY18CsMYU19+JYXlQBcRKRKRhsAwYLZP+zLGeMiX0wdVLReRG4AFQAR4QlXX+7EvY4y3/GpTQFXnAfP82r4xxh/Wo9EYE8eSgjEmjiUFY0wcSwrGmDiWFIwxcSwpGGPiWFIwxsSxpGCMiWNJwRgTx5KCMSaOJQVjTBxLCsaYOJYUTEKzP1oedAgmIJYUTA13bFlDPpH07EwSDdJlgmRJwcT54NnTGdCkkoik6V/DZj0PnZQ/eRFpJyKviMgGEVkvIjc65feKyEcistr5GexduMbPb9b+a75hQ7+nAHj4i3Z1LO2NBTtXp2U/JnluBlkpB36pqqtEpDmwUkQWOu/9SVX/6D48U4NP36zFb8Odx20EoEIreaFHG6Ifsck1KScFVS0Dypzn+0RkA9Gh3Y2HPryrL5HvYO3NEygpLPZ8+5XnFvPRgKYsaDPhUFm6Th0kP5+SK64B1qRlfyY5ngzHJiIdgTOBt4B+wA0icg2wgmhtYo8X+8kFkZYtIU9oMVv5e9ErQLR6fdrSEbTFu2EuI8e2Yt7alw9tPwiRghOoqFCsVSFcXH8liEgz4B/ATaq6F5gIdAaKidYkxtey3hgRWSEiKw5ywG0YGS+/3Yl8s6CIaWvnMW/ty05COKztFSkmhARtEPsv7+0khOipQiJaUZHa/uph7ltz0OVrfd+PqR9XNQURaUA0IUxV1X8CqOqumPcfBeYkWldVJwGTAI6WVjn9ZbHnJ+ew7P9NdF41AaIHa1U1fvDGwVSbNiN51dogog17h2sHtZ4q+HxVoPXiFr5u36Qu5aQgIgI8DmxQ1Qdiyguc9gaAy4F17kLMbn/Z8Sad81dRvdIWe7DKcG++tf+y402oz/R8Ir4lh6c7vurLdo17bmoK/YCrgbUiUvXVcycwXESKiU4Ttx241lWEWWjTk2cDsK3kcep1kNblCAfxbVvW0rVBPfflU0L4ct5JBNmWYY7MzdWHN0g8Z6TN9VCLHb/ty3s/n0B9D4i5qxYkd+UhwUFc/v2zWfT04/Xan5/Kv382S4vDE4+pybfJYEy8knV7uaXVhLoX9HyfAR+AMbWXT8b15e270vs3MPVnScFnkZYtaTo7wo0ta7Yb+OXTseew6n9HGy5jGywD4SSE7b87h42jLCFkAksKPsrv0I65S15wXqV2YFYd1At2rubiTn2o/PbbI+4PYPndDwN5wScER36HdmwcNbHuBU0oWFJIVl0t8QneP5wQUhd7UPdY8i0b9p7AzqeLKPzxNg70//jQeztv7cvam6u+ifNqrBuURv8+gdld3P8dTPpYUkhWXS3x1d5/YPsSqvoceOX3bVZDG+C3TkFc14XwteaPfX8zQ48KX1zmyCwp+GDr/efQvWGOHAy11KBmlC7hmDxvk6JJD0sKKdg27YxDz1ssaErLyUvQfsWUDmjKu+Pqf8kxjLZNO4Oi4e/UvWBMQjjwUkd27j6GTf2n4HUtyaSPJYV6+PaSXvx70iTiDvr+0P+zMU559tjUfwr9LxlD4znLal0mr7gb+09sxgeXKtsufZSwJMNIm+Op2PVJ0GFkLNEQjHxztLTS3nJ+0GEcUf4JbZi7akHQYaTd1H3HMrm0L3nnf8g1Gz+Me69P4x10btDM1fb9uB38ji1ruK9zD8+3m+n+pc+tVNWedS1nNYUkbBnfh83DHwk6jECMaL6bEae+UMv9WO4SgtdiP6f7Ao4lk1lSqEP1uwpNOFWeW3woIVyx+UKQz2z8xxRZUkhCWDoBZaNI95OpWL8x5fULlzYH4Mn2kw+V7buzLXl85ja0nGX/6bXQfsWHBhW1hOCfeQunU3ZL3xrlZbf0Ja/HKXFl+Z06xr2/YOdqnmz/Ok+2f/1QeYVWkvfGaqsluGANjbWwUYbDZdCQq5n//FNJLVvS9kxLCgkk29BoX4EJ7JnbJegQTDXJJgTAEoJLlhRiiVB5bjHLznw26EiMCYwXA7duF5G1zsQvK5yyViKyUETedx5bug81DVSZOu3hoKMwLgwc9bOgQ8h4XtUUBqpqccz5yu3AIlXtAixyXofepgm9OD7i4fBoJu0af/RV0CFkPL9OH4YAU5znU4ChPu3HPWcI9LwzTmXb0OzqqmxMKrxICgq8JCIrRWSMU9amakRn5/H46iuFZt4Hp1HqqTmPBReDMSHiReelfqq6U0SOBxaKyHvJrBSmeR8OvNSR4yJ2CTIbVK5L6t/PHIHrmoKq7nQePwFmAr2AXSJSANF5IIBw3rImAiK8etqsoCMxJjRcJQUROcqZcRoROQq4iOjkL7OBkc5iI4Hn3ezHN6qUL0zPlOsmTRJMk2fqx+3pQxtgZnSyKPKBv6vqfBFZDswQkdHAB8CVLvfjm0XdZgcdgvFQpHlzKvbuDTqMjOaqpqCqW1X1DOenu6r+zinfrarnq2oX5/Fzb8L1jpzZ3boyZ6Ftj3dwvY1I69Y5/b+Rsz0a58+dGnQIxgcb+j3FqSvzqRh4Vkrr551xKvPeWchp/329x5Fljpy8dfqCdfuCDsH46MGCFTB1BX/4vDMA/zqtedLrPjT7UaAZbX+/2Kfowi/nkkL5+Wdza6vHbYyEHHBrqy3Rx5hRo05+Yiwd714St1ykxTE0faEBv28/y/Xwctkgt5KCCIuesoSQyzb+dCL8NPq8pLCY/HYnMvetOc67lhAgx5JCpEWL6KMlBAM0eLWAOV3n1ChffSDAHrYhkFNHx7z1rwQdggmROV1fTFi+u7JpmiMJl5xKChAdrssYsP+F2uRMUhjxXilgpw7msNr+F+7vfHqaIwmXnDhCIscdyzVH2+i+JiAZ1vU6J5LCTUtfs6qiCU6GjRmZE0nhoqYH7bTBBCbTukxn/ZFSff5DY9Jtf+V3QYdQL1mfFEY03x10CCaDeH2a2WbJ0cz8usDTbfotqzsvZVq1zQTPq9PMj2/uyzu3TgD8mVnbT1lbU4i0zIxR5U343LZlbe1vxl5JyIvU+MnrcQoLdq4+lBAyUco1BRE5GZgeU9QJuAdoAfwc+NQpv1NV56UcYYqs96JJ1flNKrhpZjcAyt9pQft7Y+6YdK4k7JzZjbW9/55g7ZVxr4pmjaEry/wK1RcpJwVV3QgUA4hIBPiI6BiNo4A/qeofPYkwldjOOQObPt64ceiA7w2MOVze856xrPjPiST7/9V5RrnnsfnNq9OH84EtqrrDo+25MmW6zfJk/BFNCMmLvLrKp0j841VSGAZMi3l9g4isEZEn0j1lXKT7yRTk2y2wxqTKi7kkGwKXAVWzsk4EOhM9tSgDxteyni+TwcxbeLiZw3oxmiBtO5iZU9h5UVO4GFilqrsAVHWXqlaoaiXwKNF5IGpQ1Umq2lNVezagkQdh1GS9GE2QfrRuVNAhpMSLo2Y4MacOVZPAOC4nOg9EWlyyfk+6dmVMnfIks+55qOKq85KINAUuBK6NKb5fRIqJzjG5vdp7vvpFy1C0cxoDwDGDNwcdQkpcJQVV3Q8cW63salcRGWMClVUn3dawaIx7WZUUrGHRGPey5iiSBg2DDsGYrJA1SeHA93sEHYIxcSLdTw46hJRkTVJ45cnHgg7BmDif3Z+ZbVxZkxSMCZtlZz5b90IhlBVJIXJsq6BDMCYhaeRPb10/ZUVS2DEps4a7Mrmj++KDQYdQb1mRFHq1/SDoEIxJaHzBKr4c0SfoMOolK5LCk+1fDzoEY2r15v0TeP+h3kGHkbSsSArGhFWFVhKRPLZe8Vc+eDYzpqOzpGCMj2J72W7o9xQLdq7mwOD/EWBEdcv4Id4jXTtj4zGaTPLqY4/GvT5p6lgANo84PNTb//3sFO4+7j0g/UPEZ3xS+LTf8UGHYIwrscmgSlVC+I9tA4H0jhOS8acPy39Xv4E0jckku/ulf+CgpJKCMwDrJyKyLqaslYgsFJH3nceWTrmIyJ9FZLMzeOtZfgVvTDa7ZNPFgew32ZrCZGBQtbLbgUWq2gVY5LyG6JiNXZyfMUQHcvWeiE0LZ7LawQFl8TNSpUlSSUFVXwM+r1Y8BJjiPJ8CDI0p/5tGLQVaVBu30TVp0JAFH73t5SaNCSdN/ziPbtoU2qhqGYDzWNXi1xaInf+91ClzLdK6NV//sDfzd2TWNFzG1FePZcMD27cfVx8S1XdqpDsRGYMzIVdjmia14Xvemk+fxhFXwRljjsxNTWFX1WmB8/iJU14KtItZ7kRgZ/WVU5n3wRKCMf5zkxRmAyOd5yOB52PKr3GuQvQBvqw6zfCLDdhqjHeSOn0QkWnAAOA4ESkFfgP8FzBDREYDHwBXOovPAwYDm4H9RGeh9pUN2GqyjWr6rzpUSSopqGptrR7nJ1hWgXFugqrNm99WMnLJaDYPfNKPzRsTGhe1fy99U6tVkzlfsSL8Z6ez6DzibUoKi+n+l+s5qBVBR2WML8YXrIK8YNrQMufeh2rXa0+8bzH9t47jwDHRatbKe627s8kurd9ozqd9v0j7fkUD6BxR3dHSSntLjTORemnwagFzur7oUUTGhMPsr5vycJeunmzrX/rcSlXtWddymXP6UIeDAz8OOgRjPHfZUfvJa5pcPx6vZM7pQ11UD41yY0w2eXHzYgCK5v2Mhrsa0PGuJb7uL3uSAnZp0mS3bYOjEx5dOH8UqJL3hj83BGbVUfTMvpZBh2CM7xZOf5L5059g84P+jBKdVUnhyZM7BB2CMWkRkTy2XPUIm56os92w3rIqKRiTa7YNeowFO1eT37bQs21aUjAmC8xdPs+zbWVdUhg05OqgQzAm7frecp1n28qqqw8Aunxt0CEYkzYVWsmlpw6g+d6lnm0z62oKEOyoNcakU4+HbqBi715Pt5l1NQWAgqEbEgzrYkz2GNShF3rwO05ksefbzsqagjHZrNOz16EHv/NtpGdLCsZkmC43Ou0HPt3MWGdSqGUimD+IyHvOZC8zRaSFU95RRL4RkdXOzyO+RG1Mjpq/P7nxTN1IpqYwmZoTwSwETlPVHsAm4I6Y97aoarHz4911EmNyXLeJ1/Onk071fT91JoVEE8Go6kuqWu68XEp0xGZjjE9Of/B62v0f7xsVE/Hi6sNPgekxr4tE5G1gL3C3qr6eaKVU5n0wJheVtD2TQk1PQgCXSUFE7gLKgalOURnQXlV3i8jZwCwR6a6qNS6kquokYBJER15yE4cx2WrRN5G0Tx2X8tUHERkJXAKMcEZwRlUPqOpu5/lKYAvgzVhSxuSg+zufnvZ9ppQURGQQ8GvgMlXdH1PeWkQizvNORGee3upFoMbkmp73jA1kv8lckpwGLAFOFpFSZ/KXh4DmwMJqlx7PA9aIyDvAc8B1qlp9tuq0OGlqMH9QY7xw0tSxHPuYv8Ou1SZrRnNOJL9TR+a+Mcvz7Rrjp5LCYl+2m3OjOSdSvnU7nadbVwmTOc79X9cGHUJ2JwWAk2727pZSY/x21HNvBR1C9icFY0z95ERSuOiHI4MOwZik5J12StAh5EZSkMXvBB2CMUl5dN5jQYeQG0kB/GvRNcZLBZGmSCP/74Q8kpxJCgAPfN4JiI5rZ0wY/fHzk9EDBwKNIaeSwoIe0RmkbHo5E0YXDh/Fy6cfFXQYuZUUqKxg9AfnBh2FMXGK77ueksJi8v79dtChAFk6cOuRlPb5ygZ1NaExuPtA2uxJ323RycitmoJj0TeRoEMwhlPeuJqKPXuCDqOGnEwKQdyOakysTi+NpsNV4Zy4KOdOH4wJ2uBu/enyxcqgw6hVTtYUAIpe/FnQIZgcc8pjYykpLKbiiy+DDuWIcjYpdJoa/C3jJnf8suwsOtwTzPgI9ZXqvA/3ishHMfM7DI557w4R2SwiG0WkxK/A3cp/ObzVN5NdBl32Y9adnTkd5lKd9wHgTzHzO8wDEJFuwDCgu7POhKrh2YzJRZ1mXouuWFf3giGS0rwPRzAEeMYZwHUbsBno5SI+X3V+eVTQIZgsUr37fElhMV3GveXbnI9+cdOmcIMzbdwTItLSKWsLfBizTKlTVoOIjBGRFSKy4iDB9PU+6cfh6EFmskNs9/l+N8aMoBSCIQ/rI9WkMBHoDBQTnethvFOeKCUm/Iuo6iRV7amqPRsQ7F1hxnjpN592p9mzwY+glKqUkoKq7lLVClWtBB7l8ClCKdAuZtETsU7FJscsLW4YdAiupDrvQ0HMy8uBqpaU2cAwEWkkIkVE531Y5i5Ef9k4C8ZLM746JuNOF6qrs0ejM+/DAOA4ESkFfgMMEJFioqcG24FrAVR1vYjMAN4lOp3cOFWt8Cd070zddywjmu8OOgyTBR7vWhR0CK5l9bwPyYocdyzz1iwKbP8me4S55mnzPtRDxWdWSzCmiiUFx39sG2jDtBnXbt68IegQXLOk4Njdb48N02Zcu7DJN0GH4JodBTHCMGWXyWwRyeObBZnd2GhJIUYYpuwymWtPxX72VOzntdNnkn9Cm6DDSZkNslLNTWU9ebBgRdBhmAw0rF1fAD697hzafJG5XegtKVSz4exy64NpjuiSTRcDUKnCx9M70PqR+HESWj+yhExusrakkMCgH4xg/typQYdhQuYHvS8BoPzD0kNlrbPwG8SSQgL69noGbxzMC13n2BWJHLTl4FcMfurWQ6873l1VEyhNvEKWsaRQi4qBO/mq9ADHSJOgQzFpdN71Y2gyaxkdqWXoNJGMv7ehLvY1eARXXnld0CGYNGsyK9T376WFJYUjkCXvUFJYzPrvMr9DivFIltcSwJJCUm7peE7QIZg0OOMP1wcdQihYUkjS4NO/H3QIxmdtXygLOoRQsKSQpIrdn9N5urUxmOyX6rwP02PmfNguIqud8o4i8k3Me4/4GXy6dXzhYNAhmDq4udN13mszPYwkcyVzSXIy8BDwt6oCVf1R1XMRGQ/EzoO1RVXDO9KEC/kvr6SksJgFO1cHHYqphfUrcc/VvA8iIsBVwDSP4wq10x+0BimTvdx2XvoesEtV348pKxKRt4G9wN2q+rrLfYRO4f2LKeo4hm1DJwUdigGKZo0B4ITX8zh62lI2TYgOLi4qbL38r0GGlpHcJoXhxNcSyoD2qrpbRM4GZolId1XdW31FERkDjAFoTFOXYaSR06Ot6/XLWFQS4fwmoR+XNquVFBbTtdqA4V2vP/y6ZNzhM9nbtqzl/s6nc9uWtXHL22cYL6mBW0WkIzBHVU+LKcsHPgLOVtWEncJF5FXgV6p6xHuRgx641Y0PnzuNd/s+HXQYOanfTdfRbMZSdxsR4asre/PlsH2s6zM11AOvupWOgVsvAN6LTQgi0rpqQlkR6UR03oetLvYReu1+uI4ey4YHHUbGcnO1wHVCAFCl2YyltL1ivX2OjmQuSU4DlgAni0ipiIx23hpGzQbG84A1IvIO8BxwnaomOzltxioYmvmDdabbGX+4nkHte8ZdLQh64Fz7HKNs3gePRLp2Zt6r/zj0esN3+zm1YQa1laRRn9uu45inlx6ejdn5Hyy7JTpy0ZpfTUhqO9lc1feDzfuQZhWbttDpn9GBXzvPuI6bOvYNOKLweWl/A0oKi6MJAaLJIOZLqeCBxRQ8sJiSwmJKCosZ9cH3Aoo0t1lS8FCXG95i8AVXHXo9bJvdLxHrv79Xv9rgzj77KLniGp+iMbWxpOCxinc3RWfYBPb0y/rmlHopL/u4/istXcOA0T9n9YED3gdkErKk4IOTbj7cKl5SWMypf7UekG4aERu9uJxfF/Vm4E9/Hleed8apbsMyCVhSSIP2v11M/zFjgg4jUJW4b9BuOH95zv8d08GSQpo0nrOMrv8eGXQYgWkQ7b7iWuM5y7j45O8Ffvkym1lSSKOi4dHh3fZU7A86lIxWuW8f3SaPCzqMrGVJIQAjug/ivLWXBx1GRuv4/FdBh5C1LCmkmwgVe/fSpGQbl70/KOhoMteyteTtswF1/WBJId1iOusc6P8xPzjn0gCDyWzlW7cHHUJWsslgAla+40NPR3Oq0EpPRh/qOnks4uSvjaMmut6eyRyWFELiB+cOZe4bs1xvx21C+Kzia0a060eRLD1Uqym56/A9BiXrokNjLDjt6KS3WbJuL7e02kp+p4727Z4B7PQhJLw4WNxepqvQSkb8zyOPWL3gtKPrlRCq1gHY8JtWKcdm0seSQoj0/vXYOpcpWjD60A1DRQtGM39/o0Pvua0lRCQPljmjEnl896zd0Zg57NbpkBm9aRtXNfsy4XtVk5/W5i873qRrg6Nc7d8O3uzl2a3TItJORF4RkQ0isl5EbnTKW4nIQhF533ls6ZSLiPxZRDaLyBoROcv9r5M7Hu9aVOt7TZ5ffsR1f9GhH73uHEuvO2vWOJI5tbBeggaSO30oB36pqqcCfYBxItINuB1YpKpdgEXOa4CLiQ7D1oXowKzWdF1PVacHNQ7SJGp1LScvoeXkJZQUFsd1kErm1OKC0dfWO1aTfZKZ96FMVVc5z/cBG4C2wBBgirPYFGCo83wI8DeNWgq0EJECzyPPAZf2uMDV+k1KtlFWbj3/TP3Uq2XKGdX5TOAtoI2qlkE0cQDHO4u1BT6MWa3UKTP1VLH7c0oKiykr/4o+q3+Y0jZ+0v7cpJdtuvGTlPZhskvSSUFEmgH/AG5KNI9D7KIJymrUe0VkjIisEJEVB7EBNI7kJ+3P5ZjBm1NeP9nGw/JtO1Leh8keSSUFEWlANCFMVdV/OsW7qk4LnMeqr5lSoF3M6icCO6tvU1UnqWpPVe3ZgEbV3zYes2HNTLKSufogwOPABlV9IOat2UDVAAEjgedjyq9xrkL0Ab6sOs0wAVq6xu6zMElJpptzP+BqYG3VlPPAncB/ATOceSA+AK503psHDAY2A/uBUZ5GbFJW/kEpr36Tx4AmdunR1K7OpKCqb5C4nQCgRo8jjfaGshEwwkiV+zr34M4f9WH4PS/yi5bWhmBqshuiclDz6UuZM70lc2gJgDRoyPwdtfeUNLnF7n0w6MHv6DHeRpw2UZYUDAAF4xdDnx5Bh2FCwJKCOTyn49I1wcZhQsGSgvH8NmmT2SwpGGPiWFIwxsSxpGCMiWNJwRgTx5KCMSaOJQVjTBxLCsaYOJYUjDFxLCkYY+JYUjDGxLGkYIyJY0nBGBPHkoIxJk4o5pIUkU+Br4HPgo7FhePI7Pgh83+HTI8f/P0dOqhq67oWCkVSABCRFclMfhlWmR4/ZP7vkOnxQzh+Bzt9MMbEsaRgjIkTpqQwKegAXMr0+CHzf4dMjx9C8DuEpk3BGBMOYaopGGNCIPCkICKDRGSjiGwWkduDjidZIrJdRNaKyGoRWeGUtRKRhSLyvvPYMug4Y4nIEyLyiYisiylLGLMzF+ifnc9ljYicFVzkh2JNFP+9IvKR8zmsFpHBMe/d4cS/UURKgon6MBFpJyKviMgGEVkvIjc65eH6DFQ1sB8gAmwBOgENgXeAbkHGVI/YtwPHVSu7H7jdeX478Pug46wW33nAWcC6umImOh/oi0SnDOwDvBXS+O8FfpVg2W7O/1MjoMj5P4sEHH8BcJbzvDmwyYkzVJ9B0DWFXsBmVd2qqt8BzwBDAo7JjSHAFOf5FGBogLHUoKqvAZ9XK64t5iHA3zRqKdBCRArSE2litcRfmyHAM6p6QFW3EZ3wuJdvwSVBVctUdZXzfB+wAWhLyD6DoJNCW+DDmNelTlkmUOAlEVkpImOcsjaqWgbRfwDg+MCiS15tMWfSZ3ODU71+IuaULdTxi0hH4EzgLUL2GQSdFBLNZp0pl0P6qepZwMXAOBE5L+iAPJYpn81EoDNQDJQB453y0MYvIs2AfwA3qereIy2aoMz33yHopFAKtIt5fSKwM6BY6kVVdzqPnwAziVZNd1VV75zHT4KLMGm1xZwRn42q7lLVClWtBB7l8ClCKOMXkQZEE8JUVf2nUxyqzyDopLAc6CIiRSLSEBgGzA44pjqJyFEi0rzqOXARsI5o7COdxUYCzwcTYb3UFvNs4BqnBbwP8GVVFTdMqp1jX070c4Bo/MNEpJGIFAFdgGXpji+WiAjwOLBBVR+IeStcn0GQrbExLaybiLYO3xV0PEnG3Iloy/Y7wPqquIFjgUXA+85jq6BjrRb3NKJV7INEv4VG1xYz0arrw87nshboGdL4n3LiW0P0ICqIWf4uJ/6NwMUhiP9cotX/NcBq52dw2D4D69FojIkT9OmDMSZkLCkYY+JYUjDGxLGkYIyJY0nBGBPHkoIxJo4lBWNMHEsKxpg4/x+S8GbHF/EHEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_test_data = list(minibatchData(test_images,None,img_size,batch_num=2))\n",
    "#print cnn_train_data.size\n",
    "plt.imshow(cnn_test_data[0][0][1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=99, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#set device to (\"gpu\" if available) \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#load pretrained AlexNet\n",
    "model_ft = models.alexnet(pretrained=True)\n",
    "features = list(model_ft.classifier.children())\n",
    "#modify last layer to match class size\n",
    "features[6] = nn.Linear(in_features=4096, out_features=99, bias=True)\n",
    "model_ft.classifier = nn.Sequential(*features)\n",
    "model_ft = model_ft.to(device)\n",
    "#show final model after modification\n",
    "print (model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 0.124\n",
      "[2,    30] loss: 0.065\n",
      "[3,    30] loss: 0.038\n",
      "[4,    30] loss: 0.028\n",
      "[5,    30] loss: 0.020\n",
      "[6,    30] loss: 0.014\n",
      "[7,    30] loss: 0.012\n",
      "[8,    30] loss: 0.008\n",
      "[9,    30] loss: 0.007\n",
      "[10,    30] loss: 0.005\n",
      "[11,    30] loss: 0.005\n",
      "[12,    30] loss: 0.004\n",
      "[13,    30] loss: 0.003\n",
      "[14,    30] loss: 0.003\n",
      "[15,    30] loss: 0.003\n",
      "('Finished Training in:', 817.9221119880676)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,data in enumerate(cnn_train_data,0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        #print inputs\n",
    "        #print inputs.shape\n",
    "        optimizer_ft.zero_grad()\n",
    "       # print inputs.type('torch.DoubleTensor')\n",
    "        #inputs=inputs.type('torch.DoubleTensor')\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_ft(inputs)\n",
    "        #print outputs.shape\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 30 == 29:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 990))   \n",
    "            running_loss = 0.0\n",
    "end_time = time.time()\n",
    "\n",
    "print('Finished Training in:',end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 99)\n"
     ]
    }
   ],
   "source": [
    "probs_cnn_train=np.empty([0,99])\n",
    "\n",
    "sm = torch.nn.Softmax()\n",
    "for data in cnn_train_data:\n",
    "    images,labels=data\n",
    "    probs_cnn_train=np.append(probs_cnn_train,(model_ft(images)).data.numpy(),axis=0)\n",
    "print probs_cnn_train.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 99)\n"
     ]
    }
   ],
   "source": [
    "probs_cnn_test=np.empty([0,99])\n",
    "for data in cnn_test_data:\n",
    "    images, labels = data\n",
    "    probs_cnn_test=np.append(probs_cnn_test,(model_ft(images)).data.numpy(),axis=0)\n",
    "    #outputs=np.append(outputs,net(images).data.numpy(),axis=0)\n",
    "    #print probs\n",
    "    \n",
    "\n",
    "print probs_cnn_test.shape\n",
    "\n",
    "#sm = torch.nn.Softmax()\n",
    "#probabilities = sm(output) \n",
    "#print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the 3 histograms\n",
    "train_margin_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'margin1':'margin64']).values\n",
    "train_shape_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'shape1':'shape64']).values\n",
    "train_texture_data=((pd.read_csv('data/train.csv').drop(['species'], axis=1)).loc[:,'texture1':'texture64']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_margin_data=((pd.read_csv('data/test.csv')).loc[:,'margin1':'margin64']).values\n",
    "test_shape_data=((pd.read_csv('data/test.csv')).loc[:,'shape1':'shape64']).values\n",
    "test_texture_data=((pd.read_csv('data/test.csv')).loc[:,'texture1':'texture64']).values\n",
    "\n",
    "#print train_margin_data.head()\n",
    "#print train_shape_data.head()\n",
    "#print train_texture_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute descriptors, clusters and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptor(images, dense=False):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    des_per_Img = np.array([sift.detectAndCompute(img,None)[1] for img in images])\n",
    "    return des_per_Img\n",
    "        \n",
    "def get_clusters(descriptors, vocabSize):\n",
    "    des_list = np.concatenate(descriptors)\n",
    "\n",
    "    kmeans = MiniBatchKMeans(vocabSize, batch_size=100)\n",
    "    kmeans.fit(np.array(des_list))\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def get_vocabulary(descriptors, clusters, vocabSize):\n",
    "    return np.array([normalize(np.histogram(clusters.predict(dscrs), bins=range(vocabSize))[0].reshape(1,-1)).ravel() for dscrs in descriptors])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_start_time =time.time()\n",
    "des_list_train = get_descriptor(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors computed in 243.471721 seconds\n"
     ]
    }
   ],
   "source": [
    "des_list_test = get_descriptor(test_images)\n",
    "des_end_time =time.time()\n",
    "print \"Descriptors computed in {:2f} seconds\".format(des_end_time-des_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed in 2.754013 seconds\n"
     ]
    }
   ],
   "source": [
    "clustering_start_time=time.time()\n",
    "clusters = get_clusters(des_list_train,150)\n",
    "clustering_end_time=time.time()\n",
    "print \"Clustering completed in {:2f} seconds\".format(clustering_end_time-clustering_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 149)\n"
     ]
    }
   ],
   "source": [
    "vocab_train = get_vocabulary(des_list_train,clusters,150)\n",
    "vocab_test = get_vocabulary(des_list_test,clusters,150)\n",
    "\n",
    "print vocab_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 495)\n"
     ]
    }
   ],
   "source": [
    "mlp_train_margin = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_margin.fit(train_margin_data, train_labels_encoded)\n",
    "mlp_train_margin_pred = mlp_train_margin.predict_proba(train_margin_data)\n",
    "\n",
    "mlp_train_texture = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_texture.fit(train_texture_data, train_labels_encoded)\n",
    "mlp_train_texture_pred = mlp_train_texture.predict_proba(train_texture_data)\n",
    "\n",
    "mlp_train_shape = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_shape.fit(train_shape_data, train_labels_encoded)\n",
    "mlp_train_shape_pred = mlp_train_shape.predict_proba(train_shape_data)\n",
    "\n",
    "mlp_train_sift_bof = MLPClassifier(learning_rate='constant', max_iter=5000,hidden_layer_sizes=(80,))\n",
    "mlp_train_sift_bof.fit(vocab_train, train_labels_encoded)\n",
    "mlp_train_sift_bof_pred = mlp_train_sift_bof.predict_proba(vocab_train)\n",
    "\n",
    "second_level_input = np.array(np.append(mlp_train_margin_pred,mlp_train_texture_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,mlp_train_shape_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,mlp_train_sift_bof_pred,axis=1))\n",
    "second_level_input = np.array(np.append(second_level_input,probs_cnn_train,axis=1))\n",
    "print second_level_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(520,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=8000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlc_model = MLPClassifier(learning_rate='constant', max_iter=8000,hidden_layer_sizes=(520,))\n",
    "mlc_model.fit(second_level_input, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 495)\n"
     ]
    }
   ],
   "source": [
    "out_file = generateDeepSubmission(test_ids,test_margin_data,test_texture_data,test_shape_data,vocab_test,mlp_train_margin, mlp_train_texture,mlp_train_shape,mlp_train_sift_bof,probs_cnn_test,mlc_model,99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDeepSubmission(ids,test_l1,test_l2,test_l3,test_l4\n",
    "                           ,model_b1,model_b2,model_b3,model_b4,cnn_test_data,model_top,num_classes):\n",
    "    num_test = len(test_l1)\n",
    "    block1_pred = model_b1.predict_proba(test_l1)\n",
    "    block2_pred = model_b2.predict_proba(test_l2)\n",
    "    block3_pred = model_b3.predict_proba(test_l3)\n",
    "    block4_pred = model_b4.predict_proba(test_l4)\n",
    "    final_input = np.array(np.append(block1_pred,block2_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,block3_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,block4_pred,axis=1))\n",
    "    final_input = np.array(np.append(final_input,cnn_test_data,axis=1))\n",
    "    print final_input.shape\n",
    "    final_pred = model_top.predict(final_input)\n",
    "    final_confidence =  model_top.predict_proba(final_input)\n",
    "    final_confidence = np.append(np.array(ids).reshape(-1,1),final_confidence,axis=1)\n",
    "    \n",
    "    return final_confidence\n",
    "    #for i in xrange()\n",
    "    #return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishsaha/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# out_file = generateSubmission(test_ids, test_data,svm_model,99)\n",
    "headerRow=np.array(['id'] + le.inverse_transform(range(99)).tolist())\n",
    "df = pd.DataFrame(data=out_file, columns = headerRow)\n",
    "df['id'] = df['id'].astype(np.int)\n",
    "df=df.set_index('id')\n",
    "#print df.head()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "# print out_file\n",
    "df.to_csv('output/27_11_18_001(ALexNet15).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
